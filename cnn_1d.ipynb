{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End FINN Flow for a 1D Convolutional Net (FINN v0.31b)\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "FINN walkthrough for 1D CNN on Ultra96 for use in CG4002 by Daniel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changelog\n",
    "\n",
    "| Date | Changes |\n",
    "|:------:|:---------:|\n",
    "| 28/9/2020 | Added creation of CNN in section 1.1 (training not setup yet) |\n",
    "| 29/9/2020 | Tweaked creation of CNN and added notes for finn compilation. Added basic training and testing, with a simple CNN being implemented |\n",
    "| 5/10/2020 | Show accuracy and loss during training and cleaned up training code |\n",
    "| 19/10/2020 | Added Brevitas Conv1D layers and create new 1D CNN |\n",
    "| 20/10/2020 | Add extra 1D CNN using 16 inputs instead of 8, cleaned up notebook with end-to-end flow. |\n",
    "| 28/10/2020 | Finish up 24 input 2D CNN with 5 channels |\n",
    "\n",
    "## To Do\n",
    "| |\n",
    "|:------|\n",
    "| Show hardware utilization by pulling logs from Vivado in /tmp/ |\n",
    "| Clean up this notebook (FINN transformation parts) |\n",
    "| Exporting to ONNX is not working for 1D CNN, for the hardware synthesis, copy over the FC layers, export and synthesize | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Recap of the End-to-End Flow\n",
    "\n",
    "The FINN compiler comes with many *transformations* that modify the ONNX representation of the network according to certain patterns. This notebook will demonstrate a *possible* sequence of such transformations to create a 1D CNN, train it and export to FINN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](finn-design-flow-example.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: toposort==1.5 from file:///workspace/finn/notebooks/CNN/toposort-1.5-py2.py3-none-any.whl in /workspace/.local/lib/python3.6/site-packages (1.5)\n",
      "toposort                      1.5      \n",
      "Requirement already satisfied: dependencies in /workspace/.local/lib/python3.6/site-packages (4.1.0)\n",
      "dependencies                  4.1.0    \n",
      "Requirement already satisfied: pandas in /workspace/.local/lib/python3.6/site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.6/site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "pandas                        1.1.3    \n"
     ]
    }
   ],
   "source": [
    "# run this to install libaries, and restart the kernel for it to take effect\n",
    "\n",
    "# download the whl file on this local machine\n",
    "# restart the entire kernel after installation to have the notebook recognize modules\n",
    "# toposort may not be needed, will delete if needed\n",
    "!pip install /workspace/finn/notebooks/CNN/toposort-1.5-py2.py3-none-any.whl --user\n",
    "!pip list | grep \"toposort\"\n",
    "\n",
    "!pip install dependencies --user\n",
    "!pip list | grep \"dependencies\"\n",
    "\n",
    "!pip install pandas --user\n",
    "!pip list | grep \"pandas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version  \r\n",
      "----------------------------- ---------\r\n",
      "alabaster                     0.7.12   \r\n",
      "appdirs                       1.4.4    \r\n",
      "argon2-cffi                   20.1.0   \r\n",
      "asn1crypto                    0.24.0   \r\n",
      "async-generator               1.10     \r\n",
      "attrs                         20.2.0   \r\n",
      "Babel                         2.8.0    \r\n",
      "backcall                      0.1.0    \r\n",
      "beautifulsoup4                4.7.1    \r\n",
      "bitstring                     3.1.7    \r\n",
      "bleach                        3.2.1    \r\n",
      "certifi                       2020.6.20\r\n",
      "cffi                          1.12.3   \r\n",
      "cfgv                          3.2.0    \r\n",
      "chardet                       3.0.4    \r\n",
      "conda                         4.6.14   \r\n",
      "conda-build                   3.17.8   \r\n",
      "cryptography                  2.6.1    \r\n",
      "cycler                        0.10.0   \r\n",
      "decorator                     4.4.0    \r\n",
      "defusedxml                    0.6.0    \r\n",
      "dependencies                  4.1.0    \r\n",
      "distlib                       0.3.1    \r\n",
      "docrep                        0.3.1    \r\n",
      "docutils                      0.16     \r\n",
      "entrypoints                   0.3      \r\n",
      "filelock                      3.0.10   \r\n",
      "future                        0.18.2   \r\n",
      "glob2                         0.6      \r\n",
      "identify                      1.5.5    \r\n",
      "idna                          2.8      \r\n",
      "imagesize                     1.2.0    \r\n",
      "importlib-metadata            2.0.0    \r\n",
      "importlib-resources           3.0.0    \r\n",
      "iniconfig                     1.0.1    \r\n",
      "ipykernel                     5.3.4    \r\n",
      "ipython                       7.5.0    \r\n",
      "ipython-genutils              0.2.0    \r\n",
      "ipywidgets                    7.5.1    \r\n",
      "jedi                          0.13.3   \r\n",
      "Jinja2                        2.10.1   \r\n",
      "jsonschema                    3.2.0    \r\n",
      "jupyter                       1.0.0    \r\n",
      "jupyter-client                6.1.7    \r\n",
      "jupyter-console               6.2.0    \r\n",
      "jupyter-core                  4.6.3    \r\n",
      "jupyterlab-pygments           0.1.1    \r\n",
      "kiwisolver                    1.2.0    \r\n",
      "libarchive-c                  2.8      \r\n",
      "lief                          0.9.0    \r\n",
      "MarkupSafe                    1.1.1    \r\n",
      "matplotlib                    3.3.2    \r\n",
      "mistune                       0.8.4    \r\n",
      "mkl-fft                       1.0.12   \r\n",
      "mkl-random                    1.0.2    \r\n",
      "nbclient                      0.5.0    \r\n",
      "nbconvert                     6.0.6    \r\n",
      "nbformat                      5.0.7    \r\n",
      "nest-asyncio                  1.4.1    \r\n",
      "netron                        4.5.3    \r\n",
      "nodeenv                       1.5.0    \r\n",
      "notebook                      6.1.4    \r\n",
      "numpy                         1.19.2   \r\n",
      "olefile                       0.46     \r\n",
      "onnx                          1.5.0    \r\n",
      "onnxruntime                   1.2.0    \r\n",
      "packaging                     20.4     \r\n",
      "pandas                        1.1.3    \r\n",
      "pandocfilters                 1.4.2    \r\n",
      "parso                         0.4.0    \r\n",
      "pexpect                       4.7.0    \r\n",
      "pickleshare                   0.7.5    \r\n",
      "Pillow                        7.2.0    \r\n",
      "pip                           19.1     \r\n",
      "pkginfo                       1.5.0.1  \r\n",
      "pluggy                        0.13.1   \r\n",
      "pre-commit                    2.7.1    \r\n",
      "prometheus-client             0.8.0    \r\n",
      "prompt-toolkit                2.0.9    \r\n",
      "protobuf                      3.13.0   \r\n",
      "psutil                        5.6.2    \r\n",
      "ptyprocess                    0.6.0    \r\n",
      "py                            1.9.0    \r\n",
      "pycosat                       0.6.3    \r\n",
      "pycparser                     2.19     \r\n",
      "Pygments                      2.4.1    \r\n",
      "pyOpenSSL                     19.0.0   \r\n",
      "pyparsing                     2.4.7    \r\n",
      "pyrsistent                    0.17.3   \r\n",
      "PySocks                       1.6.8    \r\n",
      "pytest                        6.1.0    \r\n",
      "pytest-dependency             0.5.1    \r\n",
      "python-dateutil               2.8.1    \r\n",
      "pytz                          2019.1   \r\n",
      "PyVerilator                   0.1.0    \r\n",
      "PyYAML                        5.1      \r\n",
      "pyzmq                         19.0.2   \r\n",
      "qtconsole                     4.7.7    \r\n",
      "QtPy                          1.9.0    \r\n",
      "requests                      2.21.0   \r\n",
      "ruamel-yaml                   0.15.46  \r\n",
      "scipy                         1.5.2    \r\n",
      "Send2Trash                    1.5.0    \r\n",
      "setuptools                    41.0.1   \r\n",
      "six                           1.15.0   \r\n",
      "snowballstemmer               2.0.0    \r\n",
      "soupsieve                     1.8      \r\n",
      "Sphinx                        3.1.2    \r\n",
      "sphinx-rtd-theme              0.5.0    \r\n",
      "sphinxcontrib-applehelp       1.0.2    \r\n",
      "sphinxcontrib-devhelp         1.0.2    \r\n",
      "sphinxcontrib-htmlhelp        1.0.3    \r\n",
      "sphinxcontrib-jsmath          1.0.1    \r\n",
      "sphinxcontrib-qthelp          1.0.3    \r\n",
      "sphinxcontrib-serializinghtml 1.1.4    \r\n",
      "tclwrapper                    0.0.1    \r\n",
      "terminado                     0.9.1    \r\n",
      "testpath                      0.4.4    \r\n",
      "toml                          0.10.1   \r\n",
      "toposort                      1.5      \r\n",
      "torch                         1.1.0    \r\n",
      "torchvision                   0.2.2    \r\n",
      "tornado                       6.0.4    \r\n",
      "tqdm                          4.31.1   \r\n",
      "traitlets                     4.3.2    \r\n",
      "typing                        3.7.4.3  \r\n",
      "typing-extensions             3.7.4.3  \r\n",
      "urllib3                       1.24.2   \r\n",
      "virtualenv                    20.0.31  \r\n",
      "wcwidth                       0.1.7    \r\n",
      "webencodings                  0.5.1    \r\n",
      "wget                          3.2      \r\n",
      "wheel                         0.33.1   \r\n",
      "widgetsnbextension            3.5.1    \r\n",
      "zipp                          3.2.0    \r\n"
     ]
    }
   ],
   "source": [
    "!pip list torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the build directory and file name\n",
    "build_dir = \"/workspace/finn/notebooks/CNN\"\n",
    "file_name = \"/cnn_1d_3_classes_24\"\n",
    "model_path = \"/cnv_1d_2_24.pt\"\n",
    "\n",
    "# exports a new model\n",
    "create_new_model = True\n",
    "\n",
    "# train the network, will implement loading of statedict later on\n",
    "train_network = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports are put here\n",
    "from finn.util.basic import make_build_dir\n",
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "import datetime # for tracking time for each code block\n",
    "import time\n",
    "\n",
    "# 1. Brevitas Export, FINN Import and Tidy-Up\n",
    "\n",
    "# 1.1 Network Setup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "from dependencies import Injector, value\n",
    "\n",
    "from brevitas.core.bit_width import BitWidthImplType\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.core.scaling import ScalingImplType\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module, ModuleList, BatchNorm2d, MaxPool2d, BatchNorm1d, MaxPool1d, Sequential\n",
    "\n",
    "from brevitas.nn import QuantConv2d, QuantLinear\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "\n",
    "# 1.2 Training the Network\n",
    "\n",
    "# imports\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 1.3 Split Model into Conv1D and FC Layers\n",
    "import torch.nn.functional as func\n",
    "\n",
    "# 1.4 Export and Tidy-Up\n",
    "\n",
    "import onnx\n",
    "import brevitas.onnx as bo\n",
    "\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.double_to_single_float import DoubleToSingleFloat\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames\n",
    "\n",
    "# 2. How FINN Implements Convolutions: Lowering and Streamlining\n",
    "\n",
    "from finn.transformation.streamline import Streamline\n",
    "from finn.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from finn.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC\n",
    "\n",
    "# 3. Partitioning, Conversion to HLS Layers and Folding\n",
    "\n",
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (CreateDataflowPartition,)\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "\n",
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_tlastmarker import InsertTLastMarker\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "\n",
    "from finn.transformation.fpgadataflow.prepare_ip import PrepareIP\n",
    "from finn.transformation.fpgadataflow.hlssynth_ip import HLSSynthIP\n",
    "from finn.util.basic import pynq_part_map\n",
    "\n",
    "from finn.transformation.fpgadataflow.replace_verilog_relpaths import (ReplaceVerilogRelPaths,)\n",
    "from finn.transformation.fpgadataflow.create_stitched_ip import CreateStitchedIP\n",
    "\n",
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_tlastmarker import InsertTLastMarker\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "\n",
    "import toposort\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "\n",
    "# 4. Hardware Generation\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "from finn.transformation.fpgadataflow.make_deployment import DeployToPYNQ\n",
    "\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "from finn.transformation import Transformation\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.util.basic import get_by_name, make_build_dir\n",
    "from finn.util.basic import get_num_default_workers\n",
    "from finn.util.basic import pynq_part_map\n",
    "\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (CreateDataflowPartition,)\n",
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "from finn.transformation.fpgadataflow.prepare_ip import PrepareIP\n",
    "from finn.transformation.fpgadataflow.hlssynth_ip import HLSSynthIP\n",
    "from finn.transformation.fpgadataflow.create_stitched_ip import CreateStitchedIP\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames\n",
    "from shutil import copy\n",
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "\n",
    "# 5. Deployment and Remote Execution\n",
    "\n",
    "import pkg_resources as pk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "\n",
    "import numpy as np\n",
    "from finn.core.onnx_exec import execute_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing, Training and Exporting the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Network Setup\n",
    "\n",
    "Declare the network below, and then create the CNN.\n",
    "\n",
    "Note that you may have to trail and error for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# network for CNV_1w1a\\n# QuantConv2d configuration (IN_CH, OUT_CH, bias)\\nCNV_OUT_CH_POOL = [(0, 64, False), (1, 64, True), (2, 128, False), (3, 128, True), (4, 256, False), (5, 256, False)]\\n\\n# Intermediate QuantLinear configuration\\nINTERMEDIATE_FC_PER_OUT_CH_SCALING = True\\nINTERMEDIATE_FC_FEATURES = [(256, 512), (512, 512)] # (IN_CH, OUT_CH)\\n\\n# Last QuantLinear configuration\\nLAST_FC_IN_FEATURES = 512\\nLAST_FC_PER_OUT_CH_SCALING = False\\n\\n# MaxPool2d configuration\\nPOOL_SIZE = 2\\n\\n# Network specific bit-widths and IO\\nWEIGHT_BIT_WIDTH = 1\\nACT_BIT_WIDTH = 1\\nIN_BIT_WIDTH = 8\\nNUM_CLASSES = 10\\nIN_CHANNELS = 3\\n\\nINPUT_SPECIFICATIONS = (1, 3, 32, 32)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declare features of the CNN\n",
    "\n",
    "# notes on CNN structure:\n",
    "# conv layers should have alternating maxpool layers, and the last layer should not be a max pool layer\n",
    "# last layer needs Sequential() to flatten for the fully connected layers\n",
    "#\n",
    "# maxpool layers must have their input size be divisible by kernel size, this is mandatory\n",
    "# Conv1D is non-synthesizeable under FINN 0.3b\n",
    "\n",
    "# QuantConv1d configuration (i, OUT_CH, is_maxpool_enabled)\n",
    "#CNV_OUT_CH_POOL = [(0, 16, False), (1, 32, True), (2, 64, False), (3, 128, False)]\n",
    "CNV_OUT_CH_POOL = [(0, 32, False), (1, 64, True), (2, 128, False), (3, 256, True)]\n",
    "KERNEL_SIZE = 4 # default 3\n",
    "NUM_CONV_LAYERS = 3\n",
    "\n",
    "# Intermediate QuantLinear configuration\n",
    "INTERMEDIATE_FC_PER_OUT_CH_SCALING = True\n",
    "#INTERMEDIATE_FC_FEATURES = [(768, 384), (384, 192)] # (IN_CH, OUT_CH)\n",
    "INTERMEDIATE_FC_FEATURES = [(256, 128)] # (IN_CH, OUT_CH)\n",
    "\n",
    "# Last QuantLinear configuration\n",
    "LAST_FC_IN_FEATURES = 128\n",
    "LAST_FC_PER_OUT_CH_SCALING = False\n",
    "\n",
    "# MaxPool2d configuration\n",
    "POOL_SIZE = 2\n",
    "\n",
    "# fully connected dropout layers\n",
    "IN_DROPOUT = 0.2\n",
    "HIDDEN_DROPOUT = 0.2\n",
    "\n",
    "# Network specific bit-widths and IO\n",
    "WEIGHT_BIT_WIDTH = 1\n",
    "ACT_BIT_WIDTH = 1\n",
    "IN_BIT_WIDTH = 8\n",
    "NUM_CLASSES = 3\n",
    "IN_CHANNELS = 2\n",
    "\n",
    "# only use inputs that are multiples of 4\n",
    "INPUT_SPECIFICATIONS = (1, 2, 24) # batch size, channels, length\n",
    "\n",
    "classes = [\"shrug\", \"zigzag\", \"windows\"]\n",
    "\n",
    "\"\"\"\n",
    "# network for CNV_1w1a\n",
    "# QuantConv2d configuration (IN_CH, OUT_CH, bias)\n",
    "CNV_OUT_CH_POOL = [(0, 64, False), (1, 64, True), (2, 128, False), (3, 128, True), (4, 256, False), (5, 256, False)]\n",
    "\n",
    "# Intermediate QuantLinear configuration\n",
    "INTERMEDIATE_FC_PER_OUT_CH_SCALING = True\n",
    "INTERMEDIATE_FC_FEATURES = [(256, 512), (512, 512)] # (IN_CH, OUT_CH)\n",
    "\n",
    "# Last QuantLinear configuration\n",
    "LAST_FC_IN_FEATURES = 512\n",
    "LAST_FC_PER_OUT_CH_SCALING = False\n",
    "\n",
    "# MaxPool2d configuration\n",
    "POOL_SIZE = 2\n",
    "\n",
    "# Network specific bit-widths and IO\n",
    "WEIGHT_BIT_WIDTH = 1\n",
    "ACT_BIT_WIDTH = 1\n",
    "IN_BIT_WIDTH = 8\n",
    "NUM_CLASSES = 10\n",
    "IN_CHANNELS = 3\n",
    "\n",
    "INPUT_SPECIFICATIONS = (1, 3, 32, 32)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuantConv1D as per https://github.com/Xilinx/brevitas/blob/quant_quartznet_4b-r0/brevitas/nn/quant_conv1d.py\n",
    "from enum import auto\n",
    "from typing import Union, Optional, Tuple\n",
    "import re\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import docrep\n",
    "from torch.nn import Conv1d, Module\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import conv1d\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from brevitas.core.bit_width import BitWidthParameter, BitWidthConst, BitWidthImplType\n",
    "from brevitas.core.quant import QuantType, IdentityQuant\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.core.scaling import ScalingImplType, SCALING_SCALAR_SHAPE\n",
    "from brevitas.core.stats import StatsInputViewShapeImpl, StatsOp\n",
    "from brevitas.function.ops import max_uint, ceil_ste\n",
    "#from brevitas.function.ops_ste import ceil_ste\n",
    "from brevitas.proxy.parameter_quant import WeightQuantProxy, BiasQuantProxy, WeightReg\n",
    "from brevitas.utils.python_utils import AutoName\n",
    "from brevitas.nn.quant_layer import QuantLayer, SCALING_MIN_VAL\n",
    "from brevitas.config import docstrings\n",
    "__all__ = ['QuantConv1d']\n",
    "\n",
    "\n",
    "class PaddingType(AutoName):\n",
    "    STANDARD = auto()\n",
    "    SAME = auto()\n",
    "\n",
    "\n",
    "@docstrings.dedent\n",
    "class QuantConv1d(QuantLayer, Conv1d):\n",
    "    \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        %(weight_quant_proxy.parameters_with_prefix)s\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: Union[int, Tuple[int]],\n",
    "                 stride: Union[int, Tuple[int]] = 1,\n",
    "                 padding: Union[int, Tuple[int]] = 0,\n",
    "                 padding_type: PaddingType = PaddingType.STANDARD,\n",
    "                 dilation: Union[int, Tuple[int]] = 1,\n",
    "                 groups: int = 1,\n",
    "                 bias: bool = True,\n",
    "                 bias_quant_type: QuantType = QuantType.FP,\n",
    "                 bias_narrow_range: bool = False,\n",
    "                 bias_bit_width: int = None,\n",
    "                 weight_quant_override: WeightQuantProxy = None,\n",
    "                 weight_quant_type: QuantType = QuantType.FP,\n",
    "                 weight_narrow_range: bool = False,\n",
    "                 weight_scaling_override: Optional[Module] = None,\n",
    "                 weight_bit_width_impl_override: Union[BitWidthParameter, BitWidthConst] = None,\n",
    "                 weight_bit_width_impl_type: BitWidthImplType = BitWidthImplType.CONST,\n",
    "                 weight_restrict_bit_width_type: RestrictValueType = RestrictValueType.INT,\n",
    "                 weight_bit_width: int = 32,\n",
    "                 weight_min_overall_bit_width: Optional[int] = 2,\n",
    "                 weight_max_overall_bit_width: Optional[int] = None,\n",
    "                 weight_scaling_impl_type: ScalingImplType = ScalingImplType.STATS,\n",
    "                 weight_scaling_const: Optional[float] = None,\n",
    "                 weight_scaling_stats_op: StatsOp = StatsOp.MAX,\n",
    "                 weight_scaling_per_output_channel: bool = False,\n",
    "                 weight_ternary_threshold: float = 0.5,\n",
    "                 weight_restrict_scaling_type: RestrictValueType = RestrictValueType.LOG_FP,\n",
    "                 weight_scaling_stats_sigma: float = 3.0,\n",
    "                 weight_scaling_min_val: float = SCALING_MIN_VAL,\n",
    "                 weight_override_pretrained_bit_width: bool = False,\n",
    "                 compute_output_scale: bool = False,\n",
    "                 compute_output_bit_width: bool = False,\n",
    "                 return_quant_tensor: bool = False) -> None:\n",
    "        QuantLayer.__init__(self,\n",
    "                            compute_output_scale=compute_output_scale,\n",
    "                            compute_output_bit_width=compute_output_bit_width,\n",
    "                            return_quant_tensor=return_quant_tensor)\n",
    "        Conv1d.__init__(self,\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=padding,\n",
    "                        dilation=dilation,\n",
    "                        groups=groups,\n",
    "                        bias=bias)\n",
    "        if weight_quant_type == QuantType.FP and compute_output_bit_width:\n",
    "            raise Exception(\"Computing output bit width requires enabling quantization\")\n",
    "        if bias_quant_type != QuantType.FP and not (compute_output_scale and compute_output_bit_width):\n",
    "            raise Exception(\"Quantizing bias requires to compute output scale and output bit width\")\n",
    "\n",
    "        self.per_elem_ops = 2 * self.kernel_size[0] * (in_channels // groups)\n",
    "        self.padding_type = padding_type\n",
    "        self.weight_reg = WeightReg()\n",
    "\n",
    "        if weight_quant_override is not None:\n",
    "            self.weight_quant = weight_quant_override\n",
    "            self.weight_quant.add_tracked_parameter(self.weight)\n",
    "        else:\n",
    "            weight_scaling_stats_input_concat_dim = 1\n",
    "            if weight_scaling_per_output_channel:\n",
    "                weight_stats_input_view_shape_impl = StatsInputViewShapeImpl.OVER_OUTPUT_CHANNELS\n",
    "                weight_scaling_shape = self.per_output_channel_broadcastable_shape\n",
    "                weight_scaling_stats_reduce_dim = 1\n",
    "            else:\n",
    "                weight_stats_input_view_shape_impl = StatsInputViewShapeImpl.OVER_TENSOR\n",
    "                weight_scaling_shape = SCALING_SCALAR_SHAPE\n",
    "                weight_scaling_stats_reduce_dim = None\n",
    "\n",
    "            if weight_scaling_stats_op == StatsOp.MAX_AVE:\n",
    "                weight_stats_input_view_shape_impl = StatsInputViewShapeImpl.OVER_OUTPUT_CHANNELS\n",
    "                weight_scaling_stats_reduce_dim = 1\n",
    "\n",
    "            self.weight_quant = WeightQuantProxy(bit_width=weight_bit_width,\n",
    "                                                 quant_type=weight_quant_type,\n",
    "                                                 narrow_range=weight_narrow_range,\n",
    "                                                 scaling_override=weight_scaling_override,\n",
    "                                                 restrict_scaling_type=weight_restrict_scaling_type,\n",
    "                                                 scaling_const=weight_scaling_const,\n",
    "                                                 scaling_stats_op=weight_scaling_stats_op,\n",
    "                                                 scaling_impl_type=weight_scaling_impl_type,\n",
    "                                                 scaling_stats_reduce_dim=weight_scaling_stats_reduce_dim,\n",
    "                                                 scaling_shape=weight_scaling_shape,\n",
    "                                                 bit_width_impl_type=weight_bit_width_impl_type,\n",
    "                                                 bit_width_impl_override=weight_bit_width_impl_override,\n",
    "                                                 restrict_bit_width_type=weight_restrict_bit_width_type,\n",
    "                                                 min_overall_bit_width=weight_min_overall_bit_width,\n",
    "                                                 max_overall_bit_width=weight_max_overall_bit_width,\n",
    "                                                 tracked_parameter_list_init=self.weight,\n",
    "                                                 ternary_threshold=weight_ternary_threshold,\n",
    "                                                 scaling_stats_input_view_shape_impl=weight_stats_input_view_shape_impl,\n",
    "                                                 scaling_stats_input_concat_dim=weight_scaling_stats_input_concat_dim,\n",
    "                                                 scaling_stats_sigma=weight_scaling_stats_sigma,\n",
    "                                                 scaling_min_val=weight_scaling_min_val,\n",
    "                                                 override_pretrained_bit_width=weight_override_pretrained_bit_width)\n",
    "        self.bias_quant = BiasQuantProxy(quant_type=bias_quant_type,\n",
    "                                         bit_width=bias_bit_width,\n",
    "                                         narrow_range=bias_narrow_range)\n",
    "\n",
    "    @property\n",
    "    def per_output_channel_broadcastable_shape(self):\n",
    "        if self.transposed:\n",
    "            raise Exception(\"Transposed filters are not supported.\")\n",
    "        else:\n",
    "            output_dim = 0\n",
    "        per_channel_size = [1] * len(self.weight.size())\n",
    "        per_channel_size[output_dim] = self.out_channels\n",
    "        per_channel_size = tuple(per_channel_size)\n",
    "        return per_channel_size\n",
    "\n",
    "    @property\n",
    "    def int_weight(self):\n",
    "        if isinstance(self.weight_quant.tensor_quant, IdentityQuant):\n",
    "            raise Exception(\"Can't export int weight without quantization enabled\")\n",
    "        return self.weight_quant.int_weight(self.weight)\n",
    "\n",
    "    @property\n",
    "    def quant_weight_scale(self):\n",
    "        \"\"\"\n",
    "        Returns scale factor of the quantized weights with scalar () shape or (self.out_channels, 1, 1)\n",
    "        shape depending on whether scaling is per layer or per-channel.\n",
    "        -------\n",
    "        \"\"\"\n",
    "        if isinstance(self.weight_quant.tensor_quant, IdentityQuant):\n",
    "            raise Exception(\"Can't generate scaling factor without quantization enabled\")\n",
    "        zero_hw_sentinel = self.weight_quant.zero_hw_sentinel\n",
    "        _, scale, _ = self.weight_quant.tensor_quant(self.weight, zero_hw_sentinel)\n",
    "        return scale\n",
    "\n",
    "    def forward(self, input):\n",
    "        output_scale = None\n",
    "        output_bit_width = None\n",
    "        quant_bias_bit_width = None\n",
    "\n",
    "        input, input_scale, input_bit_width = self.unpack_input(input)\n",
    "        quant_weight, quant_weight_scale, quant_weight_bit_width = self.weight_quant(self.weight)\n",
    "        quant_weight = self.weight_reg(quant_weight)\n",
    "\n",
    "        if self.compute_output_bit_width:\n",
    "            assert input_bit_width is not None\n",
    "            output_bit_width = self.max_output_bit_width(input_bit_width, quant_weight_bit_width)\n",
    "        if self.compute_output_scale:\n",
    "            assert input_scale is not None\n",
    "            output_scale = input_scale * quant_weight_scale\n",
    "\n",
    "        if self.bias is not None:\n",
    "            quant_bias, _, quant_bias_bit_width = self.bias_quant(self.bias, output_scale, output_bit_width)\n",
    "            output = self.conv1d(input, quant_weight, quant_bias)\n",
    "        else:\n",
    "            output = self.conv1d(input, quant_weight, None)\n",
    "\n",
    "        if self.compute_output_bit_width and quant_bias_bit_width is not None:\n",
    "            output_bit_width = torch.where(quant_bias_bit_width > output_bit_width,\n",
    "                                           quant_bias_bit_width,\n",
    "                                           output_bit_width)\n",
    "\n",
    "        return self.pack_output(output, output_scale, output_bit_width)\n",
    "\n",
    "    def conv1d(self, x, weight, bias):\n",
    "        if self.padding_type == PaddingType.SAME:\n",
    "            out = self.conv1d_same_padding(x, weight, bias)\n",
    "        else:\n",
    "            out = conv1d(x, weight, bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "        return out\n",
    "\n",
    "    def conv1d_same_padding(self, x, weight, bias):\n",
    "        ih = x.size()[-1]\n",
    "        kh = weight.size()[-1]\n",
    "        sh = self.stride[0]\n",
    "        oh = math.ceil(ih / sh)\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        if pad_h > 0:\n",
    "            x = F.pad(x, [pad_h // 2, pad_h - pad_h // 2])\n",
    "        out = F.conv1d(x, weight, bias, self.stride, 0, self.dilation, self.groups)\n",
    "        return out\n",
    "\n",
    "    def merge_bn_in(self, bn, affine_only, sign_only):\n",
    "        raise Exception(\"Merged Batch-Normalization is not yet supported\")\n",
    "\n",
    "    def max_output_bit_width(self, input_bit_width, weight_bit_width):\n",
    "        max_uint_input = max_uint(bit_width=input_bit_width, narrow_range=False)\n",
    "        max_kernel_val = self.weight_quant.tensor_quant.int_quant.max_uint(weight_bit_width)\n",
    "        group_size = self.out_channels // self.groups\n",
    "        max_uint_output = max_uint_input * max_kernel_val * self.kernel_size[0] * group_size\n",
    "        max_output_bit_width = ceil_ste(torch.log2(max_uint_output))\n",
    "        return max_output_bit_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common\n",
    "from brevitas.core.bit_width import BitWidthImplType\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.core.scaling import ScalingImplType\n",
    "from brevitas.core.stats import StatsOp\n",
    "from brevitas.nn import QuantConv2d, QuantHardTanh, QuantLinear\n",
    "\n",
    "# Quant common\n",
    "BIT_WIDTH_IMPL_TYPE = BitWidthImplType.CONST\n",
    "SCALING_VALUE_TYPE = RestrictValueType.LOG_FP\n",
    "SCALING_IMPL_TYPE = ScalingImplType.PARAMETER\n",
    "NARROW_RANGE_ENABLED = True\n",
    "\n",
    "# Weight quant common\n",
    "STATS_OP = StatsOp.MEAN_LEARN_SIGMA_STD\n",
    "BIAS_ENABLED = False\n",
    "WEIGHT_SCALING_IMPL_TYPE = ScalingImplType.STATS\n",
    "SIGMA = 0.001\n",
    "\n",
    "# QuantHardTanh configuration\n",
    "HARD_TANH_MIN = -1.0\n",
    "HARD_TANH_MAX = 1.0\n",
    "ACT_PER_OUT_CH_SCALING = False\n",
    "\n",
    "# QuantConv2d configuration\n",
    "CONV_PER_OUT_CH_SCALING = True\n",
    "\n",
    "def get_stats_op(quant_type):\n",
    "    if quant_type == QuantType.BINARY:\n",
    "        return StatsOp.AVE\n",
    "    else:\n",
    "        return StatsOp.MAX\n",
    "\n",
    "\n",
    "def get_quant_type(bit_width):\n",
    "    if bit_width is None:\n",
    "        return QuantType.FP\n",
    "    elif bit_width == 1:\n",
    "        return QuantType.BINARY\n",
    "    else:\n",
    "        return QuantType.INT\n",
    "\n",
    "\n",
    "def get_act_quant(act_bit_width, act_quant_type):\n",
    "    if act_quant_type == QuantType.INT:\n",
    "        act_scaling_impl_type = ScalingImplType.PARAMETER\n",
    "    else:\n",
    "        act_scaling_impl_type = ScalingImplType.CONST\n",
    "    return QuantHardTanh(quant_type=act_quant_type,\n",
    "                         bit_width=act_bit_width,\n",
    "                         bit_width_impl_type=BIT_WIDTH_IMPL_TYPE,\n",
    "                         min_val=HARD_TANH_MIN,\n",
    "                         max_val=HARD_TANH_MAX,\n",
    "                         scaling_impl_type=act_scaling_impl_type,\n",
    "                         restrict_scaling_type=SCALING_VALUE_TYPE,\n",
    "                         scaling_per_channel=ACT_PER_OUT_CH_SCALING,\n",
    "                         narrow_range=NARROW_RANGE_ENABLED)\n",
    "\n",
    "\n",
    "def get_quant_linear(in_features, out_features, per_out_ch_scaling, bit_width, quant_type, stats_op):\n",
    "    return QuantLinear(bias=BIAS_ENABLED,\n",
    "                       in_features=in_features,\n",
    "                       out_features=out_features,\n",
    "                       weight_quant_type=quant_type,\n",
    "                       weight_narrow_range=NARROW_RANGE_ENABLED,\n",
    "                       weight_bit_width=bit_width,\n",
    "                       weight_bit_width_impl_type=BIT_WIDTH_IMPL_TYPE,\n",
    "                       weight_scaling_per_output_channel=per_out_ch_scaling,\n",
    "                       weight_scaling_stats_op=stats_op,\n",
    "                       weight_scaling_stats_sigma=SIGMA)\n",
    "\n",
    "\n",
    "def get_quant_conv2d(in_ch, out_ch, bit_width, quant_type, stats_op):\n",
    "    return QuantConv2d(in_channels=in_ch,\n",
    "                       kernel_size=KERNEL_SIZE,\n",
    "                       out_channels=out_ch,\n",
    "                       weight_quant_type=quant_type,\n",
    "                       weight_bit_width=bit_width,\n",
    "                       weight_narrow_range=NARROW_RANGE_ENABLED,\n",
    "                       weight_scaling_impl_type=WEIGHT_SCALING_IMPL_TYPE,\n",
    "                       weight_scaling_stats_op=stats_op,\n",
    "                       weight_scaling_stats_sigma=SIGMA,\n",
    "                       weight_scaling_per_output_channel=CONV_PER_OUT_CH_SCALING,\n",
    "                       weight_restrict_scaling_type=SCALING_VALUE_TYPE,\n",
    "                       weight_bit_width_impl_type=BIT_WIDTH_IMPL_TYPE,\n",
    "                       bias=BIAS_ENABLED)\n",
    "\n",
    "def get_quant_conv1d(in_ch, out_ch, bit_width, quant_type, stats_op):\n",
    "    return QuantConv1d(in_channels=in_ch,\n",
    "                       kernel_size=KERNEL_SIZE,\n",
    "                       out_channels=out_ch,\n",
    "                       weight_quant_type=quant_type,\n",
    "                       weight_bit_width=bit_width,\n",
    "                       weight_narrow_range=NARROW_RANGE_ENABLED,\n",
    "                       weight_scaling_impl_type=WEIGHT_SCALING_IMPL_TYPE,\n",
    "                       weight_scaling_stats_op=stats_op,\n",
    "                       weight_scaling_stats_sigma=SIGMA,\n",
    "                       weight_scaling_per_output_channel=CONV_PER_OUT_CH_SCALING,\n",
    "                       weight_restrict_scaling_type=SCALING_VALUE_TYPE,\n",
    "                       weight_bit_width_impl_type=BIT_WIDTH_IMPL_TYPE,\n",
    "                       bias=BIAS_ENABLED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the classes needed for the CNN, taken from: https://github.com/maltanar/brevitas_cnv_lfc\n",
    "# this is where the pre-trained models also come from, however, we will import the whole thing here to make custom CNNs\n",
    "class CNV(Module):\n",
    "\n",
    "    def __init__(self, num_classes=10, weight_bit_width=None, act_bit_width=None,in_bit_width=None, in_ch=3, device=\"cpu\"):\n",
    "        super(CNV, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        weight_quant_type = get_quant_type(weight_bit_width)\n",
    "        act_quant_type = get_quant_type(act_bit_width)\n",
    "        in_quant_type = get_quant_type(in_bit_width)\n",
    "        stats_op = get_stats_op(weight_quant_type)\n",
    "\n",
    "        self.conv_features = ModuleList()\n",
    "        self.linear_features = ModuleList()\n",
    "        self.conv_features.append(get_act_quant(in_bit_width, in_quant_type))\n",
    "\n",
    "        # convolution layers\n",
    "        for i, out_ch, is_pool_enabled in CNV_OUT_CH_POOL:\n",
    "            self.conv_features.append(get_quant_conv1d(in_ch=in_ch,\n",
    "                                                       out_ch=out_ch,\n",
    "                                                       bit_width=weight_bit_width,\n",
    "                                                       quant_type=weight_quant_type,\n",
    "                                                       stats_op=stats_op))\n",
    "            in_ch = out_ch\n",
    "            self.conv_features.append(BatchNorm1d(in_ch))\n",
    "            if i == (NUM_CONV_LAYERS - 1):\n",
    "                self.conv_features.append(Sequential())\n",
    "            else:\n",
    "                self.conv_features.append(get_act_quant(act_bit_width, act_quant_type))\n",
    "            if is_pool_enabled:\n",
    "                self.conv_features.append(MaxPool1d(kernel_size=2))\n",
    "\n",
    "        # fully connected layers\n",
    "        self.linear_features.append(get_act_quant(in_bit_width, in_quant_type))\n",
    "        \n",
    "        for in_features, out_features in INTERMEDIATE_FC_FEATURES:\n",
    "            self.linear_features.append(get_quant_linear(in_features=in_features,\n",
    "                                                         out_features=out_features,\n",
    "                                                         per_out_ch_scaling=INTERMEDIATE_FC_PER_OUT_CH_SCALING,\n",
    "                                                         bit_width=weight_bit_width,\n",
    "                                                         quant_type=weight_quant_type,\n",
    "                                                         stats_op=stats_op))\n",
    "            self.linear_features.append(BatchNorm1d(out_features))\n",
    "            self.linear_features.append(get_act_quant(act_bit_width, act_quant_type))\n",
    "            \n",
    "        # last layer\n",
    "        self.fc = get_quant_linear(in_features=LAST_FC_IN_FEATURES,\n",
    "                                   out_features=num_classes,\n",
    "                                   per_out_ch_scaling=LAST_FC_PER_OUT_CH_SCALING,\n",
    "                                   bit_width=weight_bit_width,\n",
    "                                   quant_type=weight_quant_type,\n",
    "                                   stats_op=stats_op)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = 2.0 * x - torch.tensor([1.0]).to(self.device)\n",
    "        for mod in self.conv_features:\n",
    "            x = mod(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        for mod in self.linear_features:\n",
    "            x = mod(x)\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        #out = self.fc_bn(x)\n",
    "        return out\n",
    "\n",
    "# will not be used as we wont use cfg\n",
    "def cnv(cfg):\n",
    "    weight_bit_width = cfg.getint('QUANT', 'WEIGHT_BIT_WIDTH')\n",
    "    act_bit_width = cfg.getint('QUANT', 'ACT_BIT_WIDTH')\n",
    "    in_bit_width = cfg.getint('QUANT', 'IN_BIT_WIDTH')\n",
    "    num_classes = cfg.getint('MODEL', 'NUM_CLASSES')\n",
    "    in_channels = cfg.getint('MODEL', 'IN_CHANNELS')\n",
    "    net = CNV(weight_bit_width=weight_bit_width,\n",
    "              act_bit_width=act_bit_width,\n",
    "              in_bit_width=in_bit_width,\n",
    "              num_classes=num_classes,\n",
    "              in_ch=in_channels)\n",
    "    return net\n",
    "\n",
    "def cnv_manual(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS):\n",
    "    net = CNV(weight_bit_width=WEIGHT_BIT_WIDTH,\n",
    "              act_bit_width=ACT_BIT_WIDTH,\n",
    "              in_bit_width=IN_BIT_WIDTH,\n",
    "              num_classes=NUM_CLASSES,\n",
    "              in_ch=IN_CHANNELS)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNV(\n",
      "  (conv_features): ModuleList(\n",
      "    (0): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): RescalingIntQuant(\n",
      "            (int_quant): IntQuant(\n",
      "              (float_to_int_impl): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): RoundSte()\n",
      "                  (1): Identity()\n",
      "                )\n",
      "              )\n",
      "              (tensor_clamp_impl): TensorClamp()\n",
      "            )\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (int_scaling_impl): IntScaling(\n",
      "              (forward_impl): SignedFpIntScale()\n",
      "            )\n",
      "            (msb_clamp_bit_width_impl): BitWidthConst()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): QuantConv1d(\n",
      "      2, 32, kernel_size=(4,), stride=(1,), bias=False\n",
      "      (weight_reg): WeightReg()\n",
      "      (weight_quant): WeightQuantProxy(\n",
      "        (tensor_quant): BinaryQuant(\n",
      "          (scaling_impl): ParameterStatsScaling(\n",
      "            (parameter_list_stats): ParameterListStats(\n",
      "              (first_tracked_param): _ViewParameterWrapper()\n",
      "              (stats): Stats(\n",
      "                (stats_impl): AbsAve()\n",
      "              )\n",
      "            )\n",
      "            (stats_scaling_impl): StatsScaling(\n",
      "              (affine_rescaling): Identity()\n",
      "              (restrict_scaling): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "              (restrict_scaling_preprocess): LogTwo()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bias_quant): BiasQuantProxy()\n",
      "    )\n",
      "    (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): ClampedBinaryQuant(\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): QuantConv1d(\n",
      "      32, 64, kernel_size=(4,), stride=(1,), bias=False\n",
      "      (weight_reg): WeightReg()\n",
      "      (weight_quant): WeightQuantProxy(\n",
      "        (tensor_quant): BinaryQuant(\n",
      "          (scaling_impl): ParameterStatsScaling(\n",
      "            (parameter_list_stats): ParameterListStats(\n",
      "              (first_tracked_param): _ViewParameterWrapper()\n",
      "              (stats): Stats(\n",
      "                (stats_impl): AbsAve()\n",
      "              )\n",
      "            )\n",
      "            (stats_scaling_impl): StatsScaling(\n",
      "              (affine_rescaling): Identity()\n",
      "              (restrict_scaling): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "              (restrict_scaling_preprocess): LogTwo()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bias_quant): BiasQuantProxy()\n",
      "    )\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): ClampedBinaryQuant(\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): QuantConv1d(\n",
      "      64, 128, kernel_size=(4,), stride=(1,), bias=False\n",
      "      (weight_reg): WeightReg()\n",
      "      (weight_quant): WeightQuantProxy(\n",
      "        (tensor_quant): BinaryQuant(\n",
      "          (scaling_impl): ParameterStatsScaling(\n",
      "            (parameter_list_stats): ParameterListStats(\n",
      "              (first_tracked_param): _ViewParameterWrapper()\n",
      "              (stats): Stats(\n",
      "                (stats_impl): AbsAve()\n",
      "              )\n",
      "            )\n",
      "            (stats_scaling_impl): StatsScaling(\n",
      "              (affine_rescaling): Identity()\n",
      "              (restrict_scaling): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "              (restrict_scaling_preprocess): LogTwo()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bias_quant): BiasQuantProxy()\n",
      "    )\n",
      "    (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Sequential()\n",
      "    (11): QuantConv1d(\n",
      "      128, 256, kernel_size=(4,), stride=(1,), bias=False\n",
      "      (weight_reg): WeightReg()\n",
      "      (weight_quant): WeightQuantProxy(\n",
      "        (tensor_quant): BinaryQuant(\n",
      "          (scaling_impl): ParameterStatsScaling(\n",
      "            (parameter_list_stats): ParameterListStats(\n",
      "              (first_tracked_param): _ViewParameterWrapper()\n",
      "              (stats): Stats(\n",
      "                (stats_impl): AbsAve()\n",
      "              )\n",
      "            )\n",
      "            (stats_scaling_impl): StatsScaling(\n",
      "              (affine_rescaling): Identity()\n",
      "              (restrict_scaling): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "              (restrict_scaling_preprocess): LogTwo()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bias_quant): BiasQuantProxy()\n",
      "    )\n",
      "    (12): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): ClampedBinaryQuant(\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_features): ModuleList(\n",
      "    (0): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): RescalingIntQuant(\n",
      "            (int_quant): IntQuant(\n",
      "              (float_to_int_impl): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): RoundSte()\n",
      "                  (1): Identity()\n",
      "                )\n",
      "              )\n",
      "              (tensor_clamp_impl): TensorClamp()\n",
      "            )\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (int_scaling_impl): IntScaling(\n",
      "              (forward_impl): SignedFpIntScale()\n",
      "            )\n",
      "            (msb_clamp_bit_width_impl): BitWidthConst()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): QuantLinear(\n",
      "      in_features=256, out_features=128, bias=False\n",
      "      (weight_reg): WeightReg()\n",
      "      (weight_quant): WeightQuantProxy(\n",
      "        (tensor_quant): BinaryQuant(\n",
      "          (scaling_impl): ParameterStatsScaling(\n",
      "            (parameter_list_stats): ParameterListStats(\n",
      "              (first_tracked_param): _ViewParameterWrapper()\n",
      "              (stats): Stats(\n",
      "                (stats_impl): AbsAve()\n",
      "              )\n",
      "            )\n",
      "            (stats_scaling_impl): StatsScaling(\n",
      "              (affine_rescaling): Identity()\n",
      "              (restrict_scaling): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "              (restrict_scaling_preprocess): LogTwo()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bias_quant): BiasQuantProxy()\n",
      "    )\n",
      "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): ClampedBinaryQuant(\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): QuantLinear(\n",
      "    in_features=128, out_features=3, bias=False\n",
      "    (weight_reg): WeightReg()\n",
      "    (weight_quant): WeightQuantProxy(\n",
      "      (tensor_quant): BinaryQuant(\n",
      "        (scaling_impl): ParameterStatsScaling(\n",
      "          (parameter_list_stats): ParameterListStats(\n",
      "            (first_tracked_param): _ViewParameterWrapper()\n",
      "            (stats): Stats(\n",
      "              (stats_impl): AbsAve()\n",
      "            )\n",
      "          )\n",
      "          (stats_scaling_impl): StatsScaling(\n",
      "            (affine_rescaling): Identity()\n",
      "            (restrict_scaling): RestrictValue(\n",
      "              (forward_impl): Sequential(\n",
      "                (0): PowerOfTwo()\n",
      "                (1): ClampMin()\n",
      "              )\n",
      "            )\n",
      "            (restrict_scaling_preprocess): LogTwo()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bias_quant): BiasQuantProxy()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create the model and export\n",
    "bnn_pynq_model = cnv_manual(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS)\n",
    "#bo.export_finn_onnx(bnn_pynq_model, INPUT_SPECIFICATIONS, build_dir + file_name + \".onnx\")\n",
    "\n",
    "# print for reference\n",
    "print(bnn_pynq_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Training the Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5159, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the data from csv\n",
    "df = pd.read_csv('dataset_24_inputs.csv')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([172, 2, 24])\n",
      "torch.Size([42, 2, 24])\n"
     ]
    }
   ],
   "source": [
    "def parse_input(df, starting_index):\n",
    "    data = np.array(((df.iloc[starting_index, 0],df.iloc[starting_index + 1,0],df.iloc[starting_index + 2,0],df.iloc[starting_index + 3,0],df.iloc[starting_index + 4,0],df.iloc[starting_index + 5,0],df.iloc[starting_index + 6,0],df.iloc[starting_index + 7,0],\n",
    "                  df.iloc[starting_index + 8, 0], df.iloc[starting_index + 9, 0], df.iloc[starting_index + 10, 0], df.iloc[starting_index + 11, 0], df.iloc[starting_index + 12, 0], df.iloc[starting_index + 13, 0], df.iloc[starting_index + 14, 0], df.iloc[starting_index + 15, 0],\n",
    "                  df.iloc[starting_index + 16, 0], df.iloc[starting_index + 17, 0], df.iloc[starting_index + 18, 0], df.iloc[starting_index + 19, 0], df.iloc[starting_index + 20, 0], df.iloc[starting_index + 21, 0], df.iloc[starting_index + 22, 0], df.iloc[starting_index + 23, 0]),\n",
    "                 (df.iloc[starting_index, 1],df.iloc[starting_index + 1, 1],df.iloc[starting_index + 2, 1],df.iloc[starting_index + 3, 1],df.iloc[starting_index + 4, 1],df.iloc[starting_index + 5, 1],df.iloc[starting_index + 6, 1],df.iloc[starting_index + 7, 1], \n",
    "                  df.iloc[starting_index + 8, 1], df.iloc[starting_index + 9, 1], df.iloc[starting_index + 10, 1], df.iloc[starting_index + 11, 1], df.iloc[starting_index + 12, 1], df.iloc[starting_index + 13, 1], df.iloc[starting_index + 14, 1], df.iloc[starting_index + 15, 1],\n",
    "                  df.iloc[starting_index + 16, 1], df.iloc[starting_index + 17, 1], df.iloc[starting_index + 18, 1], df.iloc[starting_index + 19, 1], df.iloc[starting_index + 20, 1], df.iloc[starting_index + 21, 1], df.iloc[starting_index + 22, 1], df.iloc[starting_index + 23, 1])))\n",
    "    \"\"\"\n",
    "    data = np.array(((df.iloc[starting_index, 0],df.iloc[starting_index + 1,0],df.iloc[starting_index + 2,0],df.iloc[starting_index + 3,0],df.iloc[starting_index + 4,0],df.iloc[starting_index + 5,0],df.iloc[starting_index + 6,0],df.iloc[starting_index + 7,0],\n",
    "                  df.iloc[starting_index + 8, 0], df.iloc[starting_index + 9, 0], df.iloc[starting_index + 10, 0], df.iloc[starting_index + 11, 0], df.iloc[starting_index + 12, 0], df.iloc[starting_index + 13, 0], df.iloc[starting_index + 14, 0], df.iloc[starting_index + 15, 0]),\n",
    "                 (df.iloc[starting_index, 1],df.iloc[starting_index + 1, 1],df.iloc[starting_index + 2, 1],df.iloc[starting_index + 3, 1],df.iloc[starting_index + 4, 1],df.iloc[starting_index + 5, 1],df.iloc[starting_index + 6, 1],df.iloc[starting_index + 7, 1], \n",
    "                  df.iloc[starting_index + 8, 1], df.iloc[starting_index + 9, 1], df.iloc[starting_index + 10, 1], df.iloc[starting_index + 11, 1], df.iloc[starting_index + 12, 1], df.iloc[starting_index + 13, 1], df.iloc[starting_index + 14, 1], df.iloc[starting_index + 15, 1])))\n",
    "    \"\"\"\n",
    "    return data\n",
    "\n",
    "# 20% set aside for testing\n",
    "num_items = (df.shape[0]) // 24\n",
    "max_data = 900\n",
    "batch_size = 2\n",
    "\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "x_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for i in range(num_items):\n",
    "    if i == max_data:\n",
    "        break\n",
    "    starting_index = i * 24\n",
    "    \n",
    "    # 16 inputs\n",
    "    data = parse_input(df, starting_index)\n",
    "    \n",
    "    # do encoding, go by index as shown below\n",
    "    if 'zigzag' in df.iloc[starting_index, 5]:\n",
    "        value = (0)\n",
    "    elif 'rocket' in df.iloc[starting_index, 5]:\n",
    "        value = (1)\n",
    "    elif 'hair' in df.iloc[starting_index, 5]:\n",
    "        value = (2)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    if i % 5 != 4: # training\n",
    "        x_train_list.append(data)\n",
    "        y_train_list.append(value) \n",
    "    else: # testing\n",
    "        x_test_list.append(data)\n",
    "        y_test_list.append(value)\n",
    "        \n",
    "# remove extra inputs that cannot fit in batch_size\n",
    "while len(x_train_list) % batch_size != 0:\n",
    "    x_train_list.pop()\n",
    "    y_train_list.pop()\n",
    "    \n",
    "while len(x_test_list) % batch_size != 0:\n",
    "    x_test_list.pop()\n",
    "    y_test_list.pop()\n",
    "        \n",
    "# transform to PyTorch DataLoader\n",
    "tensor_x_train = torch.Tensor(x_train_list) # transform to torch tensor\n",
    "tensor_y_train = torch.Tensor(y_train_list)\n",
    "    \n",
    "print(tensor_x_train.size())\n",
    "\n",
    "dataset_train = TensorDataset(tensor_x_train,tensor_y_train)\n",
    "train_loader = DataLoader(dataset_train, batch_size = batch_size, shuffle = True, num_workers = 1)\n",
    "\n",
    "tensor_x_test = torch.Tensor(x_test_list) # transform to torch tensor\n",
    "tensor_y_test = torch.Tensor(y_test_list)\n",
    "    \n",
    "print(tensor_x_test.size())\n",
    "\n",
    "dataset_test = TensorDataset(tensor_x_test,tensor_y_test)\n",
    "test_loader = DataLoader(dataset_test, batch_size = batch_size, shuffle = True, num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 24])\n"
     ]
    }
   ],
   "source": [
    "# validate input data size\n",
    "for data, target in train_loader:\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for training\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.1\n",
    "WEIGHT_DECAY = 0.0001\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "log_freq = 10\n",
    "\n",
    "EPOCHS = 25 # we'll see how long it takes first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show statistics\n",
    "num_inputs_train_stats = 0\n",
    "num_inputs_test_stats = 0\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def get_prediction(output_tensor):\n",
    "    prediction_list = softmax(output_tensor.tolist())\n",
    "    max_val = -1\n",
    "    prediction = 0\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] > max_val:\n",
    "            max_val = prediction_list[i]\n",
    "            prediction = i\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def test_on_everything(print_output):\n",
    "\n",
    "    list_input_train_total = [0] * NUM_CLASSES\n",
    "    list_input_train_correct_total =  [0] * NUM_CLASSES\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        (input, target) = data\n",
    "        input = input.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        output = bnn_pynq_model(input)\n",
    "        #print(target, output)\n",
    "\n",
    "        for i in range(len(output)):\n",
    "            if print_output:\n",
    "                print(softmax(output[i].tolist()), \"prediction\", get_prediction(output[i]), \"target\", target[i].tolist())\n",
    "\n",
    "            list_input_train_total[int(target[i].tolist())] += 1\n",
    "\n",
    "            if int(target[i].tolist()) == get_prediction(output[i]):\n",
    "                list_input_train_correct_total[int(target[i].tolist())] += 1\n",
    "\n",
    "    for i, data in enumerate(test_loader):\n",
    "        (input, target) = data\n",
    "        input = input.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        output = bnn_pynq_model(input)\n",
    "\n",
    "        for i in range(len(output)):\n",
    "            if print_output:\n",
    "                print(softmax(output[i].tolist()), \"prediction\", get_prediction(output[i]), \"target\", target[i].tolist())\n",
    "\n",
    "            list_input_train_total[int(target[i].tolist())] += 1\n",
    "\n",
    "            if int(target[i].tolist()) == get_prediction(output[i]):\n",
    "                list_input_train_correct_total[int(target[i].tolist())] += 1    \n",
    "\n",
    "    print(\"number of inputs of each class\", list_input_train_total)\n",
    "    print(\"correctly predicted\", list_input_train_correct_total)\n",
    "\n",
    "    running_total = 0\n",
    "    running_total_correct = 0\n",
    "    for i in range(len(list_input_train_total)):\n",
    "        print(\"correctly predicted class %d at %.3f\" % ((i + 1), (list_input_train_correct_total[i] * 100 / list_input_train_total[i])))\n",
    "        running_total += list_input_train_total[i]\n",
    "        running_total_correct += list_input_train_correct_total[i]\n",
    "    \n",
    "    print(\"accuracy at %.3f\" % ((running_total_correct * 100 / running_total)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' # linux CUDA not set up yet\n",
    "\n",
    "# training stuff\n",
    "# like the CNN model, it is taken from https://github.com/maltanar/brevitas_cnv_lfc\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class TrainingEpochMeters(object):\n",
    "    def __init__(self):\n",
    "        self.batch_time = AverageMeter()\n",
    "        self.data_time = AverageMeter()\n",
    "        self.losses = AverageMeter()\n",
    "        self.top1 = AverageMeter()\n",
    "        self.top5 = AverageMeter()\n",
    "\n",
    "class EvalEpochMeters(object):\n",
    "    def __init__(self):\n",
    "        self.model_time = AverageMeter()\n",
    "        self.loss_time = AverageMeter()\n",
    "        self.losses = AverageMeter()\n",
    "        self.top1 = AverageMeter()\n",
    "        self.top5 = AverageMeter()\n",
    "\n",
    "def eval_model(epoch=None):\n",
    "    eval_meters = EvalEpochMeters()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    bnn_pynq_model.eval()\n",
    "    criterion.eval()\n",
    "\n",
    "    for i, data in enumerate(test_loader):\n",
    "\n",
    "        end = time.time()\n",
    "        (input, target) = data\n",
    "\n",
    "        input = input.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        # compute output\n",
    "        output = bnn_pynq_model(input)\n",
    "\n",
    "        # measure model elapsed time\n",
    "        eval_meters.model_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        #compute loss\n",
    "        loss = criterion(output, target.long())\n",
    "        eval_meters.loss_time.update(time.time() - end)\n",
    "\n",
    "        prec1, prec5 = accuracy(output, target, topk=(1, NUM_CLASSES))\n",
    "        eval_meters.losses.update(loss.item(), input.size(0))\n",
    "        eval_meters.top1.update(prec1.item(), input.size(0))\n",
    "        eval_meters.top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred).long())\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def softmax_train(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def get_prediction_train(output_tensor):\n",
    "    prediction_list = softmax_train(output_tensor.tolist())\n",
    "    max_val = -1\n",
    "    prediction = 0\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] > max_val:\n",
    "            max_val = prediction_list[i]\n",
    "            prediction = i\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# returns the number of correct predictions\n",
    "def compare_output(output, target):\n",
    "    running_total = 0\n",
    "    \n",
    "    # iterate for each item in \n",
    "    for i in range(len(output)):\n",
    "        if get_prediction_train(output[i]) == int(target.tolist()[i]):\n",
    "            running_total += 1\n",
    "    \n",
    "    return running_total\n",
    "        \n",
    "def train_model():\n",
    "    optimizer = optim.SGD(bnn_pynq_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "    # training starts\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # keep track of training / testing metrics\n",
    "    num_inputs_training = len(train_loader) * batch_size\n",
    "    num_inputs_testing = len(test_loader) * batch_size\n",
    "        \n",
    "    for epoch in range(EPOCHS):\n",
    "        # keep track of training / testing metrics\n",
    "        running_loss_training = 0.0\n",
    "        running_loss_testing = 0.0\n",
    "        num_inputs_training_correct = 0\n",
    "        num_inputs_testing_correct = 0\n",
    "\n",
    "        # Set to training mode\n",
    "        bnn_pynq_model.train()\n",
    "        criterion.train()\n",
    "\n",
    "        # Init metrics\n",
    "        epoch_meters = TrainingEpochMeters()\n",
    "        start_data_loading = time.time()\n",
    "        \n",
    "        # train\n",
    "        for i, data in enumerate(train_loader):\n",
    "            (input, target) = data\n",
    "            #print(\"input\", input)\n",
    "            #print(\"target\", target)\n",
    "            \n",
    "            input = input.to(device, non_blocking=True)\n",
    "            target = target.to(device, non_blocking=True)\n",
    "            \n",
    "            # measure data loading time\n",
    "            epoch_meters.data_time.update(time.time() - start_data_loading)\n",
    "\n",
    "            # Training batch starts\n",
    "            start_batch = time.time()\n",
    "            output = bnn_pynq_model(input)\n",
    "            loss = criterion(output, target.long())\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # measure elapsed time\n",
    "            epoch_meters.batch_time.update(time.time() - start_batch)\n",
    "            if i % int(log_freq) == 0 or i == len(train_loader) - 1:\n",
    "                # set the third value of topk depending on the number of classes\n",
    "                prec1, prec5 = accuracy(output.detach(), target, topk=(1, NUM_CLASSES))\n",
    "                epoch_meters.losses.update(loss.item(), input.size(0))\n",
    "                epoch_meters.top1.update(prec1.item(), input.size(0))\n",
    "                epoch_meters.top5.update(prec5.item(), input.size(0))\n",
    "                #self.logger.training_batch_cli_log(epoch_meters, epoch, i, len(self.train_loader))\n",
    "                \n",
    "            # update loss and accuracy\n",
    "            running_loss_training += loss.item()\n",
    "            num_inputs_training_correct += compare_output(output, target.long())\n",
    "\n",
    "            # training batch ends\n",
    "            #start_data_loading = time.time()\n",
    "            \n",
    "        # validate\n",
    "        running_loss_training = 0.0\n",
    "        num_inputs_training_correct = 0\n",
    "        for j, data in enumerate(train_loader):\n",
    "            (input, target) = data\n",
    "            \n",
    "            input = input.to(device, non_blocking=True)\n",
    "            target = target.to(device, non_blocking=True)\n",
    "            \n",
    "            output = bnn_pynq_model(input)\n",
    "            loss = criterion(output, target.long())\n",
    "        \n",
    "            # update loss and accuracy\n",
    "            running_loss_training += loss.item()\n",
    "            \n",
    "            for i in range(len(output)):\n",
    "                if int(target[i].tolist()) == get_prediction_train(output[i]):\n",
    "                    num_inputs_training_correct += 1\n",
    "            #num_inputs_training_correct += compare_output(output, target.long())\n",
    "            \n",
    "        for j, data in enumerate(test_loader):\n",
    "            (input, target) = data\n",
    "            \n",
    "            input = input.to(device, non_blocking=True)\n",
    "            target = target.to(device, non_blocking=True)\n",
    "            \n",
    "            output = bnn_pynq_model(input)\n",
    "            loss = criterion(output, target.long())\n",
    "        \n",
    "            # update loss and accuracy\n",
    "            running_loss_testing += loss.item()\n",
    "            \n",
    "            for i in range(len(output)):\n",
    "                if int(target[i].tolist()) == get_prediction_train(output[i]):\n",
    "                    num_inputs_testing_correct += 1\n",
    "            #num_inputs_testing_correct += compare_output(output, target.long())\n",
    "            \n",
    "        print('Epoch [%d] (took %.3f) training loss: %.3f, accuracy: %.3f, validation loss: %.3f, accuracy: %.3f' \n",
    "              % (epoch + 1, time.time() - start_data_loading, running_loss_training / num_inputs_training, num_inputs_training_correct / num_inputs_training, \n",
    "                running_loss_testing / num_inputs_testing, num_inputs_testing_correct / num_inputs_testing))\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Perform eval\n",
    "        with torch.no_grad():\n",
    "            top1avg = eval_model(epoch)\n",
    "            \n",
    "            \n",
    "        test_on_everything(print_output = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] (took 17.473) training loss: 0.502, accuracy: 0.529, validation loss: 0.487, accuracy: 0.548\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [17, 61, 62]\n",
      "correctly predicted class 1 at 26.154\n",
      "correctly predicted class 2 at 73.494\n",
      "correctly predicted class 3 at 93.939\n",
      "accuracy at 65.421\n",
      "Epoch [2] (took 16.017) training loss: 0.440, accuracy: 0.523, validation loss: 0.395, accuracy: 0.619\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [19, 77, 57]\n",
      "correctly predicted class 1 at 29.231\n",
      "correctly predicted class 2 at 92.771\n",
      "correctly predicted class 3 at 86.364\n",
      "accuracy at 71.495\n",
      "Epoch [3] (took 16.470) training loss: 0.503, accuracy: 0.494, validation loss: 0.403, accuracy: 0.524\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [30, 80, 63]\n",
      "correctly predicted class 1 at 46.154\n",
      "correctly predicted class 2 at 96.386\n",
      "correctly predicted class 3 at 95.455\n",
      "accuracy at 80.841\n",
      "Epoch [4] (took 17.261) training loss: 0.432, accuracy: 0.558, validation loss: 0.448, accuracy: 0.476\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [29, 81, 60]\n",
      "correctly predicted class 1 at 44.615\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 90.909\n",
      "accuracy at 79.439\n",
      "Epoch [5] (took 18.611) training loss: 0.446, accuracy: 0.564, validation loss: 0.458, accuracy: 0.452\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [29, 81, 38]\n",
      "correctly predicted class 1 at 44.615\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 57.576\n",
      "accuracy at 69.159\n",
      "Epoch [6] (took 22.781) training loss: 0.415, accuracy: 0.564, validation loss: 0.358, accuracy: 0.619\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [18, 81, 53]\n",
      "correctly predicted class 1 at 27.692\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 80.303\n",
      "accuracy at 71.028\n",
      "Epoch [7] (took 22.400) training loss: 0.505, accuracy: 0.576, validation loss: 0.468, accuracy: 0.571\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [25, 81, 55]\n",
      "correctly predicted class 1 at 38.462\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 83.333\n",
      "accuracy at 75.234\n",
      "Epoch [8] (took 18.153) training loss: 0.495, accuracy: 0.506, validation loss: 0.495, accuracy: 0.452\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [36, 81, 44]\n",
      "correctly predicted class 1 at 55.385\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 66.667\n",
      "accuracy at 75.234\n",
      "Epoch [9] (took 19.696) training loss: 0.416, accuracy: 0.605, validation loss: 0.451, accuracy: 0.524\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [26, 81, 50]\n",
      "correctly predicted class 1 at 40.000\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 75.758\n",
      "accuracy at 73.364\n",
      "Epoch [10] (took 18.567) training loss: 0.436, accuracy: 0.599, validation loss: 0.419, accuracy: 0.571\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [23, 81, 61]\n",
      "correctly predicted class 1 at 35.385\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 92.424\n",
      "accuracy at 77.103\n",
      "Epoch [11] (took 19.201) training loss: 0.420, accuracy: 0.576, validation loss: 0.445, accuracy: 0.571\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [31, 81, 44]\n",
      "correctly predicted class 1 at 47.692\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 66.667\n",
      "accuracy at 72.897\n",
      "Epoch [12] (took 18.363) training loss: 0.403, accuracy: 0.616, validation loss: 0.406, accuracy: 0.643\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [36, 81, 35]\n",
      "correctly predicted class 1 at 55.385\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 53.030\n",
      "accuracy at 71.028\n",
      "Epoch [13] (took 17.334) training loss: 0.408, accuracy: 0.587, validation loss: 0.504, accuracy: 0.357\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [45, 82, 33]\n",
      "correctly predicted class 1 at 69.231\n",
      "correctly predicted class 2 at 98.795\n",
      "correctly predicted class 3 at 50.000\n",
      "accuracy at 74.766\n",
      "Epoch [14] (took 16.529) training loss: 0.411, accuracy: 0.558, validation loss: 0.418, accuracy: 0.452\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [51, 81, 21]\n",
      "correctly predicted class 1 at 78.462\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 31.818\n",
      "accuracy at 71.495\n",
      "Epoch [15] (took 15.047) training loss: 0.419, accuracy: 0.576, validation loss: 0.448, accuracy: 0.524\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [36, 82, 39]\n",
      "correctly predicted class 1 at 55.385\n",
      "correctly predicted class 2 at 98.795\n",
      "correctly predicted class 3 at 59.091\n",
      "accuracy at 73.364\n",
      "Epoch [16] (took 16.392) training loss: 0.432, accuracy: 0.570, validation loss: 0.444, accuracy: 0.548\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [53, 81, 8]\n",
      "correctly predicted class 1 at 81.538\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 12.121\n",
      "accuracy at 66.355\n",
      "Epoch [17] (took 15.266) training loss: 0.421, accuracy: 0.541, validation loss: 0.410, accuracy: 0.571\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [53, 80, 7]\n",
      "correctly predicted class 1 at 81.538\n",
      "correctly predicted class 2 at 96.386\n",
      "correctly predicted class 3 at 10.606\n",
      "accuracy at 65.421\n",
      "Epoch [18] (took 15.497) training loss: 0.422, accuracy: 0.541, validation loss: 0.441, accuracy: 0.524\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [47, 81, 28]\n",
      "correctly predicted class 1 at 72.308\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 42.424\n",
      "accuracy at 72.897\n",
      "Epoch [19] (took 15.996) training loss: 0.416, accuracy: 0.552, validation loss: 0.369, accuracy: 0.714\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [24, 81, 59]\n",
      "correctly predicted class 1 at 36.923\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 89.394\n",
      "accuracy at 76.636\n",
      "Epoch [20] (took 15.227) training loss: 0.416, accuracy: 0.628, validation loss: 0.419, accuracy: 0.548\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [36, 81, 58]\n",
      "correctly predicted class 1 at 55.385\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 87.879\n",
      "accuracy at 81.776\n",
      "Epoch [21] (took 16.136) training loss: 0.397, accuracy: 0.605, validation loss: 0.560, accuracy: 0.333\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [38, 81, 48]\n",
      "correctly predicted class 1 at 58.462\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 72.727\n",
      "accuracy at 78.037\n",
      "Epoch [22] (took 16.112) training loss: 0.425, accuracy: 0.599, validation loss: 0.382, accuracy: 0.619\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [32, 81, 53]\n",
      "correctly predicted class 1 at 49.231\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 80.303\n",
      "accuracy at 77.570\n",
      "Epoch [23] (took 16.191) training loss: 0.445, accuracy: 0.547, validation loss: 0.389, accuracy: 0.667\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [30, 81, 53]\n",
      "correctly predicted class 1 at 46.154\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 80.303\n",
      "accuracy at 76.636\n",
      "Epoch [24] (took 16.165) training loss: 0.399, accuracy: 0.605, validation loss: 0.364, accuracy: 0.571\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [23, 81, 61]\n",
      "correctly predicted class 1 at 35.385\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 92.424\n",
      "accuracy at 77.103\n",
      "Epoch [25] (took 16.021) training loss: 0.451, accuracy: 0.535, validation loss: 0.348, accuracy: 0.595\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [34, 81, 53]\n",
      "correctly predicted class 1 at 52.308\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 80.303\n",
      "accuracy at 78.505\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "# to do: put up accuracy / loss\n",
    "\n",
    "if train_network:\n",
    "    bnn_pynq_model = cnv_manual(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS)\n",
    "    train_model()\n",
    "    #bo.export_finn_onnx(bnn_pynq_model, INPUT_SPECIFICATIONS, build_dir + file_name + \".onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bnn_pynq_model.state_dict(), build_dir + model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25632393 0.61909519 0.12458088] prediction 1 target 1.0\n",
      "[0.20007519 0.66592113 0.13400368] prediction 1 target 1.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.11899135 0.75208556 0.12892309] prediction 1 target 1.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 0.0\n",
      "[0.45449006 0.18817248 0.35733745] prediction 0 target 2.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "[0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "[0.12335408 0.77966024 0.09698568] prediction 1 target 1.0\n",
      "[0.42430648 0.24208759 0.33360593] prediction 0 target 0.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "[0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 0.0\n",
      "[0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "[0.44636572 0.25467345 0.29896083] prediction 0 target 0.0\n",
      "[0.21362728 0.07534558 0.71102714] prediction 2 target 0.0\n",
      "[0.2163782 0.613497  0.1701248] prediction 1 target 1.0\n",
      "[0.45449006 0.18817248 0.35733745] prediction 0 target 2.0\n",
      "[0.36885619 0.08042041 0.5507234 ] prediction 2 target 0.0\n",
      "[0.26912586 0.65001558 0.08085856] prediction 1 target 1.0\n",
      "[0.45449009 0.35733741 0.18817249] prediction 0 target 0.0\n",
      "[0.56938609 0.10575127 0.32486265] prediction 0 target 0.0\n",
      "[0.09634224 0.83913096 0.06452681] prediction 1 target 1.0\n",
      "[0.41511123 0.38313265 0.20175612] prediction 0 target 0.0\n",
      "[0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "[0.52971898 0.1154928  0.35478821] prediction 0 target 0.0\n",
      "[0.46707355 0.10183443 0.43109203] prediction 0 target 0.0\n",
      "[0.5863751  0.20681245 0.20681245] prediction 0 target 0.0\n",
      "[0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "[0.40040106 0.16577799 0.43382095] prediction 2 target 0.0\n",
      "[0.45449006 0.18817248 0.35733745] prediction 0 target 2.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.2163782 0.613497  0.1701248] prediction 1 target 1.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "[0.46752245 0.16489351 0.36758404] prediction 0 target 0.0\n",
      "[0.25122763 0.51689834 0.23187402] prediction 1 target 2.0\n",
      "[0.440685   0.08184778 0.47746722] prediction 2 target 2.0\n",
      "[0.43943292 0.40558067 0.15498641] prediction 0 target 0.0\n",
      "[0.2163782 0.613497  0.1701248] prediction 1 target 1.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "[0.53893963 0.10009649 0.36096388] prediction 0 target 0.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.62464946 0.18767527 0.18767527] prediction 0 target 0.0\n",
      "[0.39559774 0.10124969 0.50315257] prediction 2 target 0.0\n",
      "[0.48596971 0.06549779 0.4485325 ] prediction 0 target 0.0\n",
      "[0.56601866 0.2343486  0.19963274] prediction 0 target 0.0\n",
      "[0.38101396 0.13438225 0.48460379] prediction 2 target 0.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "[0.25632393 0.61909519 0.12458088] prediction 1 target 1.0\n",
      "[0.14378663 0.77417606 0.08203731] prediction 1 target 1.0\n",
      "[0.12694771 0.68351209 0.1895402 ] prediction 1 target 1.0\n",
      "[0.29218708 0.19569715 0.51211577] prediction 2 target 2.0\n",
      "[0.17931056 0.12009622 0.70059321] prediction 2 target 2.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.25632393 0.61909519 0.12458088] prediction 1 target 1.0\n",
      "[0.17564234 0.68626097 0.13809669] prediction 1 target 1.0\n",
      "[0.17931056 0.12009622 0.70059321] prediction 2 target 2.0\n",
      "[0.41048165 0.14477543 0.44474293] prediction 2 target 0.0\n",
      "[0.13685198 0.73683859 0.12630943] prediction 1 target 1.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.11001825 0.8162952  0.07368654] prediction 1 target 1.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 0.0\n",
      "[0.17931056 0.12009622 0.70059321] prediction 2 target 2.0\n",
      "[0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "[0.17564234 0.68626097 0.13809669] prediction 1 target 1.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.60490752 0.21334872 0.18174375] prediction 0 target 1.0\n",
      "[0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "[0.40689973 0.07557289 0.51752738] prediction 2 target 2.0\n",
      "[0.50388526 0.28749119 0.20862355] prediction 0 target 0.0\n",
      "[0.23533126 0.09743417 0.66723457] prediction 2 target 2.0\n",
      "[0.09634224 0.83913096 0.06452681] prediction 1 target 1.0\n",
      "[0.17564231 0.68626099 0.1380967 ] prediction 1 target 1.0\n",
      "[0.16692773 0.65221174 0.18086053] prediction 1 target 1.0\n",
      "[0.16664923 0.76435301 0.06899776] prediction 1 target 1.0\n",
      "[0.56574823 0.0553324  0.37891936] prediction 0 target 0.0\n",
      "[0.42430648 0.33360592 0.2420876 ] prediction 0 target 0.0\n",
      "[0.54749824 0.14012724 0.31237452] prediction 0 target 0.0\n",
      "[0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "[0.20007521 0.66592111 0.13400368] prediction 1 target 1.0\n",
      "[0.51928963 0.13290752 0.34780284] prediction 0 target 0.0\n",
      "[0.09527316 0.82981946 0.07490738] prediction 1 target 1.0\n",
      "[0.3688562  0.08042042 0.55072338] prediction 2 target 0.0\n",
      "[0.41511123 0.38313264 0.20175613] prediction 0 target 0.0\n",
      "[0.40040104 0.165778   0.43382096] prediction 2 target 0.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 0.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.46386586 0.31068183 0.22545231] prediction 0 target 0.0\n",
      "[0.25632393 0.61909519 0.12458088] prediction 1 target 1.0\n",
      "[0.11899135 0.75208556 0.12892309] prediction 1 target 1.0\n",
      "[0.4194781  0.12603168 0.45449023] prediction 2 target 0.0\n",
      "[0.20007521 0.66592111 0.13400368] prediction 1 target 1.0\n",
      "[0.17152321 0.67016705 0.15830973] prediction 1 target 1.0\n",
      "[0.45449006 0.18817248 0.35733745] prediction 0 target 2.0\n",
      "[0.45449006 0.18817248 0.35733745] prediction 0 target 2.0\n",
      "[0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "[0.32753091 0.09840626 0.57406283] prediction 2 target 2.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.46707355 0.10183443 0.43109203] prediction 0 target 0.0\n",
      "[0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "[0.09634224 0.83913096 0.06452681] prediction 1 target 1.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 0.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.53893965 0.10009648 0.36096387] prediction 0 target 0.0\n",
      "[0.5740626  0.14692619 0.27901121] prediction 0 target 0.0\n",
      "[0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.28220534 0.13716    0.58063466] prediction 2 target 0.0\n",
      "[0.17564234 0.68626097 0.13809669] prediction 1 target 1.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 0.0\n",
      "[0.17931056 0.12009622 0.70059321] prediction 2 target 2.0\n",
      "[0.17564234 0.68626097 0.13809669] prediction 1 target 1.0\n",
      "[0.40689973 0.07557289 0.51752738] prediction 2 target 2.0\n",
      "[0.38101396 0.13438225 0.4846038 ] prediction 2 target 0.0\n",
      "[0.2163782 0.613497  0.1701248] prediction 1 target 1.0\n",
      "[0.11638211 0.73559391 0.14802397] prediction 1 target 1.0\n",
      "[0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "[0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "[0.17931056 0.12009622 0.70059321] prediction 2 target 2.0\n",
      "[0.53447421 0.16058212 0.30494367] prediction 0 target 0.0\n",
      "[0.23050775 0.21275033 0.55674192] prediction 2 target 0.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 0.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "[0.53447421 0.16058212 0.30494367] prediction 0 target 0.0\n",
      "[0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "[0.36885619 0.08042041 0.5507234 ] prediction 2 target 0.0\n",
      "[0.20007519 0.66592113 0.13400368] prediction 1 target 1.0\n",
      "[0.42430648 0.24208759 0.33360593] prediction 0 target 0.0\n",
      "[0.12335408 0.77966024 0.09698568] prediction 1 target 1.0\n",
      "[0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "[0.15983722 0.73310914 0.10705363] prediction 1 target 1.0\n",
      "[0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "[0.17564234 0.68626097 0.13809669] prediction 1 target 1.0\n",
      "[0.2108644  0.70183143 0.08730417] prediction 1 target 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44975909 0.13512957 0.41511134] prediction 0 target 2.0\n",
      "[0.17931056 0.12009622 0.70059321] prediction 2 target 2.0\n",
      "[0.2163782 0.613497  0.1701248] prediction 1 target 1.0\n",
      "[0.20007521 0.66592111 0.13400368] prediction 1 target 1.0\n",
      "[0.27238859 0.06971542 0.65789599] prediction 2 target 0.0\n",
      "[0.42743571 0.28628214 0.28628214] prediction 0 target 1.0\n",
      "[0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "[0.25632393 0.61909519 0.12458088] prediction 1 target 1.0\n",
      "[0.17564234 0.68626097 0.13809669] prediction 1 target 1.0\n",
      "[0.20007521 0.66592111 0.13400368] prediction 1 target 1.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "[0.20771093 0.69133548 0.10095358] prediction 1 target 2.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.45449006 0.18817248 0.35733745] prediction 0 target 2.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "[0.16241289 0.74492259 0.09266452] prediction 1 target 1.0\n",
      "[0.2163782 0.613497  0.1701248] prediction 1 target 1.0\n",
      "[0.38101396 0.13438227 0.48460377] prediction 2 target 0.0\n",
      "[0.195519   0.15372448 0.65075651] prediction 2 target 0.0\n",
      "[0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 0.0\n",
      "[0.48967302 0.12532739 0.38499958] prediction 0 target 0.0\n",
      "[0.48596973 0.06549779 0.44853247] prediction 0 target 2.0\n",
      "[0.32753091 0.09840626 0.57406283] prediction 2 target 2.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "[0.32753091 0.09840626 0.57406283] prediction 2 target 2.0\n",
      "[0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "[0.11899135 0.75208555 0.1289231 ] prediction 1 target 1.0\n",
      "[0.58326769 0.24149024 0.17524207] prediction 0 target 2.0\n",
      "[0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "[0.17564234 0.68626097 0.13809669] prediction 1 target 1.0\n",
      "[0.2163782 0.613497  0.1701248] prediction 1 target 1.0\n",
      "[0.25632393 0.61909519 0.12458088] prediction 1 target 1.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "[0.44975909 0.13512957 0.41511134] prediction 0 target 2.0\n",
      "[0.12335408 0.77966024 0.09698568] prediction 1 target 1.0\n",
      "[0.2163782 0.613497  0.1701248] prediction 1 target 1.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 0.0\n",
      "[0.26912586 0.65001558 0.08085856] prediction 1 target 1.0\n",
      "[0.17152322 0.67016705 0.15830973] prediction 1 target 1.0\n",
      "[0.30876326 0.15006793 0.54116882] prediction 2 target 2.0\n",
      "[0.15983724 0.10705365 0.73310911] prediction 2 target 2.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.45449006 0.18817248 0.35733745] prediction 0 target 2.0\n",
      "[0.11123246 0.825304   0.06346354] prediction 1 target 1.0\n",
      "[0.61272394 0.25368598 0.13359008] prediction 0 target 0.0\n",
      "[0.58326769 0.17524207 0.24149024] prediction 0 target 0.0\n",
      "[0.20007521 0.66592111 0.13400368] prediction 1 target 1.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.40716532 0.27270576 0.32012893] prediction 0 target 0.0\n",
      "[0.21813607 0.05582997 0.72603396] prediction 2 target 2.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 0.0\n",
      "[0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.16692772 0.18086052 0.65221177] prediction 2 target 0.0\n",
      "[0.25088854 0.14314418 0.60596728] prediction 2 target 0.0\n",
      "[0.20007519 0.66592113 0.13400368] prediction 1 target 1.0\n",
      "[0.2163782 0.613497  0.1701248] prediction 1 target 1.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.5863751  0.20681245 0.20681245] prediction 0 target 0.0\n",
      "[0.33661819 0.07339169 0.58999011] prediction 2 target 0.0\n",
      "[0.17564234 0.68626097 0.13809669] prediction 1 target 1.0\n",
      "[0.47989332 0.19869017 0.32141651] prediction 0 target 0.0\n",
      "[0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "[0.37330343 0.06933311 0.55736346] prediction 2 target 0.0\n",
      "[0.27563099 0.15726096 0.56710805] prediction 2 target 0.0\n",
      "[0.47922855 0.14398365 0.3767878 ] prediction 0 target 0.0\n",
      "[0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "[0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "[0.11899135 0.75208556 0.12892309] prediction 1 target 1.0\n",
      "number of inputs of each class [65, 83, 66]\n",
      "correctly predicted [34, 81, 53]\n",
      "correctly predicted class 1 at 52.308\n",
      "correctly predicted class 2 at 97.590\n",
      "correctly predicted class 3 at 80.303\n",
      "accuracy at 78.505\n"
     ]
    }
   ],
   "source": [
    "test_on_everything(print_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0003)\n",
      "tensor([[[ 0.1897, -0.2334,  0.0999,  0.3074],\n",
      "         [-0.1952, -0.1541, -0.0417,  0.1761]],\n",
      "\n",
      "        [[ 0.0153, -0.0848, -0.1458, -0.1087],\n",
      "         [-0.2014, -0.0028,  0.1153, -0.3012]],\n",
      "\n",
      "        [[ 0.2176,  0.0808, -0.2058, -0.4027],\n",
      "         [ 0.3052,  0.0084, -0.0639, -0.0401]],\n",
      "\n",
      "        [[-0.1632, -0.1746,  0.0811, -0.2844],\n",
      "         [ 0.1262,  0.0546,  0.3440,  0.0271]],\n",
      "\n",
      "        [[-0.0169, -0.2973,  0.1580, -0.2712],\n",
      "         [ 0.1108, -0.0332, -0.2981, -0.1849]],\n",
      "\n",
      "        [[ 0.0550, -0.1278,  0.3177,  0.2081],\n",
      "         [ 0.3411, -0.0240,  0.0873,  0.3508]],\n",
      "\n",
      "        [[ 0.3735, -0.3262, -0.2033, -0.1625],\n",
      "         [-0.2184, -0.3277,  0.1822, -0.3178]],\n",
      "\n",
      "        [[-0.3618,  0.1095, -0.0582, -0.3274],\n",
      "         [-0.0707, -0.2446,  0.1825,  0.0196]],\n",
      "\n",
      "        [[ 0.5567,  0.3586,  0.4280,  0.0125],\n",
      "         [ 0.2044,  0.1297,  0.1836,  0.5140]],\n",
      "\n",
      "        [[-0.2227, -0.2652, -0.3281,  0.2316],\n",
      "         [-0.0969,  0.1137,  0.3237,  0.0428]],\n",
      "\n",
      "        [[-0.2292,  0.3545, -0.2481, -0.1313],\n",
      "         [-0.0109, -0.2764,  0.2279, -0.0421]],\n",
      "\n",
      "        [[-0.1619, -0.3408,  0.1885, -0.1739],\n",
      "         [-0.0946,  0.1303,  0.2258,  0.1483]],\n",
      "\n",
      "        [[-0.3217, -0.2825, -0.2593, -0.2140],\n",
      "         [ 0.1682, -0.1490, -0.1466,  0.1254]],\n",
      "\n",
      "        [[-0.0753, -0.1722,  0.0682,  0.1318],\n",
      "         [-0.1064,  0.1933,  0.1202,  0.2505]],\n",
      "\n",
      "        [[-0.3033, -0.0485, -0.0818,  0.2196],\n",
      "         [ 0.1850, -0.2118,  0.1389, -0.3099]],\n",
      "\n",
      "        [[-0.0940, -0.0416, -0.1028,  0.1037],\n",
      "         [-0.3820, -0.4143, -0.2705, -0.3870]],\n",
      "\n",
      "        [[-0.1767,  0.0633,  0.2393, -0.0467],\n",
      "         [ 0.1146, -0.2771,  0.1257,  0.2515]],\n",
      "\n",
      "        [[-0.4260, -0.3725, -0.1222,  0.0327],\n",
      "         [ 0.1253,  0.1982,  0.1771, -0.1518]],\n",
      "\n",
      "        [[ 0.6329,  0.2152, -0.0590,  0.5411],\n",
      "         [ 0.0015,  0.4249,  0.3170,  0.1767]],\n",
      "\n",
      "        [[-0.1213, -0.2418, -0.0772,  0.0529],\n",
      "         [ 0.1654,  0.2767,  0.2188,  0.0040]],\n",
      "\n",
      "        [[-0.2805, -0.3065, -0.4494,  0.1145],\n",
      "         [-0.0163, -0.0606, -0.0162, -0.5569]],\n",
      "\n",
      "        [[ 0.2510,  0.0027,  0.3550,  0.0044],\n",
      "         [ 0.0635, -0.2826,  0.0859, -0.3044]],\n",
      "\n",
      "        [[ 0.3136, -0.1572,  0.1159,  0.1521],\n",
      "         [ 0.2505, -0.1516, -0.2199, -0.2146]],\n",
      "\n",
      "        [[ 0.3163, -0.2647,  0.0480, -0.2728],\n",
      "         [-0.2445, -0.0570,  0.0857,  0.0346]],\n",
      "\n",
      "        [[-0.0646,  0.0899, -0.2694,  0.0493],\n",
      "         [-0.1599, -0.2093,  0.0741, -0.1959]],\n",
      "\n",
      "        [[ 0.3493,  0.0193,  0.2312, -0.0740],\n",
      "         [-0.2955,  0.3051, -0.3263,  0.2119]],\n",
      "\n",
      "        [[-0.3502, -0.0416, -0.1404, -0.1195],\n",
      "         [-0.0299, -0.3361, -0.0773,  0.2634]],\n",
      "\n",
      "        [[ 0.1645,  0.3393,  0.0936,  0.2962],\n",
      "         [-0.0815, -0.1320, -0.0536, -0.1060]],\n",
      "\n",
      "        [[-0.2107,  0.1313,  0.1626,  0.3076],\n",
      "         [ 0.3323, -0.0889, -0.1927,  0.2776]],\n",
      "\n",
      "        [[ 0.8783,  0.7841,  0.5819,  1.0163],\n",
      "         [ 0.7107,  0.6753,  0.4500,  0.6693]],\n",
      "\n",
      "        [[ 0.0542, -0.1111,  0.1228,  0.2924],\n",
      "         [ 0.3229, -0.2052,  0.1485, -0.0550]],\n",
      "\n",
      "        [[ 0.3124, -0.1941,  0.2454, -0.2726],\n",
      "         [ 0.3196,  0.0242,  0.1752,  0.0366]]])\n",
      "tensor([ 0.6720,  0.1212,  0.9549,  0.9851,  0.1937,  0.9237,  0.3508,  0.5211,\n",
      "         0.2411,  0.2254,  0.9465,  0.0519,  0.9357,  0.9318,  0.4898,  0.5003,\n",
      "         0.6284,  0.3460,  0.3453,  0.2341,  0.5582,  0.5397,  0.1358,  0.4534,\n",
      "        -0.0581,  0.8381,  0.2520,  0.0843,  0.1619,  0.7505,  0.2298,  0.9439])\n",
      "tensor([ 9.4733e-03,  3.3529e-08,  1.7027e-02, -7.1186e-02,  1.2365e-04,\n",
      "         1.5541e-03,  4.1804e-03,  1.4618e-02, -2.7360e-04, -1.3480e-03,\n",
      "         2.7028e-02, -1.3929e-08, -3.1726e-02,  1.4247e-02,  1.4651e-02,\n",
      "         5.3564e-03,  6.4325e-02,  1.8548e-04,  1.8466e-02,  4.6000e-04,\n",
      "        -3.1336e-02, -2.6253e-02,  9.3159e-09, -2.6647e-03,  5.3055e-08,\n",
      "        -2.8414e-02, -2.3006e-04,  2.2786e-08,  7.1946e-05,  2.7084e-02,\n",
      "        -3.0086e-04, -2.2673e-02])\n",
      "tensor([[[-0.0104, -0.0665,  0.0708,  0.0794],\n",
      "         [-0.0429, -0.0614,  0.0456, -0.0412],\n",
      "         [ 0.0567,  0.0685,  0.0390,  0.0055],\n",
      "         ...,\n",
      "         [-0.0708,  0.0311, -0.0189,  0.0525],\n",
      "         [ 0.0412,  0.0133,  0.0547,  0.0064],\n",
      "         [-0.0461,  0.0404,  0.0066, -0.0265]],\n",
      "\n",
      "        [[ 0.0234,  0.0034, -0.0289,  0.0464],\n",
      "         [-0.0758, -0.0188,  0.0540,  0.0236],\n",
      "         [-0.0725, -0.0846,  0.0067, -0.0251],\n",
      "         ...,\n",
      "         [-0.0026,  0.0268,  0.0449,  0.0711],\n",
      "         [ 0.0486,  0.0298,  0.0631, -0.0007],\n",
      "         [-0.0530, -0.0635, -0.0440,  0.0119]],\n",
      "\n",
      "        [[-0.0809, -0.0560, -0.0228, -0.0610],\n",
      "         [-0.0047,  0.0671, -0.0092,  0.0497],\n",
      "         [-0.0003, -0.0137, -0.0335,  0.0507],\n",
      "         ...,\n",
      "         [-0.0597, -0.0228, -0.0463,  0.0469],\n",
      "         [ 0.0430, -0.0082,  0.0636,  0.0221],\n",
      "         [-0.0860, -0.0120,  0.0583,  0.0806]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0106,  0.0242, -0.0429, -0.0548],\n",
      "         [ 0.0658, -0.0632,  0.0137, -0.0663],\n",
      "         [ 0.0025, -0.0638,  0.0244, -0.0560],\n",
      "         ...,\n",
      "         [-0.0113, -0.0348, -0.0878, -0.0584],\n",
      "         [-0.0487,  0.0365,  0.0183, -0.0544],\n",
      "         [-0.0360, -0.0302, -0.0651, -0.0676]],\n",
      "\n",
      "        [[ 0.0221, -0.0690, -0.0088, -0.0413],\n",
      "         [-0.0307, -0.0498, -0.0862,  0.0042],\n",
      "         [-0.0532,  0.0952, -0.0725, -0.0893],\n",
      "         ...,\n",
      "         [ 0.0410,  0.0904, -0.0217,  0.0193],\n",
      "         [ 0.0260,  0.0829,  0.0171,  0.0445],\n",
      "         [-0.0793, -0.0566,  0.0639,  0.0573]],\n",
      "\n",
      "        [[ 0.0100,  0.0043,  0.0663,  0.0117],\n",
      "         [-0.0513, -0.0628,  0.0862, -0.0577],\n",
      "         [-0.0467, -0.0550, -0.0056,  0.0489],\n",
      "         ...,\n",
      "         [-0.0203, -0.0630, -0.0833, -0.0396],\n",
      "         [-0.0817, -0.0209,  0.0017,  0.0659],\n",
      "         [ 0.0441, -0.0868, -0.0195, -0.0393]]])\n",
      "tensor([ 0.6099,  0.6064,  0.5334,  0.7753,  0.5006,  0.7203,  0.3092,  0.1017,\n",
      "         0.7983,  0.6582,  0.6466, -0.0093,  1.0113,  0.5104,  0.8409,  0.5792,\n",
      "         0.6192,  0.2861,  0.4363,  0.4843,  0.7400,  0.1352,  0.6650,  0.3928,\n",
      "         0.7797,  0.4451,  0.0371,  0.8107,  0.5415,  0.4738,  0.1177,  0.6848,\n",
      "         0.2237,  0.3652,  0.8187,  0.8484,  0.4092,  0.4537,  0.5256,  0.0142,\n",
      "         0.8651,  0.9654,  0.8936,  0.6682,  0.0636,  0.5185,  0.6181,  1.0219,\n",
      "         0.5568,  0.7521,  0.1892,  0.0794,  0.3129,  0.4559,  0.9159,  0.7343,\n",
      "         0.3686,  0.6853,  0.1477,  0.6600,  0.7209,  0.8557,  0.4794,  0.0786])\n",
      "tensor([ 6.8105e-03, -3.4233e-02, -7.1883e-03, -3.4004e-02,  2.1691e-02,\n",
      "        -1.4135e-02, -3.7140e-05, -2.1095e-08, -1.5449e-02,  2.3593e-02,\n",
      "        -4.7134e-02,  4.0245e-08,  2.3771e-02,  3.0018e-03, -5.4255e-02,\n",
      "        -4.4829e-03,  5.6340e-02,  1.6736e-02, -4.8882e-03, -1.0183e-02,\n",
      "         2.7305e-02,  9.6606e-04,  3.6864e-02, -6.9262e-03,  1.7517e-02,\n",
      "        -2.6882e-02, -1.3067e-09,  2.3615e-02, -1.1998e-02, -8.0865e-03,\n",
      "         8.2686e-09, -2.2886e-02, -2.2789e-03,  1.1717e-03,  1.0119e-02,\n",
      "        -3.1491e-02, -2.2655e-03, -7.6693e-03, -6.0466e-03,  1.1652e-08,\n",
      "         2.8568e-02,  6.0571e-02,  2.9728e-03, -2.3599e-02, -1.1526e-08,\n",
      "        -6.5121e-03, -3.4653e-02,  5.3179e-02,  4.6727e-02, -1.9965e-02,\n",
      "        -1.2319e-08, -3.5979e-08, -6.9428e-03, -9.6534e-03,  3.3978e-02,\n",
      "        -7.0328e-03,  2.2896e-03, -1.1860e-02, -1.4776e-08, -2.2922e-02,\n",
      "        -2.3898e-03,  2.0707e-02, -2.8185e-02,  1.3478e-08])\n",
      "tensor([[[-0.0120, -0.0124, -0.0486,  0.0004],\n",
      "         [ 0.0151, -0.0455, -0.0303,  0.0106],\n",
      "         [-0.0372,  0.0585, -0.0471, -0.0112],\n",
      "         ...,\n",
      "         [ 0.0669, -0.0558,  0.0498,  0.0087],\n",
      "         [-0.0144, -0.0177, -0.0506,  0.0282],\n",
      "         [ 0.0589,  0.0218, -0.0022, -0.0241]],\n",
      "\n",
      "        [[-0.0176,  0.0224, -0.0242, -0.0209],\n",
      "         [-0.0615, -0.0478, -0.0549, -0.0091],\n",
      "         [-0.0328, -0.0034, -0.0116,  0.0337],\n",
      "         ...,\n",
      "         [-0.0202,  0.0196,  0.0179, -0.0185],\n",
      "         [ 0.0273,  0.0419, -0.0240, -0.0035],\n",
      "         [ 0.0085, -0.0316,  0.0602,  0.0398]],\n",
      "\n",
      "        [[ 0.0341,  0.0406,  0.0444,  0.0502],\n",
      "         [ 0.0086,  0.0236,  0.0274,  0.0449],\n",
      "         [-0.0024,  0.0716, -0.0399,  0.0316],\n",
      "         ...,\n",
      "         [-0.0696, -0.0038,  0.0127, -0.0117],\n",
      "         [ 0.0024,  0.0374,  0.0638,  0.0382],\n",
      "         [-0.0456, -0.0414, -0.0324, -0.0121]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0481, -0.0170,  0.0315, -0.0366],\n",
      "         [-0.0123, -0.0056, -0.0552,  0.0048],\n",
      "         [-0.0102, -0.0071,  0.0252,  0.0155],\n",
      "         ...,\n",
      "         [-0.0193, -0.0319,  0.0469,  0.0618],\n",
      "         [ 0.0316,  0.0561, -0.0093, -0.0354],\n",
      "         [ 0.0613, -0.0348, -0.0469,  0.0205]],\n",
      "\n",
      "        [[ 0.0365, -0.0314,  0.0123,  0.0069],\n",
      "         [ 0.0319,  0.0160,  0.0029,  0.0274],\n",
      "         [-0.0398, -0.0152,  0.0339,  0.0061],\n",
      "         ...,\n",
      "         [-0.0504,  0.0046,  0.0062,  0.0359],\n",
      "         [-0.0221,  0.0213, -0.0003, -0.0247],\n",
      "         [ 0.0230,  0.0599,  0.0050, -0.0527]],\n",
      "\n",
      "        [[-0.0350, -0.0049,  0.0281, -0.0291],\n",
      "         [ 0.0070,  0.0385,  0.0071, -0.0006],\n",
      "         [-0.0436, -0.0051,  0.0582, -0.0571],\n",
      "         ...,\n",
      "         [ 0.0009,  0.0007, -0.0198,  0.0557],\n",
      "         [ 0.0017, -0.0455,  0.0231, -0.0137],\n",
      "         [ 0.0527,  0.0554,  0.0057, -0.0049]]])\n",
      "tensor([ 0.4261,  0.7198,  1.0780,  0.9912,  0.9463,  0.2163,  0.7443,  0.7844,\n",
      "         0.1681,  0.5200,  0.5530,  0.5491,  0.8200,  0.7117,  0.6364,  0.1120,\n",
      "         0.6274,  0.9264,  0.9792,  0.0929,  0.5914,  0.3163,  0.6768,  0.4024,\n",
      "         0.7942,  0.6668,  0.6302,  0.3788,  0.6623,  0.5311,  0.3950,  0.9593,\n",
      "         0.5226,  0.2839,  0.7109,  0.1347,  0.1001,  0.4013,  0.1462,  0.6243,\n",
      "         0.3538,  0.3983,  0.7138,  0.4442,  0.4316,  0.8168,  0.5669,  0.6623,\n",
      "         0.3136,  0.0624,  0.6411,  0.9921,  1.1008,  0.4739,  0.1297,  0.4040,\n",
      "         0.3919,  0.2513, -0.0477,  0.1642,  0.0566,  0.2624,  0.3156,  0.8733,\n",
      "         0.0613,  0.3492,  0.6679,  0.1323,  0.7897,  0.4261,  0.6526,  0.2760,\n",
      "         0.2897,  0.3215,  0.9797,  0.8933,  1.0803,  1.0742,  0.4247,  0.8233,\n",
      "         0.0822,  0.4217,  0.0409,  0.2201,  0.4835,  0.7527,  0.1221,  0.1021,\n",
      "         0.9023,  0.6950,  0.8230,  0.0771,  0.0120,  0.6815,  0.2381,  0.4917,\n",
      "         0.5711,  0.6372,  0.1916,  0.7553,  0.4617,  0.3612,  1.0660,  0.1057,\n",
      "         0.9570,  0.0930,  0.7436,  0.2409,  0.9282,  0.4478,  0.6589,  0.5610,\n",
      "         0.5350,  0.3639,  1.0138,  0.0514,  0.8581,  0.5902,  0.1963,  0.1039,\n",
      "         0.7524,  0.9357,  0.8042,  1.0085,  0.2344,  0.0077,  0.0788,  0.2644])\n",
      "tensor([ 7.3677e-09, -2.2476e-08,  1.7152e-08,  4.2578e-09, -9.3677e-10,\n",
      "        -1.0118e-08,  3.9888e-09,  1.5765e-08, -1.3113e-08, -6.7535e-09,\n",
      "        -2.7671e-08, -4.7451e-09,  9.9612e-09, -5.6004e-09,  1.0394e-09,\n",
      "        -3.3642e-08, -1.9318e-08,  1.3062e-09,  6.1179e-10,  1.0392e-08,\n",
      "        -1.1741e-08, -5.0689e-11,  1.1978e-09,  3.2284e-08, -1.3253e-08,\n",
      "         3.9531e-08, -2.3085e-09,  3.2033e-08, -1.5258e-09, -3.9732e-08,\n",
      "         1.6399e-09,  9.8534e-09, -4.3635e-09,  2.9055e-08,  1.1860e-08,\n",
      "         5.0860e-09,  2.7464e-08,  7.9003e-09,  6.1376e-09,  1.9851e-08,\n",
      "        -2.3709e-08, -5.1731e-09, -4.1663e-09, -2.2178e-08,  1.2325e-08,\n",
      "        -1.3001e-08, -1.3296e-08,  1.6949e-08,  1.9939e-08, -6.2492e-09,\n",
      "         4.5817e-09,  2.1118e-09,  1.9523e-08, -1.7807e-08, -1.9612e-09,\n",
      "         6.7704e-09,  3.5268e-08,  3.4908e-08, -3.0771e-08, -1.1535e-08,\n",
      "         9.4420e-09, -3.3191e-09, -5.6055e-09,  1.7733e-08, -3.0216e-08,\n",
      "        -1.7834e-08,  1.4797e-08,  2.6025e-08,  5.0203e-09,  1.0325e-08,\n",
      "         1.9224e-08, -7.0254e-09,  9.8787e-09, -1.8044e-08,  1.6214e-08,\n",
      "         6.2701e-09, -6.7314e-09,  1.3024e-08, -2.7355e-08,  2.0102e-08,\n",
      "        -6.0567e-10, -3.5525e-08, -7.4576e-09,  2.9646e-08, -7.5982e-10,\n",
      "        -1.1633e-08,  1.1850e-08,  2.7593e-08, -1.7319e-08,  1.5100e-08,\n",
      "        -1.4409e-08,  8.5483e-09, -4.2634e-10,  2.6253e-08,  2.0392e-09,\n",
      "         8.2106e-09, -3.7893e-08, -1.7635e-09, -1.1529e-08, -3.1828e-09,\n",
      "         1.1764e-08, -6.7182e-09, -1.2200e-08,  1.8536e-08, -1.0676e-10,\n",
      "         3.7641e-09, -7.0753e-09,  6.8659e-09,  5.8407e-09,  2.0102e-09,\n",
      "         3.2588e-10, -1.1603e-08,  2.4049e-08,  1.9482e-09, -2.3104e-09,\n",
      "         2.6531e-08,  1.4454e-08, -2.0059e-08, -2.8582e-08, -7.8687e-09,\n",
      "         6.9055e-09, -6.8903e-09, -2.5888e-08, -2.2642e-08,  1.5862e-08,\n",
      "         7.4208e-09,  1.1866e-08, -7.8525e-09])\n",
      "tensor([[[ 2.1477e-02, -7.2598e-03, -3.8330e-02,  1.0516e-02],\n",
      "         [ 7.3999e-03, -2.6661e-02,  2.6349e-02,  1.1319e-02],\n",
      "         [-2.8740e-02, -3.5538e-03,  4.1835e-02,  1.4433e-02],\n",
      "         ...,\n",
      "         [-2.9939e-03,  3.0421e-02, -4.2425e-02, -4.3201e-03],\n",
      "         [-1.5329e-02,  1.6133e-02, -9.6477e-03, -2.5618e-02],\n",
      "         [-3.2468e-02,  1.2615e-02, -3.5957e-02,  6.6273e-03]],\n",
      "\n",
      "        [[-3.0738e-02,  2.8432e-02,  2.6320e-02, -1.9540e-02],\n",
      "         [-3.6625e-02,  1.5659e-03,  1.9759e-02, -1.3299e-02],\n",
      "         [ 5.4202e-03,  1.7177e-02, -1.4831e-02,  3.6438e-02],\n",
      "         ...,\n",
      "         [ 1.4714e-02,  3.0873e-02, -1.4963e-02, -4.1165e-02],\n",
      "         [ 3.0643e-02,  1.5603e-02,  7.2217e-03,  1.7222e-02],\n",
      "         [-1.7307e-02, -7.0022e-04,  2.7460e-02, -1.7924e-02]],\n",
      "\n",
      "        [[ 3.1187e-02, -8.6220e-03, -2.7170e-02, -7.5418e-03],\n",
      "         [-2.6981e-02,  2.3441e-02, -1.8208e-02,  3.9842e-02],\n",
      "         [ 2.9202e-02,  3.9894e-02,  1.0107e-02,  3.2406e-02],\n",
      "         ...,\n",
      "         [-2.2495e-02, -1.5927e-02, -1.7447e-02,  2.0811e-02],\n",
      "         [ 7.5863e-03,  2.0263e-02, -1.5956e-02, -3.1528e-02],\n",
      "         [ 2.7224e-02,  4.1734e-02,  5.2767e-03,  1.8579e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-8.9268e-03,  1.1433e-02,  1.3275e-02,  2.3859e-02],\n",
      "         [ 3.2108e-02, -3.3344e-02,  3.4955e-02,  2.1604e-02],\n",
      "         [-1.0099e-02,  3.4361e-05,  3.8562e-02,  1.8939e-02],\n",
      "         ...,\n",
      "         [ 4.2845e-02,  1.4775e-02, -3.7151e-02,  3.9216e-02],\n",
      "         [-9.7061e-03, -3.9192e-03, -1.4093e-02, -4.3451e-02],\n",
      "         [ 1.8041e-02,  1.2140e-03, -2.1024e-02, -4.1802e-02]],\n",
      "\n",
      "        [[ 2.4617e-02, -1.0012e-02, -1.2157e-02,  3.2291e-02],\n",
      "         [ 1.8251e-02, -4.6292e-02, -1.2662e-02,  3.3668e-02],\n",
      "         [-2.6287e-02,  4.1525e-02,  5.1949e-03,  4.6545e-02],\n",
      "         ...,\n",
      "         [-1.5569e-02, -1.5896e-02,  3.3865e-03, -1.8158e-02],\n",
      "         [ 1.1564e-02, -6.6662e-03,  2.6267e-02,  3.1095e-02],\n",
      "         [-1.6768e-02,  3.0561e-02,  5.8953e-03,  3.3094e-02]],\n",
      "\n",
      "        [[ 3.1216e-02, -8.4297e-03,  4.8117e-04,  3.0942e-02],\n",
      "         [-3.1545e-02,  3.2052e-02,  1.1769e-02, -2.7490e-02],\n",
      "         [-4.3905e-02, -2.6925e-02, -3.6366e-03,  1.1977e-02],\n",
      "         ...,\n",
      "         [-2.1439e-02,  3.1763e-02,  4.1630e-03,  3.2729e-02],\n",
      "         [-7.8290e-03,  3.4423e-02,  1.8132e-02,  3.0737e-02],\n",
      "         [-1.2849e-03,  4.1108e-02, -3.3902e-02, -8.2020e-03]]])\n",
      "tensor([ 0.0198,  0.1872,  0.0140,  0.9673,  0.5246,  0.9160,  0.7447,  0.6061,\n",
      "         0.1880, -0.0964,  0.1647, -0.0089,  0.1280,  0.9005,  0.6959,  0.5945,\n",
      "         0.8811,  0.0013,  0.1577,  0.6202,  0.9331,  0.9537,  0.6263,  0.2294,\n",
      "        -0.0069,  0.4901,  0.5467,  0.6169,  0.5931, -0.0137,  0.0632,  0.8429,\n",
      "         0.9931,  0.1622,  0.7109,  0.6646,  0.5253,  0.2864,  0.2550,  0.5500,\n",
      "         0.7402,  0.9749,  0.2653,  0.1596,  0.8102,  0.3910,  0.9566,  0.7371,\n",
      "         0.3460,  0.8638,  0.6666,  0.3099,  0.9709,  0.2859,  0.8192,  0.9712,\n",
      "         0.2100,  0.8063,  0.2795,  0.6780,  0.3783,  0.6148,  0.3997,  0.6320,\n",
      "         0.9796,  0.6512,  0.1287,  0.7698,  0.1345,  0.9443,  0.5824,  0.8037,\n",
      "         0.6156,  0.5835,  0.9828,  0.7632,  0.5094,  0.2028,  0.5803,  0.4755,\n",
      "         0.4002,  0.2887,  0.1712,  0.8527,  0.5484,  0.7753,  0.9201,  0.4843,\n",
      "         0.9034,  0.9286,  0.7881,  0.5003,  0.4158,  0.6391,  0.9042,  0.9461,\n",
      "         0.8483,  0.2452,  0.9042,  0.5162,  0.2028,  0.0813,  0.7179,  0.8829,\n",
      "         0.5308,  0.0256,  0.7785,  0.2655,  0.8478,  0.2044,  0.2260,  0.2621,\n",
      "        -0.0084,  0.9138,  0.9513,  0.9136,  0.9211,  0.7103,  0.1878,  0.4437,\n",
      "         0.0840,  0.7308,  0.6141,  0.3105,  0.9679,  0.9657,  0.0258,  0.3986,\n",
      "         0.2597,  0.2062,  0.1158,  0.0040,  0.4236,  0.5678,  0.2251,  0.9888,\n",
      "         0.4049,  0.6420,  0.5342,  0.3698,  0.0398,  0.8473,  0.4636,  0.5478,\n",
      "         0.2240,  0.3374,  0.7849,  0.4615,  0.1526,  0.5506,  0.8892,  0.3460,\n",
      "         0.0443,  0.6453,  0.2174,  0.4982,  0.3846,  0.7274,  0.6939,  0.0178,\n",
      "         0.9257,  0.9601,  0.6621,  0.2583,  0.9018,  0.0996,  0.4738,  0.6398,\n",
      "         0.0654,  0.6047,  0.9236,  0.8292,  0.3384,  0.0260,  0.9284,  0.3773,\n",
      "         0.7253,  0.9347,  0.9347,  0.7866,  0.5928,  0.7533,  0.8739,  0.2413,\n",
      "         0.8970,  0.2240,  0.2941,  0.0261,  0.6201,  0.1834,  0.3603,  0.5585,\n",
      "         0.0701,  0.8378,  0.0184,  0.3316,  0.9555,  0.9548,  0.0979,  0.3971,\n",
      "         0.9542,  0.6606,  0.2600,  0.5473, -0.0477,  0.2497,  0.4740,  0.9547,\n",
      "         0.1125,  0.9149,  0.3218,  0.3045,  0.2725,  0.6263,  0.5913,  0.1427,\n",
      "         0.1924,  0.7709,  0.2562,  0.8423,  0.7822,  0.8845,  0.4493,  0.0381,\n",
      "         0.3061,  0.4727,  0.5045,  0.1466,  0.1613,  0.9998,  0.0714,  0.5487,\n",
      "         0.7649,  0.8276,  0.0084,  0.5747,  0.6474,  0.3114,  0.3763,  0.0977,\n",
      "         0.1266,  0.5577,  0.8346,  0.5757,  0.5694,  0.6971,  0.2403,  0.1246,\n",
      "         0.1059,  0.0502,  0.5135, -0.0470, -0.0418,  0.0866,  0.6593,  0.0177])\n",
      "tensor([-3.9324e-09,  3.8917e-09,  5.4149e-09,  7.3805e-03, -4.3026e-03,\n",
      "        -3.0395e-02,  4.0487e-03,  1.8265e-02,  5.0267e-10,  5.7050e-09,\n",
      "        -7.4784e-10, -7.3202e-10,  3.7837e-09, -1.5051e-02,  1.0677e-02,\n",
      "         9.0868e-04, -1.1944e-02, -6.7095e-09,  3.9747e-09, -5.9111e-03,\n",
      "         1.6287e-03,  2.6386e-03,  4.1470e-03, -3.7346e-09,  1.7959e-09,\n",
      "        -2.1087e-03, -3.9597e-03,  2.3748e-02, -1.4570e-02,  3.3245e-09,\n",
      "         7.6373e-09, -5.6134e-03,  1.3050e-02, -5.1414e-09,  3.3872e-02,\n",
      "         1.0421e-02, -1.3021e-03, -6.4322e-09,  3.0432e-09,  2.5366e-04,\n",
      "        -1.9918e-02,  1.3636e-02, -1.0136e-08,  9.5297e-09,  1.7952e-03,\n",
      "         1.1147e-08,  3.8405e-02, -1.4032e-02, -2.9252e-09, -3.5127e-02,\n",
      "        -1.9501e-02, -1.8427e-09, -3.2558e-03, -1.5220e-09, -1.1924e-02,\n",
      "         6.2192e-03,  1.6000e-10,  8.0308e-03,  3.0872e-10, -4.9284e-03,\n",
      "        -6.5486e-09, -2.1253e-02, -8.7767e-09, -4.5835e-02,  1.1088e-02,\n",
      "         1.8146e-02,  1.0150e-09, -4.6475e-02,  2.6592e-09,  3.6606e-02,\n",
      "        -1.0215e-02,  3.9535e-02, -1.0142e-02, -2.3453e-04,  4.3369e-02,\n",
      "        -5.2496e-02,  7.6324e-03,  2.8500e-09, -7.4686e-03,  1.1391e-03,\n",
      "         1.1826e-03,  6.4275e-10,  2.4670e-09, -2.3057e-02,  1.1848e-02,\n",
      "         7.2833e-02, -3.2388e-02, -1.1624e-03,  3.5882e-02, -1.2240e-02,\n",
      "         6.6295e-03,  3.1693e-03,  3.5129e-04, -2.2141e-02, -1.3851e-03,\n",
      "         5.3958e-02, -6.9877e-04, -3.8950e-11, -1.2542e-02,  1.7771e-05,\n",
      "         9.8880e-09, -3.7502e-10,  2.0898e-02,  4.6051e-02,  1.5181e-02,\n",
      "         9.3101e-09,  3.6927e-02,  1.1907e-09,  2.1696e-03, -1.4342e-09,\n",
      "        -1.0215e-08,  8.9331e-10, -1.3034e-09,  6.1350e-02,  4.1102e-03,\n",
      "         4.1762e-02,  2.2893e-02,  8.5586e-03, -2.4624e-10,  6.2797e-10,\n",
      "        -3.1154e-09,  1.7041e-02, -1.4603e-02, -4.4219e-10, -5.6522e-02,\n",
      "        -4.9308e-02,  7.6909e-09, -1.8987e-09, -6.5371e-10, -1.0150e-08,\n",
      "        -2.5070e-09, -2.3132e-09, -4.0147e-03,  2.4050e-05,  6.5324e-09,\n",
      "        -5.5510e-02, -3.7288e-09, -1.3709e-02,  2.3231e-03, -5.9614e-09,\n",
      "         3.9437e-10, -6.6180e-04, -1.0997e-02,  4.0553e-03, -5.8356e-09,\n",
      "         2.7529e-10,  1.9303e-02, -2.4860e-03, -4.3234e-09, -2.0216e-03,\n",
      "         2.1411e-02,  2.3612e-09, -4.2069e-09, -2.1633e-02,  2.9805e-09,\n",
      "        -1.4984e-03, -5.8792e-09, -2.2535e-03,  3.9104e-03,  3.6510e-09,\n",
      "         1.3772e-02,  2.3921e-02,  2.3467e-02,  3.3330e-09, -2.1026e-02,\n",
      "         3.6763e-09, -4.3219e-04, -1.7134e-02,  2.3482e-10, -2.7737e-02,\n",
      "         9.5521e-03, -1.0829e-02,  5.7646e-10,  2.1488e-09,  1.0139e-03,\n",
      "        -2.1733e-09,  5.9569e-03,  2.2392e-03, -5.0784e-02,  2.6025e-02,\n",
      "         8.6990e-03, -1.0865e-03,  1.7956e-02, -1.3406e-08, -2.3219e-02,\n",
      "         2.5869e-09,  2.0546e-09, -8.3071e-10,  1.1656e-02,  2.1630e-09,\n",
      "         9.5235e-09,  1.3570e-02,  6.9514e-11,  2.3632e-02,  5.0659e-09,\n",
      "         3.7998e-09, -1.1710e-02,  8.5545e-03,  7.4973e-09, -6.1722e-09,\n",
      "         7.4295e-02,  1.6220e-02, -6.3669e-09, -4.4846e-03, -3.5012e-09,\n",
      "         7.0000e-09, -2.2262e-03, -3.5316e-02, -6.4507e-09, -2.0071e-02,\n",
      "         9.5734e-09, -6.8231e-09, -3.0490e-09,  2.3851e-02,  6.7452e-03,\n",
      "         4.1637e-09,  7.4904e-10, -3.8993e-03,  3.0024e-09,  9.2482e-03,\n",
      "         2.7173e-02, -1.3516e-02,  2.2919e-03, -4.3771e-09,  3.1925e-09,\n",
      "        -4.1574e-03, -2.9836e-04,  6.6991e-09, -3.9923e-09,  1.5224e-03,\n",
      "         1.5786e-09, -2.2606e-02, -1.0773e-02,  3.9869e-03, -2.5000e-09,\n",
      "         2.5679e-02, -1.7733e-02, -3.4456e-09,  6.2406e-10,  1.0751e-08,\n",
      "         7.6742e-09,  3.2066e-02, -3.5461e-03,  1.9225e-02, -3.3655e-03,\n",
      "         1.7894e-02,  9.3457e-09,  1.3499e-09, -8.6993e-09, -5.0240e-10,\n",
      "        -6.0033e-03,  3.2841e-09,  3.2437e-10, -4.1077e-10, -3.3977e-02,\n",
      "        -5.6551e-09])\n",
      "tensor(7.6729e-09)\n",
      "tensor([[-0.0611,  0.0073, -0.0404,  ...,  0.0440, -0.0452, -0.0538],\n",
      "        [ 0.0390,  0.0637,  0.0422,  ...,  0.0436, -0.0105,  0.0527],\n",
      "        [ 0.0454, -0.0108,  0.0622,  ..., -0.0375,  0.0298, -0.0261],\n",
      "        ...,\n",
      "        [-0.0379, -0.0025,  0.0092,  ...,  0.0450,  0.0396, -0.0111],\n",
      "        [-0.0467,  0.0342,  0.0264,  ...,  0.0069,  0.0363,  0.0063],\n",
      "        [ 0.0248,  0.0564,  0.0575,  ...,  0.0331,  0.0505, -0.0234]])\n",
      "tensor([0.7496, 0.8934, 0.1871, 0.1057, 0.3161, 0.8479, 0.0624, 0.2406, 0.0056,\n",
      "        0.3629, 0.0046, 0.6333, 0.7086, 0.5497, 0.2116, 0.2757, 0.3157, 0.2654,\n",
      "        0.3761, 0.4564, 0.8221, 0.3484, 0.3425, 0.2576, 0.6260, 0.0064, 0.4747,\n",
      "        0.9150, 0.8757, 0.2886, 0.1473, 0.7848, 0.8206, 1.0016, 0.0316, 0.7795,\n",
      "        0.6581, 0.3581, 0.7360, 0.2832, 0.7443, 0.1829, 0.8870, 0.2779, 0.1996,\n",
      "        0.1493, 0.0883, 0.7847, 0.6402, 0.3174, 0.8474, 0.5311, 0.0142, 0.4570,\n",
      "        0.4395, 0.8889, 0.1042, 0.9010, 0.8509, 0.1338, 0.0554, 0.9405, 0.1003,\n",
      "        0.1446, 0.2466, 0.9328, 0.1139, 0.1505, 0.8214, 0.1783, 0.5036, 0.2966,\n",
      "        0.3109, 0.6613, 0.2282, 0.0760, 0.6762, 0.3348, 0.7103, 0.0306, 0.9278,\n",
      "        0.2958, 0.1366, 0.3024, 0.8626, 0.4674, 0.4964, 0.0036, 0.6570, 0.8381,\n",
      "        0.1130, 0.1410, 0.4958, 0.5304, 0.6742, 0.7433, 0.9827, 0.2290, 0.0955,\n",
      "        0.0208, 0.8099, 0.3126, 0.0757, 0.0795, 0.5072, 0.4606, 0.0323, 0.3481,\n",
      "        0.9037, 0.5607, 0.0035, 0.7272, 0.0093, 0.8001, 0.1021, 0.5171, 0.8603,\n",
      "        0.2270, 0.4093, 0.8530, 0.0429, 0.5050, 0.4214, 0.8977, 0.3385, 0.5664,\n",
      "        0.6649, 0.7393])\n",
      "tensor([ 3.0495e-03, -1.7773e-03, -1.0217e-03,  1.5273e-10,  3.7738e-04,\n",
      "        -1.0485e-03, -6.3257e-04, -2.3011e-03, -9.6591e-04,  5.4447e-04,\n",
      "         2.3715e-03,  1.4906e-03, -4.1597e-04,  4.1421e-04, -1.2137e-03,\n",
      "        -1.5273e-10, -1.5273e-10,  1.5273e-10,  4.4224e-03,  5.9378e-04,\n",
      "         9.1178e-04, -3.0416e-03, -1.9380e-03,  7.2770e-04, -2.4050e-04,\n",
      "         1.5273e-10,  1.5273e-10,  1.0485e-03, -6.0807e-07,  3.1693e-04,\n",
      "        -1.5273e-10,  1.5273e-10, -2.3698e-03,  9.4527e-04,  1.8842e-03,\n",
      "        -5.7162e-04,  1.9078e-03,  1.0026e-03,  6.3257e-04,  2.7609e-03,\n",
      "         1.5273e-10,  1.0485e-03,  5.4025e-04, -4.1597e-04, -1.5273e-10,\n",
      "        -4.1597e-04,  4.1695e-04, -4.4578e-04, -4.1597e-04, -4.1597e-04,\n",
      "         5.6299e-03,  5.5138e-04,  2.1461e-03, -6.3257e-04,  2.9429e-03,\n",
      "         1.5273e-10,  4.2521e-04,  2.1672e-04,  3.2746e-03, -6.3257e-04,\n",
      "         1.5273e-10,  4.1597e-04, -6.9545e-04, -7.8447e-04, -1.1360e-03,\n",
      "        -9.9138e-04, -4.3121e-04,  4.1597e-04, -1.5273e-10,  1.5273e-10,\n",
      "         2.3544e-03,  1.0693e-03,  6.3257e-04,  2.7782e-03, -1.9353e-03,\n",
      "         4.4681e-04, -4.1597e-04, -8.8115e-04, -1.5273e-10, -1.5273e-10,\n",
      "         6.3257e-04, -1.8801e-03, -1.0158e-03, -2.9183e-03, -1.7716e-03,\n",
      "         6.3257e-04, -5.2883e-04, -1.0485e-03, -4.1586e-03, -1.5273e-10,\n",
      "         2.3085e-03, -6.1121e-03,  4.1597e-04, -1.8697e-03,  1.2599e-03,\n",
      "         1.7093e-03,  9.7747e-04,  1.0485e-03, -3.0561e-03,  6.3257e-04,\n",
      "        -1.0485e-03,  1.4437e-03,  2.3393e-03, -2.3775e-03, -9.7221e-04,\n",
      "        -4.1597e-04, -6.3257e-04,  1.1208e-03,  1.6643e-03,  8.2889e-04,\n",
      "        -2.4021e-03,  6.3257e-04,  1.3080e-03,  1.0485e-03, -1.3125e-03,\n",
      "         8.0791e-04,  2.7962e-03, -6.3257e-04, -4.9263e-04,  1.6748e-03,\n",
      "         6.1262e-05,  1.6871e-03, -1.8869e-03,  8.0945e-04,  1.5273e-10,\n",
      "        -6.3257e-04, -1.0485e-03, -5.6421e-04])\n",
      "tensor([[-0.0138,  0.0212,  0.0139,  0.0973, -0.0107, -0.0319, -0.0626,  0.0697,\n",
      "         -0.0294,  0.0134, -0.0169, -0.0194,  0.1019,  0.0349,  0.0275, -0.0267,\n",
      "         -0.0454,  0.0397, -0.0070, -0.0224, -0.0332, -0.0072,  0.0591,  0.0400,\n",
      "          0.0451,  0.0761,  0.0198,  0.0207, -0.0503,  0.0593, -0.0594,  0.0644,\n",
      "         -0.0183,  0.0476,  0.0321,  0.0614, -0.0163,  0.0654,  0.0243,  0.0209,\n",
      "          0.0224,  0.0571, -0.0674,  0.0716, -0.0509,  0.0948, -0.0466,  0.0120,\n",
      "          0.0836,  0.0652, -0.0278, -0.0693,  0.0081, -0.0204,  0.0307,  0.0322,\n",
      "         -0.0256, -0.0561,  0.0287, -0.0353,  0.0438, -0.0978, -0.0293,  0.0197,\n",
      "         -0.0727, -0.0490,  0.0266, -0.0137, -0.0397,  0.0823,  0.0438,  0.0663,\n",
      "          0.0419, -0.0388, -0.0279, -0.0117,  0.0809, -0.0776, -0.0997, -0.0628,\n",
      "          0.0513,  0.0228,  0.0123, -0.0504,  0.0275,  0.0204, -0.0291, -0.0307,\n",
      "         -0.0768, -0.0512, -0.0767,  0.0061, -0.0468, -0.0607, -0.0201,  0.0132,\n",
      "          0.0157,  0.0739, -0.0197,  0.0607, -0.0729,  0.0397,  0.0377,  0.0145,\n",
      "          0.0258,  0.0211, -0.0583, -0.0160, -0.0340, -0.0506,  0.0092,  0.0581,\n",
      "          0.0982,  0.0906, -0.0227,  0.0143, -0.0202, -0.0398, -0.0609, -0.0134,\n",
      "          0.0530,  0.0577, -0.0520, -0.0766,  0.0503, -0.0134, -0.0319, -0.0226],\n",
      "        [ 0.0737, -0.0200, -0.0664,  0.0175,  0.0726, -0.0660,  0.0766, -0.0970,\n",
      "         -0.0113,  0.0874, -0.0750,  0.0204, -0.0393, -0.0186,  0.0105, -0.0735,\n",
      "         -0.0148,  0.0908, -0.0635, -0.0167, -0.0314, -0.0453, -0.0455, -0.0351,\n",
      "          0.0869,  0.0808,  0.0296,  0.0716, -0.0482,  0.0513, -0.0487,  0.0091,\n",
      "          0.0657, -0.0504, -0.0198,  0.0811,  0.0589,  0.0252, -0.0354, -0.0642,\n",
      "          0.0862,  0.0256, -0.0127, -0.0318, -0.0759, -0.0570, -0.0106, -0.0666,\n",
      "         -0.0298, -0.0209,  0.0203, -0.0523,  0.0201,  0.0994,  0.0280,  0.0253,\n",
      "         -0.0620, -0.0332,  0.0404,  0.0497,  0.0148,  0.0923, -0.0056,  0.0266,\n",
      "          0.0629, -0.0526, -0.0110,  0.0311, -0.0295,  0.0457, -0.0342,  0.0066,\n",
      "         -0.0432,  0.0095, -0.0301,  0.0527, -0.0601,  0.0269, -0.0143, -0.0424,\n",
      "         -0.0358,  0.0353,  0.0380, -0.0159,  0.0574, -0.0279,  0.0225, -0.0179,\n",
      "          0.0076, -0.0551,  0.0402,  0.0412,  0.0758,  0.0265, -0.0349, -0.0225,\n",
      "         -0.0508,  0.0370, -0.0380, -0.0100, -0.0753, -0.0969, -0.0184,  0.0935,\n",
      "         -0.0818, -0.0692,  0.0423,  0.0160, -0.0311, -0.0214, -0.0570, -0.0253,\n",
      "         -0.0163,  0.0169,  0.0206, -0.0407,  0.0180,  0.0562,  0.0231,  0.0408,\n",
      "         -0.0290,  0.0504,  0.0073, -0.0223,  0.0408,  0.0230, -0.0800, -0.0260],\n",
      "        [-0.0309,  0.0272,  0.0811,  0.0670, -0.0618,  0.0430,  0.0260, -0.0139,\n",
      "          0.0628,  0.0123, -0.0209, -0.0653,  0.0393,  0.0199,  0.0464, -0.0179,\n",
      "         -0.0292,  0.0435, -0.0072, -0.0118, -0.0201,  0.0026, -0.0267,  0.0236,\n",
      "          0.0144,  0.0477,  0.0613, -0.0341,  0.0110, -0.0105, -0.0611,  0.0663,\n",
      "         -0.0096,  0.0484,  0.0682, -0.0125, -0.0640,  0.0105, -0.0393,  0.0166,\n",
      "          0.0521, -0.0514, -0.0382,  0.0521, -0.0149,  0.0165,  0.0531,  0.0644,\n",
      "          0.0363,  0.0749,  0.0841,  0.0136, -0.0579,  0.0151,  0.0133,  0.0663,\n",
      "         -0.0586,  0.0095, -0.0188,  0.0089,  0.0622, -0.0233,  0.0501,  0.0323,\n",
      "         -0.0165,  0.0085,  0.0690, -0.0810, -0.0399,  0.0533, -0.0625,  0.0253,\n",
      "         -0.0240,  0.0343, -0.0155,  0.0065,  0.0541, -0.0229, -0.0225, -0.0750,\n",
      "         -0.0386,  0.0327,  0.0076,  0.0236,  0.0735, -0.0348, -0.0112,  0.0514,\n",
      "          0.0255, -0.0685,  0.0112, -0.0251, -0.0486, -0.0205,  0.0837, -0.0569,\n",
      "          0.0172, -0.0578, -0.0180, -0.0365,  0.0666,  0.0144,  0.0482, -0.0133,\n",
      "          0.0104,  0.0564,  0.0124, -0.0725, -0.0147,  0.0207, -0.0183, -0.0408,\n",
      "          0.0095, -0.0602, -0.0415,  0.0082, -0.0650,  0.0211,  0.0168, -0.0440,\n",
      "         -0.0135,  0.0291,  0.0542,  0.0610,  0.0602,  0.0538,  0.0104, -0.0105]])\n"
     ]
    }
   ],
   "source": [
    "# print the model weights\n",
    "for param in bnn_pynq_model.parameters():\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Split Model into Conv1D and FC Layers\n",
    "\n",
    "Given that FINN cannot synthesize Conv1D layers, we need to split the model into two components, so the FC layers can undergo synthesis and be accelerated through hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the software half\n",
    "class CNV_software(Module):\n",
    "\n",
    "    def __init__(self, num_classes=10, weight_bit_width=None, act_bit_width=None,in_bit_width=None, in_ch=3, device=\"cpu\"):\n",
    "        super(CNV_software, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        weight_quant_type = get_quant_type(weight_bit_width)\n",
    "        act_quant_type = get_quant_type(act_bit_width)\n",
    "        in_quant_type = get_quant_type(in_bit_width)\n",
    "        stats_op = get_stats_op(weight_quant_type)\n",
    "\n",
    "        self.conv_features = ModuleList()\n",
    "        self.conv_features.append(get_act_quant(in_bit_width, in_quant_type))\n",
    "\n",
    "        # convolution layers\n",
    "        for i, out_ch, is_pool_enabled in CNV_OUT_CH_POOL:\n",
    "            self.conv_features.append(get_quant_conv1d(in_ch=in_ch,\n",
    "                                                       out_ch=out_ch,\n",
    "                                                       bit_width=weight_bit_width,\n",
    "                                                       quant_type=weight_quant_type,\n",
    "                                                       stats_op=stats_op))\n",
    "            in_ch = out_ch\n",
    "            self.conv_features.append(BatchNorm1d(in_ch))\n",
    "            if i == (NUM_CONV_LAYERS - 1):\n",
    "                self.conv_features.append(Sequential())\n",
    "            else:\n",
    "                self.conv_features.append(get_act_quant(act_bit_width, act_quant_type))\n",
    "            if is_pool_enabled:\n",
    "                self.conv_features.append(MaxPool1d(kernel_size=2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = 2.0 * x - torch.tensor([1.0]).to(self.device)\n",
    "        for mod in self.conv_features:\n",
    "            x = mod(x)\n",
    "            \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "def cnv_software(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS):\n",
    "    net = CNV_software(weight_bit_width=WEIGHT_BIT_WIDTH,\n",
    "              act_bit_width=ACT_BIT_WIDTH,\n",
    "              in_bit_width=IN_BIT_WIDTH,\n",
    "              num_classes=NUM_CLASSES,\n",
    "              in_ch=IN_CHANNELS)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hardware half\n",
    "# declare the classes needed for the CNN, taken from: https://github.com/maltanar/brevitas_cnv_lfc\n",
    "# this is where the pre-trained models also come from, however, we will import the whole thing here to make custom CNNs\n",
    "class CNV_hardware(Module):\n",
    "\n",
    "    def __init__(self, num_classes=10, weight_bit_width=None, act_bit_width=None,in_bit_width=None, in_ch=3, device=\"cpu\"):\n",
    "        super(CNV_hardware, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        weight_quant_type = get_quant_type(weight_bit_width)\n",
    "        act_quant_type = get_quant_type(act_bit_width)\n",
    "        in_quant_type = get_quant_type(in_bit_width)\n",
    "        stats_op = get_stats_op(weight_quant_type)\n",
    "\n",
    "        self.linear_features = ModuleList()\n",
    "\n",
    "        # fully connected layers\n",
    "        self.linear_features.append(get_act_quant(in_bit_width, in_quant_type))\n",
    "        #in_features = reduce(mul, in_features)\n",
    "        \n",
    "        for in_features, out_features in INTERMEDIATE_FC_FEATURES:\n",
    "            self.linear_features.append(get_quant_linear(in_features=in_features,\n",
    "                                                         out_features=out_features,\n",
    "                                                         per_out_ch_scaling=INTERMEDIATE_FC_PER_OUT_CH_SCALING,\n",
    "                                                         bit_width=weight_bit_width,\n",
    "                                                         quant_type=weight_quant_type,\n",
    "                                                         stats_op=stats_op))\n",
    "            self.linear_features.append(BatchNorm1d(out_features))\n",
    "            self.linear_features.append(get_act_quant(act_bit_width, act_quant_type))\n",
    "            \n",
    "        # last layer\n",
    "        self.fc = get_quant_linear(in_features=LAST_FC_IN_FEATURES,\n",
    "                                   out_features=num_classes,\n",
    "                                   per_out_ch_scaling=LAST_FC_PER_OUT_CH_SCALING,\n",
    "                                   bit_width=weight_bit_width,\n",
    "                                   quant_type=weight_quant_type,\n",
    "                                   stats_op=stats_op)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        for mod in self.linear_features:\n",
    "            x = mod(x)\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "def cnv_hardware(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS):\n",
    "    net = CNV_hardware(weight_bit_width=WEIGHT_BIT_WIDTH,\n",
    "              act_bit_width=ACT_BIT_WIDTH,\n",
    "              in_bit_width=IN_BIT_WIDTH,\n",
    "              num_classes=NUM_CLASSES,\n",
    "              in_ch=IN_CHANNELS)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0003)\n",
      "tensor([[[ 0.1897, -0.2334,  0.0999,  0.3074],\n",
      "         [-0.1952, -0.1541, -0.0417,  0.1761]],\n",
      "\n",
      "        [[ 0.0153, -0.0848, -0.1458, -0.1087],\n",
      "         [-0.2014, -0.0028,  0.1153, -0.3012]],\n",
      "\n",
      "        [[ 0.2176,  0.0808, -0.2058, -0.4027],\n",
      "         [ 0.3052,  0.0084, -0.0639, -0.0401]],\n",
      "\n",
      "        [[-0.1632, -0.1746,  0.0811, -0.2844],\n",
      "         [ 0.1262,  0.0546,  0.3440,  0.0271]],\n",
      "\n",
      "        [[-0.0169, -0.2973,  0.1580, -0.2712],\n",
      "         [ 0.1108, -0.0332, -0.2981, -0.1849]],\n",
      "\n",
      "        [[ 0.0550, -0.1278,  0.3177,  0.2081],\n",
      "         [ 0.3411, -0.0240,  0.0873,  0.3508]],\n",
      "\n",
      "        [[ 0.3735, -0.3262, -0.2033, -0.1625],\n",
      "         [-0.2184, -0.3277,  0.1822, -0.3178]],\n",
      "\n",
      "        [[-0.3618,  0.1095, -0.0582, -0.3274],\n",
      "         [-0.0707, -0.2446,  0.1825,  0.0196]],\n",
      "\n",
      "        [[ 0.5567,  0.3586,  0.4280,  0.0125],\n",
      "         [ 0.2044,  0.1297,  0.1836,  0.5140]],\n",
      "\n",
      "        [[-0.2227, -0.2652, -0.3281,  0.2316],\n",
      "         [-0.0969,  0.1137,  0.3237,  0.0428]],\n",
      "\n",
      "        [[-0.2292,  0.3545, -0.2481, -0.1313],\n",
      "         [-0.0109, -0.2764,  0.2279, -0.0421]],\n",
      "\n",
      "        [[-0.1619, -0.3408,  0.1885, -0.1739],\n",
      "         [-0.0946,  0.1303,  0.2258,  0.1483]],\n",
      "\n",
      "        [[-0.3217, -0.2825, -0.2593, -0.2140],\n",
      "         [ 0.1682, -0.1490, -0.1466,  0.1254]],\n",
      "\n",
      "        [[-0.0753, -0.1722,  0.0682,  0.1318],\n",
      "         [-0.1064,  0.1933,  0.1202,  0.2505]],\n",
      "\n",
      "        [[-0.3033, -0.0485, -0.0818,  0.2196],\n",
      "         [ 0.1850, -0.2118,  0.1389, -0.3099]],\n",
      "\n",
      "        [[-0.0940, -0.0416, -0.1028,  0.1037],\n",
      "         [-0.3820, -0.4143, -0.2705, -0.3870]],\n",
      "\n",
      "        [[-0.1767,  0.0633,  0.2393, -0.0467],\n",
      "         [ 0.1146, -0.2771,  0.1257,  0.2515]],\n",
      "\n",
      "        [[-0.4260, -0.3725, -0.1222,  0.0327],\n",
      "         [ 0.1253,  0.1982,  0.1771, -0.1518]],\n",
      "\n",
      "        [[ 0.6329,  0.2152, -0.0590,  0.5411],\n",
      "         [ 0.0015,  0.4249,  0.3170,  0.1767]],\n",
      "\n",
      "        [[-0.1213, -0.2418, -0.0772,  0.0529],\n",
      "         [ 0.1654,  0.2767,  0.2188,  0.0040]],\n",
      "\n",
      "        [[-0.2805, -0.3065, -0.4494,  0.1145],\n",
      "         [-0.0163, -0.0606, -0.0162, -0.5569]],\n",
      "\n",
      "        [[ 0.2510,  0.0027,  0.3550,  0.0044],\n",
      "         [ 0.0635, -0.2826,  0.0859, -0.3044]],\n",
      "\n",
      "        [[ 0.3136, -0.1572,  0.1159,  0.1521],\n",
      "         [ 0.2505, -0.1516, -0.2199, -0.2146]],\n",
      "\n",
      "        [[ 0.3163, -0.2647,  0.0480, -0.2728],\n",
      "         [-0.2445, -0.0570,  0.0857,  0.0346]],\n",
      "\n",
      "        [[-0.0646,  0.0899, -0.2694,  0.0493],\n",
      "         [-0.1599, -0.2093,  0.0741, -0.1959]],\n",
      "\n",
      "        [[ 0.3493,  0.0193,  0.2312, -0.0740],\n",
      "         [-0.2955,  0.3051, -0.3263,  0.2119]],\n",
      "\n",
      "        [[-0.3502, -0.0416, -0.1404, -0.1195],\n",
      "         [-0.0299, -0.3361, -0.0773,  0.2634]],\n",
      "\n",
      "        [[ 0.1645,  0.3393,  0.0936,  0.2962],\n",
      "         [-0.0815, -0.1320, -0.0536, -0.1060]],\n",
      "\n",
      "        [[-0.2107,  0.1313,  0.1626,  0.3076],\n",
      "         [ 0.3323, -0.0889, -0.1927,  0.2776]],\n",
      "\n",
      "        [[ 0.8783,  0.7841,  0.5819,  1.0163],\n",
      "         [ 0.7107,  0.6753,  0.4500,  0.6693]],\n",
      "\n",
      "        [[ 0.0542, -0.1111,  0.1228,  0.2924],\n",
      "         [ 0.3229, -0.2052,  0.1485, -0.0550]],\n",
      "\n",
      "        [[ 0.3124, -0.1941,  0.2454, -0.2726],\n",
      "         [ 0.3196,  0.0242,  0.1752,  0.0366]]])\n",
      "tensor([ 0.6720,  0.1212,  0.9549,  0.9851,  0.1937,  0.9237,  0.3508,  0.5211,\n",
      "         0.2411,  0.2254,  0.9465,  0.0519,  0.9357,  0.9318,  0.4898,  0.5003,\n",
      "         0.6284,  0.3460,  0.3453,  0.2341,  0.5582,  0.5397,  0.1358,  0.4534,\n",
      "        -0.0581,  0.8381,  0.2520,  0.0843,  0.1619,  0.7505,  0.2298,  0.9439])\n",
      "tensor([ 9.4733e-03,  3.3529e-08,  1.7027e-02, -7.1186e-02,  1.2365e-04,\n",
      "         1.5541e-03,  4.1804e-03,  1.4618e-02, -2.7360e-04, -1.3480e-03,\n",
      "         2.7028e-02, -1.3929e-08, -3.1726e-02,  1.4247e-02,  1.4651e-02,\n",
      "         5.3564e-03,  6.4325e-02,  1.8548e-04,  1.8466e-02,  4.6000e-04,\n",
      "        -3.1336e-02, -2.6253e-02,  9.3159e-09, -2.6647e-03,  5.3055e-08,\n",
      "        -2.8414e-02, -2.3006e-04,  2.2786e-08,  7.1946e-05,  2.7084e-02,\n",
      "        -3.0086e-04, -2.2673e-02])\n",
      "tensor([[[-0.0104, -0.0665,  0.0708,  0.0794],\n",
      "         [-0.0429, -0.0614,  0.0456, -0.0412],\n",
      "         [ 0.0567,  0.0685,  0.0390,  0.0055],\n",
      "         ...,\n",
      "         [-0.0708,  0.0311, -0.0189,  0.0525],\n",
      "         [ 0.0412,  0.0133,  0.0547,  0.0064],\n",
      "         [-0.0461,  0.0404,  0.0066, -0.0265]],\n",
      "\n",
      "        [[ 0.0234,  0.0034, -0.0289,  0.0464],\n",
      "         [-0.0758, -0.0188,  0.0540,  0.0236],\n",
      "         [-0.0725, -0.0846,  0.0067, -0.0251],\n",
      "         ...,\n",
      "         [-0.0026,  0.0268,  0.0449,  0.0711],\n",
      "         [ 0.0486,  0.0298,  0.0631, -0.0007],\n",
      "         [-0.0530, -0.0635, -0.0440,  0.0119]],\n",
      "\n",
      "        [[-0.0809, -0.0560, -0.0228, -0.0610],\n",
      "         [-0.0047,  0.0671, -0.0092,  0.0497],\n",
      "         [-0.0003, -0.0137, -0.0335,  0.0507],\n",
      "         ...,\n",
      "         [-0.0597, -0.0228, -0.0463,  0.0469],\n",
      "         [ 0.0430, -0.0082,  0.0636,  0.0221],\n",
      "         [-0.0860, -0.0120,  0.0583,  0.0806]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0106,  0.0242, -0.0429, -0.0548],\n",
      "         [ 0.0658, -0.0632,  0.0137, -0.0663],\n",
      "         [ 0.0025, -0.0638,  0.0244, -0.0560],\n",
      "         ...,\n",
      "         [-0.0113, -0.0348, -0.0878, -0.0584],\n",
      "         [-0.0487,  0.0365,  0.0183, -0.0544],\n",
      "         [-0.0360, -0.0302, -0.0651, -0.0676]],\n",
      "\n",
      "        [[ 0.0221, -0.0690, -0.0088, -0.0413],\n",
      "         [-0.0307, -0.0498, -0.0862,  0.0042],\n",
      "         [-0.0532,  0.0952, -0.0725, -0.0893],\n",
      "         ...,\n",
      "         [ 0.0410,  0.0904, -0.0217,  0.0193],\n",
      "         [ 0.0260,  0.0829,  0.0171,  0.0445],\n",
      "         [-0.0793, -0.0566,  0.0639,  0.0573]],\n",
      "\n",
      "        [[ 0.0100,  0.0043,  0.0663,  0.0117],\n",
      "         [-0.0513, -0.0628,  0.0862, -0.0577],\n",
      "         [-0.0467, -0.0550, -0.0056,  0.0489],\n",
      "         ...,\n",
      "         [-0.0203, -0.0630, -0.0833, -0.0396],\n",
      "         [-0.0817, -0.0209,  0.0017,  0.0659],\n",
      "         [ 0.0441, -0.0868, -0.0195, -0.0393]]])\n",
      "tensor([ 0.6099,  0.6064,  0.5334,  0.7753,  0.5006,  0.7203,  0.3092,  0.1017,\n",
      "         0.7983,  0.6582,  0.6466, -0.0093,  1.0113,  0.5104,  0.8409,  0.5792,\n",
      "         0.6192,  0.2861,  0.4363,  0.4843,  0.7400,  0.1352,  0.6650,  0.3928,\n",
      "         0.7797,  0.4451,  0.0371,  0.8107,  0.5415,  0.4738,  0.1177,  0.6848,\n",
      "         0.2237,  0.3652,  0.8187,  0.8484,  0.4092,  0.4537,  0.5256,  0.0142,\n",
      "         0.8651,  0.9654,  0.8936,  0.6682,  0.0636,  0.5185,  0.6181,  1.0219,\n",
      "         0.5568,  0.7521,  0.1892,  0.0794,  0.3129,  0.4559,  0.9159,  0.7343,\n",
      "         0.3686,  0.6853,  0.1477,  0.6600,  0.7209,  0.8557,  0.4794,  0.0786])\n",
      "tensor([ 6.8105e-03, -3.4233e-02, -7.1883e-03, -3.4004e-02,  2.1691e-02,\n",
      "        -1.4135e-02, -3.7140e-05, -2.1095e-08, -1.5449e-02,  2.3593e-02,\n",
      "        -4.7134e-02,  4.0245e-08,  2.3771e-02,  3.0018e-03, -5.4255e-02,\n",
      "        -4.4829e-03,  5.6340e-02,  1.6736e-02, -4.8882e-03, -1.0183e-02,\n",
      "         2.7305e-02,  9.6606e-04,  3.6864e-02, -6.9262e-03,  1.7517e-02,\n",
      "        -2.6882e-02, -1.3067e-09,  2.3615e-02, -1.1998e-02, -8.0865e-03,\n",
      "         8.2686e-09, -2.2886e-02, -2.2789e-03,  1.1717e-03,  1.0119e-02,\n",
      "        -3.1491e-02, -2.2655e-03, -7.6693e-03, -6.0466e-03,  1.1652e-08,\n",
      "         2.8568e-02,  6.0571e-02,  2.9728e-03, -2.3599e-02, -1.1526e-08,\n",
      "        -6.5121e-03, -3.4653e-02,  5.3179e-02,  4.6727e-02, -1.9965e-02,\n",
      "        -1.2319e-08, -3.5979e-08, -6.9428e-03, -9.6534e-03,  3.3978e-02,\n",
      "        -7.0328e-03,  2.2896e-03, -1.1860e-02, -1.4776e-08, -2.2922e-02,\n",
      "        -2.3898e-03,  2.0707e-02, -2.8185e-02,  1.3478e-08])\n",
      "tensor([[[-0.0120, -0.0124, -0.0486,  0.0004],\n",
      "         [ 0.0151, -0.0455, -0.0303,  0.0106],\n",
      "         [-0.0372,  0.0585, -0.0471, -0.0112],\n",
      "         ...,\n",
      "         [ 0.0669, -0.0558,  0.0498,  0.0087],\n",
      "         [-0.0144, -0.0177, -0.0506,  0.0282],\n",
      "         [ 0.0589,  0.0218, -0.0022, -0.0241]],\n",
      "\n",
      "        [[-0.0176,  0.0224, -0.0242, -0.0209],\n",
      "         [-0.0615, -0.0478, -0.0549, -0.0091],\n",
      "         [-0.0328, -0.0034, -0.0116,  0.0337],\n",
      "         ...,\n",
      "         [-0.0202,  0.0196,  0.0179, -0.0185],\n",
      "         [ 0.0273,  0.0419, -0.0240, -0.0035],\n",
      "         [ 0.0085, -0.0316,  0.0602,  0.0398]],\n",
      "\n",
      "        [[ 0.0341,  0.0406,  0.0444,  0.0502],\n",
      "         [ 0.0086,  0.0236,  0.0274,  0.0449],\n",
      "         [-0.0024,  0.0716, -0.0399,  0.0316],\n",
      "         ...,\n",
      "         [-0.0696, -0.0038,  0.0127, -0.0117],\n",
      "         [ 0.0024,  0.0374,  0.0638,  0.0382],\n",
      "         [-0.0456, -0.0414, -0.0324, -0.0121]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0481, -0.0170,  0.0315, -0.0366],\n",
      "         [-0.0123, -0.0056, -0.0552,  0.0048],\n",
      "         [-0.0102, -0.0071,  0.0252,  0.0155],\n",
      "         ...,\n",
      "         [-0.0193, -0.0319,  0.0469,  0.0618],\n",
      "         [ 0.0316,  0.0561, -0.0093, -0.0354],\n",
      "         [ 0.0613, -0.0348, -0.0469,  0.0205]],\n",
      "\n",
      "        [[ 0.0365, -0.0314,  0.0123,  0.0069],\n",
      "         [ 0.0319,  0.0160,  0.0029,  0.0274],\n",
      "         [-0.0398, -0.0152,  0.0339,  0.0061],\n",
      "         ...,\n",
      "         [-0.0504,  0.0046,  0.0062,  0.0359],\n",
      "         [-0.0221,  0.0213, -0.0003, -0.0247],\n",
      "         [ 0.0230,  0.0599,  0.0050, -0.0527]],\n",
      "\n",
      "        [[-0.0350, -0.0049,  0.0281, -0.0291],\n",
      "         [ 0.0070,  0.0385,  0.0071, -0.0006],\n",
      "         [-0.0436, -0.0051,  0.0582, -0.0571],\n",
      "         ...,\n",
      "         [ 0.0009,  0.0007, -0.0198,  0.0557],\n",
      "         [ 0.0017, -0.0455,  0.0231, -0.0137],\n",
      "         [ 0.0527,  0.0554,  0.0057, -0.0049]]])\n",
      "tensor([ 0.4261,  0.7198,  1.0780,  0.9912,  0.9463,  0.2163,  0.7443,  0.7844,\n",
      "         0.1681,  0.5200,  0.5530,  0.5491,  0.8200,  0.7117,  0.6364,  0.1120,\n",
      "         0.6274,  0.9264,  0.9792,  0.0929,  0.5914,  0.3163,  0.6768,  0.4024,\n",
      "         0.7942,  0.6668,  0.6302,  0.3788,  0.6623,  0.5311,  0.3950,  0.9593,\n",
      "         0.5226,  0.2839,  0.7109,  0.1347,  0.1001,  0.4013,  0.1462,  0.6243,\n",
      "         0.3538,  0.3983,  0.7138,  0.4442,  0.4316,  0.8168,  0.5669,  0.6623,\n",
      "         0.3136,  0.0624,  0.6411,  0.9921,  1.1008,  0.4739,  0.1297,  0.4040,\n",
      "         0.3919,  0.2513, -0.0477,  0.1642,  0.0566,  0.2624,  0.3156,  0.8733,\n",
      "         0.0613,  0.3492,  0.6679,  0.1323,  0.7897,  0.4261,  0.6526,  0.2760,\n",
      "         0.2897,  0.3215,  0.9797,  0.8933,  1.0803,  1.0742,  0.4247,  0.8233,\n",
      "         0.0822,  0.4217,  0.0409,  0.2201,  0.4835,  0.7527,  0.1221,  0.1021,\n",
      "         0.9023,  0.6950,  0.8230,  0.0771,  0.0120,  0.6815,  0.2381,  0.4917,\n",
      "         0.5711,  0.6372,  0.1916,  0.7553,  0.4617,  0.3612,  1.0660,  0.1057,\n",
      "         0.9570,  0.0930,  0.7436,  0.2409,  0.9282,  0.4478,  0.6589,  0.5610,\n",
      "         0.5350,  0.3639,  1.0138,  0.0514,  0.8581,  0.5902,  0.1963,  0.1039,\n",
      "         0.7524,  0.9357,  0.8042,  1.0085,  0.2344,  0.0077,  0.0788,  0.2644])\n",
      "tensor([ 7.3677e-09, -2.2476e-08,  1.7152e-08,  4.2578e-09, -9.3677e-10,\n",
      "        -1.0118e-08,  3.9888e-09,  1.5765e-08, -1.3113e-08, -6.7535e-09,\n",
      "        -2.7671e-08, -4.7451e-09,  9.9612e-09, -5.6004e-09,  1.0394e-09,\n",
      "        -3.3642e-08, -1.9318e-08,  1.3062e-09,  6.1179e-10,  1.0392e-08,\n",
      "        -1.1741e-08, -5.0689e-11,  1.1978e-09,  3.2284e-08, -1.3253e-08,\n",
      "         3.9531e-08, -2.3085e-09,  3.2033e-08, -1.5258e-09, -3.9732e-08,\n",
      "         1.6399e-09,  9.8534e-09, -4.3635e-09,  2.9055e-08,  1.1860e-08,\n",
      "         5.0860e-09,  2.7464e-08,  7.9003e-09,  6.1376e-09,  1.9851e-08,\n",
      "        -2.3709e-08, -5.1731e-09, -4.1663e-09, -2.2178e-08,  1.2325e-08,\n",
      "        -1.3001e-08, -1.3296e-08,  1.6949e-08,  1.9939e-08, -6.2492e-09,\n",
      "         4.5817e-09,  2.1118e-09,  1.9523e-08, -1.7807e-08, -1.9612e-09,\n",
      "         6.7704e-09,  3.5268e-08,  3.4908e-08, -3.0771e-08, -1.1535e-08,\n",
      "         9.4420e-09, -3.3191e-09, -5.6055e-09,  1.7733e-08, -3.0216e-08,\n",
      "        -1.7834e-08,  1.4797e-08,  2.6025e-08,  5.0203e-09,  1.0325e-08,\n",
      "         1.9224e-08, -7.0254e-09,  9.8787e-09, -1.8044e-08,  1.6214e-08,\n",
      "         6.2701e-09, -6.7314e-09,  1.3024e-08, -2.7355e-08,  2.0102e-08,\n",
      "        -6.0567e-10, -3.5525e-08, -7.4576e-09,  2.9646e-08, -7.5982e-10,\n",
      "        -1.1633e-08,  1.1850e-08,  2.7593e-08, -1.7319e-08,  1.5100e-08,\n",
      "        -1.4409e-08,  8.5483e-09, -4.2634e-10,  2.6253e-08,  2.0392e-09,\n",
      "         8.2106e-09, -3.7893e-08, -1.7635e-09, -1.1529e-08, -3.1828e-09,\n",
      "         1.1764e-08, -6.7182e-09, -1.2200e-08,  1.8536e-08, -1.0676e-10,\n",
      "         3.7641e-09, -7.0753e-09,  6.8659e-09,  5.8407e-09,  2.0102e-09,\n",
      "         3.2588e-10, -1.1603e-08,  2.4049e-08,  1.9482e-09, -2.3104e-09,\n",
      "         2.6531e-08,  1.4454e-08, -2.0059e-08, -2.8582e-08, -7.8687e-09,\n",
      "         6.9055e-09, -6.8903e-09, -2.5888e-08, -2.2642e-08,  1.5862e-08,\n",
      "         7.4208e-09,  1.1866e-08, -7.8525e-09])\n",
      "tensor([[[ 2.1477e-02, -7.2598e-03, -3.8330e-02,  1.0516e-02],\n",
      "         [ 7.3999e-03, -2.6661e-02,  2.6349e-02,  1.1319e-02],\n",
      "         [-2.8740e-02, -3.5538e-03,  4.1835e-02,  1.4433e-02],\n",
      "         ...,\n",
      "         [-2.9939e-03,  3.0421e-02, -4.2425e-02, -4.3201e-03],\n",
      "         [-1.5329e-02,  1.6133e-02, -9.6477e-03, -2.5618e-02],\n",
      "         [-3.2468e-02,  1.2615e-02, -3.5957e-02,  6.6273e-03]],\n",
      "\n",
      "        [[-3.0738e-02,  2.8432e-02,  2.6320e-02, -1.9540e-02],\n",
      "         [-3.6625e-02,  1.5659e-03,  1.9759e-02, -1.3299e-02],\n",
      "         [ 5.4202e-03,  1.7177e-02, -1.4831e-02,  3.6438e-02],\n",
      "         ...,\n",
      "         [ 1.4714e-02,  3.0873e-02, -1.4963e-02, -4.1165e-02],\n",
      "         [ 3.0643e-02,  1.5603e-02,  7.2217e-03,  1.7222e-02],\n",
      "         [-1.7307e-02, -7.0022e-04,  2.7460e-02, -1.7924e-02]],\n",
      "\n",
      "        [[ 3.1187e-02, -8.6220e-03, -2.7170e-02, -7.5418e-03],\n",
      "         [-2.6981e-02,  2.3441e-02, -1.8208e-02,  3.9842e-02],\n",
      "         [ 2.9202e-02,  3.9894e-02,  1.0107e-02,  3.2406e-02],\n",
      "         ...,\n",
      "         [-2.2495e-02, -1.5927e-02, -1.7447e-02,  2.0811e-02],\n",
      "         [ 7.5863e-03,  2.0263e-02, -1.5956e-02, -3.1528e-02],\n",
      "         [ 2.7224e-02,  4.1734e-02,  5.2767e-03,  1.8579e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-8.9268e-03,  1.1433e-02,  1.3275e-02,  2.3859e-02],\n",
      "         [ 3.2108e-02, -3.3344e-02,  3.4955e-02,  2.1604e-02],\n",
      "         [-1.0099e-02,  3.4361e-05,  3.8562e-02,  1.8939e-02],\n",
      "         ...,\n",
      "         [ 4.2845e-02,  1.4775e-02, -3.7151e-02,  3.9216e-02],\n",
      "         [-9.7061e-03, -3.9192e-03, -1.4093e-02, -4.3451e-02],\n",
      "         [ 1.8041e-02,  1.2140e-03, -2.1024e-02, -4.1802e-02]],\n",
      "\n",
      "        [[ 2.4617e-02, -1.0012e-02, -1.2157e-02,  3.2291e-02],\n",
      "         [ 1.8251e-02, -4.6292e-02, -1.2662e-02,  3.3668e-02],\n",
      "         [-2.6287e-02,  4.1525e-02,  5.1949e-03,  4.6545e-02],\n",
      "         ...,\n",
      "         [-1.5569e-02, -1.5896e-02,  3.3865e-03, -1.8158e-02],\n",
      "         [ 1.1564e-02, -6.6662e-03,  2.6267e-02,  3.1095e-02],\n",
      "         [-1.6768e-02,  3.0561e-02,  5.8953e-03,  3.3094e-02]],\n",
      "\n",
      "        [[ 3.1216e-02, -8.4297e-03,  4.8117e-04,  3.0942e-02],\n",
      "         [-3.1545e-02,  3.2052e-02,  1.1769e-02, -2.7490e-02],\n",
      "         [-4.3905e-02, -2.6925e-02, -3.6366e-03,  1.1977e-02],\n",
      "         ...,\n",
      "         [-2.1439e-02,  3.1763e-02,  4.1630e-03,  3.2729e-02],\n",
      "         [-7.8290e-03,  3.4423e-02,  1.8132e-02,  3.0737e-02],\n",
      "         [-1.2849e-03,  4.1108e-02, -3.3902e-02, -8.2020e-03]]])\n",
      "tensor([ 0.0198,  0.1872,  0.0140,  0.9673,  0.5246,  0.9160,  0.7447,  0.6061,\n",
      "         0.1880, -0.0964,  0.1647, -0.0089,  0.1280,  0.9005,  0.6959,  0.5945,\n",
      "         0.8811,  0.0013,  0.1577,  0.6202,  0.9331,  0.9537,  0.6263,  0.2294,\n",
      "        -0.0069,  0.4901,  0.5467,  0.6169,  0.5931, -0.0137,  0.0632,  0.8429,\n",
      "         0.9931,  0.1622,  0.7109,  0.6646,  0.5253,  0.2864,  0.2550,  0.5500,\n",
      "         0.7402,  0.9749,  0.2653,  0.1596,  0.8102,  0.3910,  0.9566,  0.7371,\n",
      "         0.3460,  0.8638,  0.6666,  0.3099,  0.9709,  0.2859,  0.8192,  0.9712,\n",
      "         0.2100,  0.8063,  0.2795,  0.6780,  0.3783,  0.6148,  0.3997,  0.6320,\n",
      "         0.9796,  0.6512,  0.1287,  0.7698,  0.1345,  0.9443,  0.5824,  0.8037,\n",
      "         0.6156,  0.5835,  0.9828,  0.7632,  0.5094,  0.2028,  0.5803,  0.4755,\n",
      "         0.4002,  0.2887,  0.1712,  0.8527,  0.5484,  0.7753,  0.9201,  0.4843,\n",
      "         0.9034,  0.9286,  0.7881,  0.5003,  0.4158,  0.6391,  0.9042,  0.9461,\n",
      "         0.8483,  0.2452,  0.9042,  0.5162,  0.2028,  0.0813,  0.7179,  0.8829,\n",
      "         0.5308,  0.0256,  0.7785,  0.2655,  0.8478,  0.2044,  0.2260,  0.2621,\n",
      "        -0.0084,  0.9138,  0.9513,  0.9136,  0.9211,  0.7103,  0.1878,  0.4437,\n",
      "         0.0840,  0.7308,  0.6141,  0.3105,  0.9679,  0.9657,  0.0258,  0.3986,\n",
      "         0.2597,  0.2062,  0.1158,  0.0040,  0.4236,  0.5678,  0.2251,  0.9888,\n",
      "         0.4049,  0.6420,  0.5342,  0.3698,  0.0398,  0.8473,  0.4636,  0.5478,\n",
      "         0.2240,  0.3374,  0.7849,  0.4615,  0.1526,  0.5506,  0.8892,  0.3460,\n",
      "         0.0443,  0.6453,  0.2174,  0.4982,  0.3846,  0.7274,  0.6939,  0.0178,\n",
      "         0.9257,  0.9601,  0.6621,  0.2583,  0.9018,  0.0996,  0.4738,  0.6398,\n",
      "         0.0654,  0.6047,  0.9236,  0.8292,  0.3384,  0.0260,  0.9284,  0.3773,\n",
      "         0.7253,  0.9347,  0.9347,  0.7866,  0.5928,  0.7533,  0.8739,  0.2413,\n",
      "         0.8970,  0.2240,  0.2941,  0.0261,  0.6201,  0.1834,  0.3603,  0.5585,\n",
      "         0.0701,  0.8378,  0.0184,  0.3316,  0.9555,  0.9548,  0.0979,  0.3971,\n",
      "         0.9542,  0.6606,  0.2600,  0.5473, -0.0477,  0.2497,  0.4740,  0.9547,\n",
      "         0.1125,  0.9149,  0.3218,  0.3045,  0.2725,  0.6263,  0.5913,  0.1427,\n",
      "         0.1924,  0.7709,  0.2562,  0.8423,  0.7822,  0.8845,  0.4493,  0.0381,\n",
      "         0.3061,  0.4727,  0.5045,  0.1466,  0.1613,  0.9998,  0.0714,  0.5487,\n",
      "         0.7649,  0.8276,  0.0084,  0.5747,  0.6474,  0.3114,  0.3763,  0.0977,\n",
      "         0.1266,  0.5577,  0.8346,  0.5757,  0.5694,  0.6971,  0.2403,  0.1246,\n",
      "         0.1059,  0.0502,  0.5135, -0.0470, -0.0418,  0.0866,  0.6593,  0.0177])\n",
      "tensor([-3.9324e-09,  3.8917e-09,  5.4149e-09,  7.3805e-03, -4.3026e-03,\n",
      "        -3.0395e-02,  4.0487e-03,  1.8265e-02,  5.0267e-10,  5.7050e-09,\n",
      "        -7.4784e-10, -7.3202e-10,  3.7837e-09, -1.5051e-02,  1.0677e-02,\n",
      "         9.0868e-04, -1.1944e-02, -6.7095e-09,  3.9747e-09, -5.9111e-03,\n",
      "         1.6287e-03,  2.6386e-03,  4.1470e-03, -3.7346e-09,  1.7959e-09,\n",
      "        -2.1087e-03, -3.9597e-03,  2.3748e-02, -1.4570e-02,  3.3245e-09,\n",
      "         7.6373e-09, -5.6134e-03,  1.3050e-02, -5.1414e-09,  3.3872e-02,\n",
      "         1.0421e-02, -1.3021e-03, -6.4322e-09,  3.0432e-09,  2.5366e-04,\n",
      "        -1.9918e-02,  1.3636e-02, -1.0136e-08,  9.5297e-09,  1.7952e-03,\n",
      "         1.1147e-08,  3.8405e-02, -1.4032e-02, -2.9252e-09, -3.5127e-02,\n",
      "        -1.9501e-02, -1.8427e-09, -3.2558e-03, -1.5220e-09, -1.1924e-02,\n",
      "         6.2192e-03,  1.6000e-10,  8.0308e-03,  3.0872e-10, -4.9284e-03,\n",
      "        -6.5486e-09, -2.1253e-02, -8.7767e-09, -4.5835e-02,  1.1088e-02,\n",
      "         1.8146e-02,  1.0150e-09, -4.6475e-02,  2.6592e-09,  3.6606e-02,\n",
      "        -1.0215e-02,  3.9535e-02, -1.0142e-02, -2.3453e-04,  4.3369e-02,\n",
      "        -5.2496e-02,  7.6324e-03,  2.8500e-09, -7.4686e-03,  1.1391e-03,\n",
      "         1.1826e-03,  6.4275e-10,  2.4670e-09, -2.3057e-02,  1.1848e-02,\n",
      "         7.2833e-02, -3.2388e-02, -1.1624e-03,  3.5882e-02, -1.2240e-02,\n",
      "         6.6295e-03,  3.1693e-03,  3.5129e-04, -2.2141e-02, -1.3851e-03,\n",
      "         5.3958e-02, -6.9877e-04, -3.8950e-11, -1.2542e-02,  1.7771e-05,\n",
      "         9.8880e-09, -3.7502e-10,  2.0898e-02,  4.6051e-02,  1.5181e-02,\n",
      "         9.3101e-09,  3.6927e-02,  1.1907e-09,  2.1696e-03, -1.4342e-09,\n",
      "        -1.0215e-08,  8.9331e-10, -1.3034e-09,  6.1350e-02,  4.1102e-03,\n",
      "         4.1762e-02,  2.2893e-02,  8.5586e-03, -2.4624e-10,  6.2797e-10,\n",
      "        -3.1154e-09,  1.7041e-02, -1.4603e-02, -4.4219e-10, -5.6522e-02,\n",
      "        -4.9308e-02,  7.6909e-09, -1.8987e-09, -6.5371e-10, -1.0150e-08,\n",
      "        -2.5070e-09, -2.3132e-09, -4.0147e-03,  2.4050e-05,  6.5324e-09,\n",
      "        -5.5510e-02, -3.7288e-09, -1.3709e-02,  2.3231e-03, -5.9614e-09,\n",
      "         3.9437e-10, -6.6180e-04, -1.0997e-02,  4.0553e-03, -5.8356e-09,\n",
      "         2.7529e-10,  1.9303e-02, -2.4860e-03, -4.3234e-09, -2.0216e-03,\n",
      "         2.1411e-02,  2.3612e-09, -4.2069e-09, -2.1633e-02,  2.9805e-09,\n",
      "        -1.4984e-03, -5.8792e-09, -2.2535e-03,  3.9104e-03,  3.6510e-09,\n",
      "         1.3772e-02,  2.3921e-02,  2.3467e-02,  3.3330e-09, -2.1026e-02,\n",
      "         3.6763e-09, -4.3219e-04, -1.7134e-02,  2.3482e-10, -2.7737e-02,\n",
      "         9.5521e-03, -1.0829e-02,  5.7646e-10,  2.1488e-09,  1.0139e-03,\n",
      "        -2.1733e-09,  5.9569e-03,  2.2392e-03, -5.0784e-02,  2.6025e-02,\n",
      "         8.6990e-03, -1.0865e-03,  1.7956e-02, -1.3406e-08, -2.3219e-02,\n",
      "         2.5869e-09,  2.0546e-09, -8.3071e-10,  1.1656e-02,  2.1630e-09,\n",
      "         9.5235e-09,  1.3570e-02,  6.9514e-11,  2.3632e-02,  5.0659e-09,\n",
      "         3.7998e-09, -1.1710e-02,  8.5545e-03,  7.4973e-09, -6.1722e-09,\n",
      "         7.4295e-02,  1.6220e-02, -6.3669e-09, -4.4846e-03, -3.5012e-09,\n",
      "         7.0000e-09, -2.2262e-03, -3.5316e-02, -6.4507e-09, -2.0071e-02,\n",
      "         9.5734e-09, -6.8231e-09, -3.0490e-09,  2.3851e-02,  6.7452e-03,\n",
      "         4.1637e-09,  7.4904e-10, -3.8993e-03,  3.0024e-09,  9.2482e-03,\n",
      "         2.7173e-02, -1.3516e-02,  2.2919e-03, -4.3771e-09,  3.1925e-09,\n",
      "        -4.1574e-03, -2.9836e-04,  6.6991e-09, -3.9923e-09,  1.5224e-03,\n",
      "         1.5786e-09, -2.2606e-02, -1.0773e-02,  3.9869e-03, -2.5000e-09,\n",
      "         2.5679e-02, -1.7733e-02, -3.4456e-09,  6.2406e-10,  1.0751e-08,\n",
      "         7.6742e-09,  3.2066e-02, -3.5461e-03,  1.9225e-02, -3.3655e-03,\n",
      "         1.7894e-02,  9.3457e-09,  1.3499e-09, -8.6993e-09, -5.0240e-10,\n",
      "        -6.0033e-03,  3.2841e-09,  3.2437e-10, -4.1077e-10, -3.3977e-02,\n",
      "        -5.6551e-09])\n",
      "tensor(7.6729e-09)\n",
      "tensor([[-0.0611,  0.0073, -0.0404,  ...,  0.0440, -0.0452, -0.0538],\n",
      "        [ 0.0390,  0.0637,  0.0422,  ...,  0.0436, -0.0105,  0.0527],\n",
      "        [ 0.0454, -0.0108,  0.0622,  ..., -0.0375,  0.0298, -0.0261],\n",
      "        ...,\n",
      "        [-0.0379, -0.0025,  0.0092,  ...,  0.0450,  0.0396, -0.0111],\n",
      "        [-0.0467,  0.0342,  0.0264,  ...,  0.0069,  0.0363,  0.0063],\n",
      "        [ 0.0248,  0.0564,  0.0575,  ...,  0.0331,  0.0505, -0.0234]])\n",
      "tensor([0.7496, 0.8934, 0.1871, 0.1057, 0.3161, 0.8479, 0.0624, 0.2406, 0.0056,\n",
      "        0.3629, 0.0046, 0.6333, 0.7086, 0.5497, 0.2116, 0.2757, 0.3157, 0.2654,\n",
      "        0.3761, 0.4564, 0.8221, 0.3484, 0.3425, 0.2576, 0.6260, 0.0064, 0.4747,\n",
      "        0.9150, 0.8757, 0.2886, 0.1473, 0.7848, 0.8206, 1.0016, 0.0316, 0.7795,\n",
      "        0.6581, 0.3581, 0.7360, 0.2832, 0.7443, 0.1829, 0.8870, 0.2779, 0.1996,\n",
      "        0.1493, 0.0883, 0.7847, 0.6402, 0.3174, 0.8474, 0.5311, 0.0142, 0.4570,\n",
      "        0.4395, 0.8889, 0.1042, 0.9010, 0.8509, 0.1338, 0.0554, 0.9405, 0.1003,\n",
      "        0.1446, 0.2466, 0.9328, 0.1139, 0.1505, 0.8214, 0.1783, 0.5036, 0.2966,\n",
      "        0.3109, 0.6613, 0.2282, 0.0760, 0.6762, 0.3348, 0.7103, 0.0306, 0.9278,\n",
      "        0.2958, 0.1366, 0.3024, 0.8626, 0.4674, 0.4964, 0.0036, 0.6570, 0.8381,\n",
      "        0.1130, 0.1410, 0.4958, 0.5304, 0.6742, 0.7433, 0.9827, 0.2290, 0.0955,\n",
      "        0.0208, 0.8099, 0.3126, 0.0757, 0.0795, 0.5072, 0.4606, 0.0323, 0.3481,\n",
      "        0.9037, 0.5607, 0.0035, 0.7272, 0.0093, 0.8001, 0.1021, 0.5171, 0.8603,\n",
      "        0.2270, 0.4093, 0.8530, 0.0429, 0.5050, 0.4214, 0.8977, 0.3385, 0.5664,\n",
      "        0.6649, 0.7393])\n",
      "tensor([ 3.0495e-03, -1.7773e-03, -1.0217e-03,  1.5273e-10,  3.7738e-04,\n",
      "        -1.0485e-03, -6.3257e-04, -2.3011e-03, -9.6591e-04,  5.4447e-04,\n",
      "         2.3715e-03,  1.4906e-03, -4.1597e-04,  4.1421e-04, -1.2137e-03,\n",
      "        -1.5273e-10, -1.5273e-10,  1.5273e-10,  4.4224e-03,  5.9378e-04,\n",
      "         9.1178e-04, -3.0416e-03, -1.9380e-03,  7.2770e-04, -2.4050e-04,\n",
      "         1.5273e-10,  1.5273e-10,  1.0485e-03, -6.0807e-07,  3.1693e-04,\n",
      "        -1.5273e-10,  1.5273e-10, -2.3698e-03,  9.4527e-04,  1.8842e-03,\n",
      "        -5.7162e-04,  1.9078e-03,  1.0026e-03,  6.3257e-04,  2.7609e-03,\n",
      "         1.5273e-10,  1.0485e-03,  5.4025e-04, -4.1597e-04, -1.5273e-10,\n",
      "        -4.1597e-04,  4.1695e-04, -4.4578e-04, -4.1597e-04, -4.1597e-04,\n",
      "         5.6299e-03,  5.5138e-04,  2.1461e-03, -6.3257e-04,  2.9429e-03,\n",
      "         1.5273e-10,  4.2521e-04,  2.1672e-04,  3.2746e-03, -6.3257e-04,\n",
      "         1.5273e-10,  4.1597e-04, -6.9545e-04, -7.8447e-04, -1.1360e-03,\n",
      "        -9.9138e-04, -4.3121e-04,  4.1597e-04, -1.5273e-10,  1.5273e-10,\n",
      "         2.3544e-03,  1.0693e-03,  6.3257e-04,  2.7782e-03, -1.9353e-03,\n",
      "         4.4681e-04, -4.1597e-04, -8.8115e-04, -1.5273e-10, -1.5273e-10,\n",
      "         6.3257e-04, -1.8801e-03, -1.0158e-03, -2.9183e-03, -1.7716e-03,\n",
      "         6.3257e-04, -5.2883e-04, -1.0485e-03, -4.1586e-03, -1.5273e-10,\n",
      "         2.3085e-03, -6.1121e-03,  4.1597e-04, -1.8697e-03,  1.2599e-03,\n",
      "         1.7093e-03,  9.7747e-04,  1.0485e-03, -3.0561e-03,  6.3257e-04,\n",
      "        -1.0485e-03,  1.4437e-03,  2.3393e-03, -2.3775e-03, -9.7221e-04,\n",
      "        -4.1597e-04, -6.3257e-04,  1.1208e-03,  1.6643e-03,  8.2889e-04,\n",
      "        -2.4021e-03,  6.3257e-04,  1.3080e-03,  1.0485e-03, -1.3125e-03,\n",
      "         8.0791e-04,  2.7962e-03, -6.3257e-04, -4.9263e-04,  1.6748e-03,\n",
      "         6.1262e-05,  1.6871e-03, -1.8869e-03,  8.0945e-04,  1.5273e-10,\n",
      "        -6.3257e-04, -1.0485e-03, -5.6421e-04])\n",
      "tensor([[-0.0138,  0.0212,  0.0139,  0.0973, -0.0107, -0.0319, -0.0626,  0.0697,\n",
      "         -0.0294,  0.0134, -0.0169, -0.0194,  0.1019,  0.0349,  0.0275, -0.0267,\n",
      "         -0.0454,  0.0397, -0.0070, -0.0224, -0.0332, -0.0072,  0.0591,  0.0400,\n",
      "          0.0451,  0.0761,  0.0198,  0.0207, -0.0503,  0.0593, -0.0594,  0.0644,\n",
      "         -0.0183,  0.0476,  0.0321,  0.0614, -0.0163,  0.0654,  0.0243,  0.0209,\n",
      "          0.0224,  0.0571, -0.0674,  0.0716, -0.0509,  0.0948, -0.0466,  0.0120,\n",
      "          0.0836,  0.0652, -0.0278, -0.0693,  0.0081, -0.0204,  0.0307,  0.0322,\n",
      "         -0.0256, -0.0561,  0.0287, -0.0353,  0.0438, -0.0978, -0.0293,  0.0197,\n",
      "         -0.0727, -0.0490,  0.0266, -0.0137, -0.0397,  0.0823,  0.0438,  0.0663,\n",
      "          0.0419, -0.0388, -0.0279, -0.0117,  0.0809, -0.0776, -0.0997, -0.0628,\n",
      "          0.0513,  0.0228,  0.0123, -0.0504,  0.0275,  0.0204, -0.0291, -0.0307,\n",
      "         -0.0768, -0.0512, -0.0767,  0.0061, -0.0468, -0.0607, -0.0201,  0.0132,\n",
      "          0.0157,  0.0739, -0.0197,  0.0607, -0.0729,  0.0397,  0.0377,  0.0145,\n",
      "          0.0258,  0.0211, -0.0583, -0.0160, -0.0340, -0.0506,  0.0092,  0.0581,\n",
      "          0.0982,  0.0906, -0.0227,  0.0143, -0.0202, -0.0398, -0.0609, -0.0134,\n",
      "          0.0530,  0.0577, -0.0520, -0.0766,  0.0503, -0.0134, -0.0319, -0.0226],\n",
      "        [ 0.0737, -0.0200, -0.0664,  0.0175,  0.0726, -0.0660,  0.0766, -0.0970,\n",
      "         -0.0113,  0.0874, -0.0750,  0.0204, -0.0393, -0.0186,  0.0105, -0.0735,\n",
      "         -0.0148,  0.0908, -0.0635, -0.0167, -0.0314, -0.0453, -0.0455, -0.0351,\n",
      "          0.0869,  0.0808,  0.0296,  0.0716, -0.0482,  0.0513, -0.0487,  0.0091,\n",
      "          0.0657, -0.0504, -0.0198,  0.0811,  0.0589,  0.0252, -0.0354, -0.0642,\n",
      "          0.0862,  0.0256, -0.0127, -0.0318, -0.0759, -0.0570, -0.0106, -0.0666,\n",
      "         -0.0298, -0.0209,  0.0203, -0.0523,  0.0201,  0.0994,  0.0280,  0.0253,\n",
      "         -0.0620, -0.0332,  0.0404,  0.0497,  0.0148,  0.0923, -0.0056,  0.0266,\n",
      "          0.0629, -0.0526, -0.0110,  0.0311, -0.0295,  0.0457, -0.0342,  0.0066,\n",
      "         -0.0432,  0.0095, -0.0301,  0.0527, -0.0601,  0.0269, -0.0143, -0.0424,\n",
      "         -0.0358,  0.0353,  0.0380, -0.0159,  0.0574, -0.0279,  0.0225, -0.0179,\n",
      "          0.0076, -0.0551,  0.0402,  0.0412,  0.0758,  0.0265, -0.0349, -0.0225,\n",
      "         -0.0508,  0.0370, -0.0380, -0.0100, -0.0753, -0.0969, -0.0184,  0.0935,\n",
      "         -0.0818, -0.0692,  0.0423,  0.0160, -0.0311, -0.0214, -0.0570, -0.0253,\n",
      "         -0.0163,  0.0169,  0.0206, -0.0407,  0.0180,  0.0562,  0.0231,  0.0408,\n",
      "         -0.0290,  0.0504,  0.0073, -0.0223,  0.0408,  0.0230, -0.0800, -0.0260],\n",
      "        [-0.0309,  0.0272,  0.0811,  0.0670, -0.0618,  0.0430,  0.0260, -0.0139,\n",
      "          0.0628,  0.0123, -0.0209, -0.0653,  0.0393,  0.0199,  0.0464, -0.0179,\n",
      "         -0.0292,  0.0435, -0.0072, -0.0118, -0.0201,  0.0026, -0.0267,  0.0236,\n",
      "          0.0144,  0.0477,  0.0613, -0.0341,  0.0110, -0.0105, -0.0611,  0.0663,\n",
      "         -0.0096,  0.0484,  0.0682, -0.0125, -0.0640,  0.0105, -0.0393,  0.0166,\n",
      "          0.0521, -0.0514, -0.0382,  0.0521, -0.0149,  0.0165,  0.0531,  0.0644,\n",
      "          0.0363,  0.0749,  0.0841,  0.0136, -0.0579,  0.0151,  0.0133,  0.0663,\n",
      "         -0.0586,  0.0095, -0.0188,  0.0089,  0.0622, -0.0233,  0.0501,  0.0323,\n",
      "         -0.0165,  0.0085,  0.0690, -0.0810, -0.0399,  0.0533, -0.0625,  0.0253,\n",
      "         -0.0240,  0.0343, -0.0155,  0.0065,  0.0541, -0.0229, -0.0225, -0.0750,\n",
      "         -0.0386,  0.0327,  0.0076,  0.0236,  0.0735, -0.0348, -0.0112,  0.0514,\n",
      "          0.0255, -0.0685,  0.0112, -0.0251, -0.0486, -0.0205,  0.0837, -0.0569,\n",
      "          0.0172, -0.0578, -0.0180, -0.0365,  0.0666,  0.0144,  0.0482, -0.0133,\n",
      "          0.0104,  0.0564,  0.0124, -0.0725, -0.0147,  0.0207, -0.0183, -0.0408,\n",
      "          0.0095, -0.0602, -0.0415,  0.0082, -0.0650,  0.0211,  0.0168, -0.0440,\n",
      "         -0.0135,  0.0291,  0.0542,  0.0610,  0.0602,  0.0538,  0.0104, -0.0105]])\n"
     ]
    }
   ],
   "source": [
    "# generate both models\n",
    "cnv_software_model = cnv_software(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS)\n",
    "cnv_hardware_model = cnv_hardware(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS)\n",
    "\n",
    "# split the layers of the original model\n",
    "cnv_pretrained_model = cnv_manual(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS)\n",
    "cnv_pretrained_model.load_state_dict(torch.load(build_dir + model_path))\n",
    "cnv_pretrained_model.eval()\n",
    "\n",
    "# print the model weights for reference if needed\n",
    "for param in cnv_pretrained_model.parameters():\n",
    "    print(param.data)\n",
    "    \n",
    "# copy over the layers\n",
    "cnv_software_model.conv_features = cnv_pretrained_model.conv_features\n",
    "cnv_hardware_model.linear_features = cnv_pretrained_model.linear_features\n",
    "cnv_hardware_model.fc = cnv_pretrained_model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "software\n",
      "tensor(-0.0003)\n",
      "tensor([[[ 0.1897, -0.2334,  0.0999,  0.3074],\n",
      "         [-0.1952, -0.1541, -0.0417,  0.1761]],\n",
      "\n",
      "        [[ 0.0153, -0.0848, -0.1458, -0.1087],\n",
      "         [-0.2014, -0.0028,  0.1153, -0.3012]],\n",
      "\n",
      "        [[ 0.2176,  0.0808, -0.2058, -0.4027],\n",
      "         [ 0.3052,  0.0084, -0.0639, -0.0401]],\n",
      "\n",
      "        [[-0.1632, -0.1746,  0.0811, -0.2844],\n",
      "         [ 0.1262,  0.0546,  0.3440,  0.0271]],\n",
      "\n",
      "        [[-0.0169, -0.2973,  0.1580, -0.2712],\n",
      "         [ 0.1108, -0.0332, -0.2981, -0.1849]],\n",
      "\n",
      "        [[ 0.0550, -0.1278,  0.3177,  0.2081],\n",
      "         [ 0.3411, -0.0240,  0.0873,  0.3508]],\n",
      "\n",
      "        [[ 0.3735, -0.3262, -0.2033, -0.1625],\n",
      "         [-0.2184, -0.3277,  0.1822, -0.3178]],\n",
      "\n",
      "        [[-0.3618,  0.1095, -0.0582, -0.3274],\n",
      "         [-0.0707, -0.2446,  0.1825,  0.0196]],\n",
      "\n",
      "        [[ 0.5567,  0.3586,  0.4280,  0.0125],\n",
      "         [ 0.2044,  0.1297,  0.1836,  0.5140]],\n",
      "\n",
      "        [[-0.2227, -0.2652, -0.3281,  0.2316],\n",
      "         [-0.0969,  0.1137,  0.3237,  0.0428]],\n",
      "\n",
      "        [[-0.2292,  0.3545, -0.2481, -0.1313],\n",
      "         [-0.0109, -0.2764,  0.2279, -0.0421]],\n",
      "\n",
      "        [[-0.1619, -0.3408,  0.1885, -0.1739],\n",
      "         [-0.0946,  0.1303,  0.2258,  0.1483]],\n",
      "\n",
      "        [[-0.3217, -0.2825, -0.2593, -0.2140],\n",
      "         [ 0.1682, -0.1490, -0.1466,  0.1254]],\n",
      "\n",
      "        [[-0.0753, -0.1722,  0.0682,  0.1318],\n",
      "         [-0.1064,  0.1933,  0.1202,  0.2505]],\n",
      "\n",
      "        [[-0.3033, -0.0485, -0.0818,  0.2196],\n",
      "         [ 0.1850, -0.2118,  0.1389, -0.3099]],\n",
      "\n",
      "        [[-0.0940, -0.0416, -0.1028,  0.1037],\n",
      "         [-0.3820, -0.4143, -0.2705, -0.3870]],\n",
      "\n",
      "        [[-0.1767,  0.0633,  0.2393, -0.0467],\n",
      "         [ 0.1146, -0.2771,  0.1257,  0.2515]],\n",
      "\n",
      "        [[-0.4260, -0.3725, -0.1222,  0.0327],\n",
      "         [ 0.1253,  0.1982,  0.1771, -0.1518]],\n",
      "\n",
      "        [[ 0.6329,  0.2152, -0.0590,  0.5411],\n",
      "         [ 0.0015,  0.4249,  0.3170,  0.1767]],\n",
      "\n",
      "        [[-0.1213, -0.2418, -0.0772,  0.0529],\n",
      "         [ 0.1654,  0.2767,  0.2188,  0.0040]],\n",
      "\n",
      "        [[-0.2805, -0.3065, -0.4494,  0.1145],\n",
      "         [-0.0163, -0.0606, -0.0162, -0.5569]],\n",
      "\n",
      "        [[ 0.2510,  0.0027,  0.3550,  0.0044],\n",
      "         [ 0.0635, -0.2826,  0.0859, -0.3044]],\n",
      "\n",
      "        [[ 0.3136, -0.1572,  0.1159,  0.1521],\n",
      "         [ 0.2505, -0.1516, -0.2199, -0.2146]],\n",
      "\n",
      "        [[ 0.3163, -0.2647,  0.0480, -0.2728],\n",
      "         [-0.2445, -0.0570,  0.0857,  0.0346]],\n",
      "\n",
      "        [[-0.0646,  0.0899, -0.2694,  0.0493],\n",
      "         [-0.1599, -0.2093,  0.0741, -0.1959]],\n",
      "\n",
      "        [[ 0.3493,  0.0193,  0.2312, -0.0740],\n",
      "         [-0.2955,  0.3051, -0.3263,  0.2119]],\n",
      "\n",
      "        [[-0.3502, -0.0416, -0.1404, -0.1195],\n",
      "         [-0.0299, -0.3361, -0.0773,  0.2634]],\n",
      "\n",
      "        [[ 0.1645,  0.3393,  0.0936,  0.2962],\n",
      "         [-0.0815, -0.1320, -0.0536, -0.1060]],\n",
      "\n",
      "        [[-0.2107,  0.1313,  0.1626,  0.3076],\n",
      "         [ 0.3323, -0.0889, -0.1927,  0.2776]],\n",
      "\n",
      "        [[ 0.8783,  0.7841,  0.5819,  1.0163],\n",
      "         [ 0.7107,  0.6753,  0.4500,  0.6693]],\n",
      "\n",
      "        [[ 0.0542, -0.1111,  0.1228,  0.2924],\n",
      "         [ 0.3229, -0.2052,  0.1485, -0.0550]],\n",
      "\n",
      "        [[ 0.3124, -0.1941,  0.2454, -0.2726],\n",
      "         [ 0.3196,  0.0242,  0.1752,  0.0366]]])\n",
      "tensor([ 0.6720,  0.1212,  0.9549,  0.9851,  0.1937,  0.9237,  0.3508,  0.5211,\n",
      "         0.2411,  0.2254,  0.9465,  0.0519,  0.9357,  0.9318,  0.4898,  0.5003,\n",
      "         0.6284,  0.3460,  0.3453,  0.2341,  0.5582,  0.5397,  0.1358,  0.4534,\n",
      "        -0.0581,  0.8381,  0.2520,  0.0843,  0.1619,  0.7505,  0.2298,  0.9439])\n",
      "tensor([ 9.4733e-03,  3.3529e-08,  1.7027e-02, -7.1186e-02,  1.2365e-04,\n",
      "         1.5541e-03,  4.1804e-03,  1.4618e-02, -2.7360e-04, -1.3480e-03,\n",
      "         2.7028e-02, -1.3929e-08, -3.1726e-02,  1.4247e-02,  1.4651e-02,\n",
      "         5.3564e-03,  6.4325e-02,  1.8548e-04,  1.8466e-02,  4.6000e-04,\n",
      "        -3.1336e-02, -2.6253e-02,  9.3159e-09, -2.6647e-03,  5.3055e-08,\n",
      "        -2.8414e-02, -2.3006e-04,  2.2786e-08,  7.1946e-05,  2.7084e-02,\n",
      "        -3.0086e-04, -2.2673e-02])\n",
      "tensor([[[-0.0104, -0.0665,  0.0708,  0.0794],\n",
      "         [-0.0429, -0.0614,  0.0456, -0.0412],\n",
      "         [ 0.0567,  0.0685,  0.0390,  0.0055],\n",
      "         ...,\n",
      "         [-0.0708,  0.0311, -0.0189,  0.0525],\n",
      "         [ 0.0412,  0.0133,  0.0547,  0.0064],\n",
      "         [-0.0461,  0.0404,  0.0066, -0.0265]],\n",
      "\n",
      "        [[ 0.0234,  0.0034, -0.0289,  0.0464],\n",
      "         [-0.0758, -0.0188,  0.0540,  0.0236],\n",
      "         [-0.0725, -0.0846,  0.0067, -0.0251],\n",
      "         ...,\n",
      "         [-0.0026,  0.0268,  0.0449,  0.0711],\n",
      "         [ 0.0486,  0.0298,  0.0631, -0.0007],\n",
      "         [-0.0530, -0.0635, -0.0440,  0.0119]],\n",
      "\n",
      "        [[-0.0809, -0.0560, -0.0228, -0.0610],\n",
      "         [-0.0047,  0.0671, -0.0092,  0.0497],\n",
      "         [-0.0003, -0.0137, -0.0335,  0.0507],\n",
      "         ...,\n",
      "         [-0.0597, -0.0228, -0.0463,  0.0469],\n",
      "         [ 0.0430, -0.0082,  0.0636,  0.0221],\n",
      "         [-0.0860, -0.0120,  0.0583,  0.0806]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0106,  0.0242, -0.0429, -0.0548],\n",
      "         [ 0.0658, -0.0632,  0.0137, -0.0663],\n",
      "         [ 0.0025, -0.0638,  0.0244, -0.0560],\n",
      "         ...,\n",
      "         [-0.0113, -0.0348, -0.0878, -0.0584],\n",
      "         [-0.0487,  0.0365,  0.0183, -0.0544],\n",
      "         [-0.0360, -0.0302, -0.0651, -0.0676]],\n",
      "\n",
      "        [[ 0.0221, -0.0690, -0.0088, -0.0413],\n",
      "         [-0.0307, -0.0498, -0.0862,  0.0042],\n",
      "         [-0.0532,  0.0952, -0.0725, -0.0893],\n",
      "         ...,\n",
      "         [ 0.0410,  0.0904, -0.0217,  0.0193],\n",
      "         [ 0.0260,  0.0829,  0.0171,  0.0445],\n",
      "         [-0.0793, -0.0566,  0.0639,  0.0573]],\n",
      "\n",
      "        [[ 0.0100,  0.0043,  0.0663,  0.0117],\n",
      "         [-0.0513, -0.0628,  0.0862, -0.0577],\n",
      "         [-0.0467, -0.0550, -0.0056,  0.0489],\n",
      "         ...,\n",
      "         [-0.0203, -0.0630, -0.0833, -0.0396],\n",
      "         [-0.0817, -0.0209,  0.0017,  0.0659],\n",
      "         [ 0.0441, -0.0868, -0.0195, -0.0393]]])\n",
      "tensor([ 0.6099,  0.6064,  0.5334,  0.7753,  0.5006,  0.7203,  0.3092,  0.1017,\n",
      "         0.7983,  0.6582,  0.6466, -0.0093,  1.0113,  0.5104,  0.8409,  0.5792,\n",
      "         0.6192,  0.2861,  0.4363,  0.4843,  0.7400,  0.1352,  0.6650,  0.3928,\n",
      "         0.7797,  0.4451,  0.0371,  0.8107,  0.5415,  0.4738,  0.1177,  0.6848,\n",
      "         0.2237,  0.3652,  0.8187,  0.8484,  0.4092,  0.4537,  0.5256,  0.0142,\n",
      "         0.8651,  0.9654,  0.8936,  0.6682,  0.0636,  0.5185,  0.6181,  1.0219,\n",
      "         0.5568,  0.7521,  0.1892,  0.0794,  0.3129,  0.4559,  0.9159,  0.7343,\n",
      "         0.3686,  0.6853,  0.1477,  0.6600,  0.7209,  0.8557,  0.4794,  0.0786])\n",
      "tensor([ 6.8105e-03, -3.4233e-02, -7.1883e-03, -3.4004e-02,  2.1691e-02,\n",
      "        -1.4135e-02, -3.7140e-05, -2.1095e-08, -1.5449e-02,  2.3593e-02,\n",
      "        -4.7134e-02,  4.0245e-08,  2.3771e-02,  3.0018e-03, -5.4255e-02,\n",
      "        -4.4829e-03,  5.6340e-02,  1.6736e-02, -4.8882e-03, -1.0183e-02,\n",
      "         2.7305e-02,  9.6606e-04,  3.6864e-02, -6.9262e-03,  1.7517e-02,\n",
      "        -2.6882e-02, -1.3067e-09,  2.3615e-02, -1.1998e-02, -8.0865e-03,\n",
      "         8.2686e-09, -2.2886e-02, -2.2789e-03,  1.1717e-03,  1.0119e-02,\n",
      "        -3.1491e-02, -2.2655e-03, -7.6693e-03, -6.0466e-03,  1.1652e-08,\n",
      "         2.8568e-02,  6.0571e-02,  2.9728e-03, -2.3599e-02, -1.1526e-08,\n",
      "        -6.5121e-03, -3.4653e-02,  5.3179e-02,  4.6727e-02, -1.9965e-02,\n",
      "        -1.2319e-08, -3.5979e-08, -6.9428e-03, -9.6534e-03,  3.3978e-02,\n",
      "        -7.0328e-03,  2.2896e-03, -1.1860e-02, -1.4776e-08, -2.2922e-02,\n",
      "        -2.3898e-03,  2.0707e-02, -2.8185e-02,  1.3478e-08])\n",
      "tensor([[[-0.0120, -0.0124, -0.0486,  0.0004],\n",
      "         [ 0.0151, -0.0455, -0.0303,  0.0106],\n",
      "         [-0.0372,  0.0585, -0.0471, -0.0112],\n",
      "         ...,\n",
      "         [ 0.0669, -0.0558,  0.0498,  0.0087],\n",
      "         [-0.0144, -0.0177, -0.0506,  0.0282],\n",
      "         [ 0.0589,  0.0218, -0.0022, -0.0241]],\n",
      "\n",
      "        [[-0.0176,  0.0224, -0.0242, -0.0209],\n",
      "         [-0.0615, -0.0478, -0.0549, -0.0091],\n",
      "         [-0.0328, -0.0034, -0.0116,  0.0337],\n",
      "         ...,\n",
      "         [-0.0202,  0.0196,  0.0179, -0.0185],\n",
      "         [ 0.0273,  0.0419, -0.0240, -0.0035],\n",
      "         [ 0.0085, -0.0316,  0.0602,  0.0398]],\n",
      "\n",
      "        [[ 0.0341,  0.0406,  0.0444,  0.0502],\n",
      "         [ 0.0086,  0.0236,  0.0274,  0.0449],\n",
      "         [-0.0024,  0.0716, -0.0399,  0.0316],\n",
      "         ...,\n",
      "         [-0.0696, -0.0038,  0.0127, -0.0117],\n",
      "         [ 0.0024,  0.0374,  0.0638,  0.0382],\n",
      "         [-0.0456, -0.0414, -0.0324, -0.0121]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0481, -0.0170,  0.0315, -0.0366],\n",
      "         [-0.0123, -0.0056, -0.0552,  0.0048],\n",
      "         [-0.0102, -0.0071,  0.0252,  0.0155],\n",
      "         ...,\n",
      "         [-0.0193, -0.0319,  0.0469,  0.0618],\n",
      "         [ 0.0316,  0.0561, -0.0093, -0.0354],\n",
      "         [ 0.0613, -0.0348, -0.0469,  0.0205]],\n",
      "\n",
      "        [[ 0.0365, -0.0314,  0.0123,  0.0069],\n",
      "         [ 0.0319,  0.0160,  0.0029,  0.0274],\n",
      "         [-0.0398, -0.0152,  0.0339,  0.0061],\n",
      "         ...,\n",
      "         [-0.0504,  0.0046,  0.0062,  0.0359],\n",
      "         [-0.0221,  0.0213, -0.0003, -0.0247],\n",
      "         [ 0.0230,  0.0599,  0.0050, -0.0527]],\n",
      "\n",
      "        [[-0.0350, -0.0049,  0.0281, -0.0291],\n",
      "         [ 0.0070,  0.0385,  0.0071, -0.0006],\n",
      "         [-0.0436, -0.0051,  0.0582, -0.0571],\n",
      "         ...,\n",
      "         [ 0.0009,  0.0007, -0.0198,  0.0557],\n",
      "         [ 0.0017, -0.0455,  0.0231, -0.0137],\n",
      "         [ 0.0527,  0.0554,  0.0057, -0.0049]]])\n",
      "tensor([ 0.4261,  0.7198,  1.0780,  0.9912,  0.9463,  0.2163,  0.7443,  0.7844,\n",
      "         0.1681,  0.5200,  0.5530,  0.5491,  0.8200,  0.7117,  0.6364,  0.1120,\n",
      "         0.6274,  0.9264,  0.9792,  0.0929,  0.5914,  0.3163,  0.6768,  0.4024,\n",
      "         0.7942,  0.6668,  0.6302,  0.3788,  0.6623,  0.5311,  0.3950,  0.9593,\n",
      "         0.5226,  0.2839,  0.7109,  0.1347,  0.1001,  0.4013,  0.1462,  0.6243,\n",
      "         0.3538,  0.3983,  0.7138,  0.4442,  0.4316,  0.8168,  0.5669,  0.6623,\n",
      "         0.3136,  0.0624,  0.6411,  0.9921,  1.1008,  0.4739,  0.1297,  0.4040,\n",
      "         0.3919,  0.2513, -0.0477,  0.1642,  0.0566,  0.2624,  0.3156,  0.8733,\n",
      "         0.0613,  0.3492,  0.6679,  0.1323,  0.7897,  0.4261,  0.6526,  0.2760,\n",
      "         0.2897,  0.3215,  0.9797,  0.8933,  1.0803,  1.0742,  0.4247,  0.8233,\n",
      "         0.0822,  0.4217,  0.0409,  0.2201,  0.4835,  0.7527,  0.1221,  0.1021,\n",
      "         0.9023,  0.6950,  0.8230,  0.0771,  0.0120,  0.6815,  0.2381,  0.4917,\n",
      "         0.5711,  0.6372,  0.1916,  0.7553,  0.4617,  0.3612,  1.0660,  0.1057,\n",
      "         0.9570,  0.0930,  0.7436,  0.2409,  0.9282,  0.4478,  0.6589,  0.5610,\n",
      "         0.5350,  0.3639,  1.0138,  0.0514,  0.8581,  0.5902,  0.1963,  0.1039,\n",
      "         0.7524,  0.9357,  0.8042,  1.0085,  0.2344,  0.0077,  0.0788,  0.2644])\n",
      "tensor([ 7.3677e-09, -2.2476e-08,  1.7152e-08,  4.2578e-09, -9.3677e-10,\n",
      "        -1.0118e-08,  3.9888e-09,  1.5765e-08, -1.3113e-08, -6.7535e-09,\n",
      "        -2.7671e-08, -4.7451e-09,  9.9612e-09, -5.6004e-09,  1.0394e-09,\n",
      "        -3.3642e-08, -1.9318e-08,  1.3062e-09,  6.1179e-10,  1.0392e-08,\n",
      "        -1.1741e-08, -5.0689e-11,  1.1978e-09,  3.2284e-08, -1.3253e-08,\n",
      "         3.9531e-08, -2.3085e-09,  3.2033e-08, -1.5258e-09, -3.9732e-08,\n",
      "         1.6399e-09,  9.8534e-09, -4.3635e-09,  2.9055e-08,  1.1860e-08,\n",
      "         5.0860e-09,  2.7464e-08,  7.9003e-09,  6.1376e-09,  1.9851e-08,\n",
      "        -2.3709e-08, -5.1731e-09, -4.1663e-09, -2.2178e-08,  1.2325e-08,\n",
      "        -1.3001e-08, -1.3296e-08,  1.6949e-08,  1.9939e-08, -6.2492e-09,\n",
      "         4.5817e-09,  2.1118e-09,  1.9523e-08, -1.7807e-08, -1.9612e-09,\n",
      "         6.7704e-09,  3.5268e-08,  3.4908e-08, -3.0771e-08, -1.1535e-08,\n",
      "         9.4420e-09, -3.3191e-09, -5.6055e-09,  1.7733e-08, -3.0216e-08,\n",
      "        -1.7834e-08,  1.4797e-08,  2.6025e-08,  5.0203e-09,  1.0325e-08,\n",
      "         1.9224e-08, -7.0254e-09,  9.8787e-09, -1.8044e-08,  1.6214e-08,\n",
      "         6.2701e-09, -6.7314e-09,  1.3024e-08, -2.7355e-08,  2.0102e-08,\n",
      "        -6.0567e-10, -3.5525e-08, -7.4576e-09,  2.9646e-08, -7.5982e-10,\n",
      "        -1.1633e-08,  1.1850e-08,  2.7593e-08, -1.7319e-08,  1.5100e-08,\n",
      "        -1.4409e-08,  8.5483e-09, -4.2634e-10,  2.6253e-08,  2.0392e-09,\n",
      "         8.2106e-09, -3.7893e-08, -1.7635e-09, -1.1529e-08, -3.1828e-09,\n",
      "         1.1764e-08, -6.7182e-09, -1.2200e-08,  1.8536e-08, -1.0676e-10,\n",
      "         3.7641e-09, -7.0753e-09,  6.8659e-09,  5.8407e-09,  2.0102e-09,\n",
      "         3.2588e-10, -1.1603e-08,  2.4049e-08,  1.9482e-09, -2.3104e-09,\n",
      "         2.6531e-08,  1.4454e-08, -2.0059e-08, -2.8582e-08, -7.8687e-09,\n",
      "         6.9055e-09, -6.8903e-09, -2.5888e-08, -2.2642e-08,  1.5862e-08,\n",
      "         7.4208e-09,  1.1866e-08, -7.8525e-09])\n",
      "tensor([[[ 2.1477e-02, -7.2598e-03, -3.8330e-02,  1.0516e-02],\n",
      "         [ 7.3999e-03, -2.6661e-02,  2.6349e-02,  1.1319e-02],\n",
      "         [-2.8740e-02, -3.5538e-03,  4.1835e-02,  1.4433e-02],\n",
      "         ...,\n",
      "         [-2.9939e-03,  3.0421e-02, -4.2425e-02, -4.3201e-03],\n",
      "         [-1.5329e-02,  1.6133e-02, -9.6477e-03, -2.5618e-02],\n",
      "         [-3.2468e-02,  1.2615e-02, -3.5957e-02,  6.6273e-03]],\n",
      "\n",
      "        [[-3.0738e-02,  2.8432e-02,  2.6320e-02, -1.9540e-02],\n",
      "         [-3.6625e-02,  1.5659e-03,  1.9759e-02, -1.3299e-02],\n",
      "         [ 5.4202e-03,  1.7177e-02, -1.4831e-02,  3.6438e-02],\n",
      "         ...,\n",
      "         [ 1.4714e-02,  3.0873e-02, -1.4963e-02, -4.1165e-02],\n",
      "         [ 3.0643e-02,  1.5603e-02,  7.2217e-03,  1.7222e-02],\n",
      "         [-1.7307e-02, -7.0022e-04,  2.7460e-02, -1.7924e-02]],\n",
      "\n",
      "        [[ 3.1187e-02, -8.6220e-03, -2.7170e-02, -7.5418e-03],\n",
      "         [-2.6981e-02,  2.3441e-02, -1.8208e-02,  3.9842e-02],\n",
      "         [ 2.9202e-02,  3.9894e-02,  1.0107e-02,  3.2406e-02],\n",
      "         ...,\n",
      "         [-2.2495e-02, -1.5927e-02, -1.7447e-02,  2.0811e-02],\n",
      "         [ 7.5863e-03,  2.0263e-02, -1.5956e-02, -3.1528e-02],\n",
      "         [ 2.7224e-02,  4.1734e-02,  5.2767e-03,  1.8579e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-8.9268e-03,  1.1433e-02,  1.3275e-02,  2.3859e-02],\n",
      "         [ 3.2108e-02, -3.3344e-02,  3.4955e-02,  2.1604e-02],\n",
      "         [-1.0099e-02,  3.4361e-05,  3.8562e-02,  1.8939e-02],\n",
      "         ...,\n",
      "         [ 4.2845e-02,  1.4775e-02, -3.7151e-02,  3.9216e-02],\n",
      "         [-9.7061e-03, -3.9192e-03, -1.4093e-02, -4.3451e-02],\n",
      "         [ 1.8041e-02,  1.2140e-03, -2.1024e-02, -4.1802e-02]],\n",
      "\n",
      "        [[ 2.4617e-02, -1.0012e-02, -1.2157e-02,  3.2291e-02],\n",
      "         [ 1.8251e-02, -4.6292e-02, -1.2662e-02,  3.3668e-02],\n",
      "         [-2.6287e-02,  4.1525e-02,  5.1949e-03,  4.6545e-02],\n",
      "         ...,\n",
      "         [-1.5569e-02, -1.5896e-02,  3.3865e-03, -1.8158e-02],\n",
      "         [ 1.1564e-02, -6.6662e-03,  2.6267e-02,  3.1095e-02],\n",
      "         [-1.6768e-02,  3.0561e-02,  5.8953e-03,  3.3094e-02]],\n",
      "\n",
      "        [[ 3.1216e-02, -8.4297e-03,  4.8117e-04,  3.0942e-02],\n",
      "         [-3.1545e-02,  3.2052e-02,  1.1769e-02, -2.7490e-02],\n",
      "         [-4.3905e-02, -2.6925e-02, -3.6366e-03,  1.1977e-02],\n",
      "         ...,\n",
      "         [-2.1439e-02,  3.1763e-02,  4.1630e-03,  3.2729e-02],\n",
      "         [-7.8290e-03,  3.4423e-02,  1.8132e-02,  3.0737e-02],\n",
      "         [-1.2849e-03,  4.1108e-02, -3.3902e-02, -8.2020e-03]]])\n",
      "tensor([ 0.0198,  0.1872,  0.0140,  0.9673,  0.5246,  0.9160,  0.7447,  0.6061,\n",
      "         0.1880, -0.0964,  0.1647, -0.0089,  0.1280,  0.9005,  0.6959,  0.5945,\n",
      "         0.8811,  0.0013,  0.1577,  0.6202,  0.9331,  0.9537,  0.6263,  0.2294,\n",
      "        -0.0069,  0.4901,  0.5467,  0.6169,  0.5931, -0.0137,  0.0632,  0.8429,\n",
      "         0.9931,  0.1622,  0.7109,  0.6646,  0.5253,  0.2864,  0.2550,  0.5500,\n",
      "         0.7402,  0.9749,  0.2653,  0.1596,  0.8102,  0.3910,  0.9566,  0.7371,\n",
      "         0.3460,  0.8638,  0.6666,  0.3099,  0.9709,  0.2859,  0.8192,  0.9712,\n",
      "         0.2100,  0.8063,  0.2795,  0.6780,  0.3783,  0.6148,  0.3997,  0.6320,\n",
      "         0.9796,  0.6512,  0.1287,  0.7698,  0.1345,  0.9443,  0.5824,  0.8037,\n",
      "         0.6156,  0.5835,  0.9828,  0.7632,  0.5094,  0.2028,  0.5803,  0.4755,\n",
      "         0.4002,  0.2887,  0.1712,  0.8527,  0.5484,  0.7753,  0.9201,  0.4843,\n",
      "         0.9034,  0.9286,  0.7881,  0.5003,  0.4158,  0.6391,  0.9042,  0.9461,\n",
      "         0.8483,  0.2452,  0.9042,  0.5162,  0.2028,  0.0813,  0.7179,  0.8829,\n",
      "         0.5308,  0.0256,  0.7785,  0.2655,  0.8478,  0.2044,  0.2260,  0.2621,\n",
      "        -0.0084,  0.9138,  0.9513,  0.9136,  0.9211,  0.7103,  0.1878,  0.4437,\n",
      "         0.0840,  0.7308,  0.6141,  0.3105,  0.9679,  0.9657,  0.0258,  0.3986,\n",
      "         0.2597,  0.2062,  0.1158,  0.0040,  0.4236,  0.5678,  0.2251,  0.9888,\n",
      "         0.4049,  0.6420,  0.5342,  0.3698,  0.0398,  0.8473,  0.4636,  0.5478,\n",
      "         0.2240,  0.3374,  0.7849,  0.4615,  0.1526,  0.5506,  0.8892,  0.3460,\n",
      "         0.0443,  0.6453,  0.2174,  0.4982,  0.3846,  0.7274,  0.6939,  0.0178,\n",
      "         0.9257,  0.9601,  0.6621,  0.2583,  0.9018,  0.0996,  0.4738,  0.6398,\n",
      "         0.0654,  0.6047,  0.9236,  0.8292,  0.3384,  0.0260,  0.9284,  0.3773,\n",
      "         0.7253,  0.9347,  0.9347,  0.7866,  0.5928,  0.7533,  0.8739,  0.2413,\n",
      "         0.8970,  0.2240,  0.2941,  0.0261,  0.6201,  0.1834,  0.3603,  0.5585,\n",
      "         0.0701,  0.8378,  0.0184,  0.3316,  0.9555,  0.9548,  0.0979,  0.3971,\n",
      "         0.9542,  0.6606,  0.2600,  0.5473, -0.0477,  0.2497,  0.4740,  0.9547,\n",
      "         0.1125,  0.9149,  0.3218,  0.3045,  0.2725,  0.6263,  0.5913,  0.1427,\n",
      "         0.1924,  0.7709,  0.2562,  0.8423,  0.7822,  0.8845,  0.4493,  0.0381,\n",
      "         0.3061,  0.4727,  0.5045,  0.1466,  0.1613,  0.9998,  0.0714,  0.5487,\n",
      "         0.7649,  0.8276,  0.0084,  0.5747,  0.6474,  0.3114,  0.3763,  0.0977,\n",
      "         0.1266,  0.5577,  0.8346,  0.5757,  0.5694,  0.6971,  0.2403,  0.1246,\n",
      "         0.1059,  0.0502,  0.5135, -0.0470, -0.0418,  0.0866,  0.6593,  0.0177])\n",
      "tensor([-3.9324e-09,  3.8917e-09,  5.4149e-09,  7.3805e-03, -4.3026e-03,\n",
      "        -3.0395e-02,  4.0487e-03,  1.8265e-02,  5.0267e-10,  5.7050e-09,\n",
      "        -7.4784e-10, -7.3202e-10,  3.7837e-09, -1.5051e-02,  1.0677e-02,\n",
      "         9.0868e-04, -1.1944e-02, -6.7095e-09,  3.9747e-09, -5.9111e-03,\n",
      "         1.6287e-03,  2.6386e-03,  4.1470e-03, -3.7346e-09,  1.7959e-09,\n",
      "        -2.1087e-03, -3.9597e-03,  2.3748e-02, -1.4570e-02,  3.3245e-09,\n",
      "         7.6373e-09, -5.6134e-03,  1.3050e-02, -5.1414e-09,  3.3872e-02,\n",
      "         1.0421e-02, -1.3021e-03, -6.4322e-09,  3.0432e-09,  2.5366e-04,\n",
      "        -1.9918e-02,  1.3636e-02, -1.0136e-08,  9.5297e-09,  1.7952e-03,\n",
      "         1.1147e-08,  3.8405e-02, -1.4032e-02, -2.9252e-09, -3.5127e-02,\n",
      "        -1.9501e-02, -1.8427e-09, -3.2558e-03, -1.5220e-09, -1.1924e-02,\n",
      "         6.2192e-03,  1.6000e-10,  8.0308e-03,  3.0872e-10, -4.9284e-03,\n",
      "        -6.5486e-09, -2.1253e-02, -8.7767e-09, -4.5835e-02,  1.1088e-02,\n",
      "         1.8146e-02,  1.0150e-09, -4.6475e-02,  2.6592e-09,  3.6606e-02,\n",
      "        -1.0215e-02,  3.9535e-02, -1.0142e-02, -2.3453e-04,  4.3369e-02,\n",
      "        -5.2496e-02,  7.6324e-03,  2.8500e-09, -7.4686e-03,  1.1391e-03,\n",
      "         1.1826e-03,  6.4275e-10,  2.4670e-09, -2.3057e-02,  1.1848e-02,\n",
      "         7.2833e-02, -3.2388e-02, -1.1624e-03,  3.5882e-02, -1.2240e-02,\n",
      "         6.6295e-03,  3.1693e-03,  3.5129e-04, -2.2141e-02, -1.3851e-03,\n",
      "         5.3958e-02, -6.9877e-04, -3.8950e-11, -1.2542e-02,  1.7771e-05,\n",
      "         9.8880e-09, -3.7502e-10,  2.0898e-02,  4.6051e-02,  1.5181e-02,\n",
      "         9.3101e-09,  3.6927e-02,  1.1907e-09,  2.1696e-03, -1.4342e-09,\n",
      "        -1.0215e-08,  8.9331e-10, -1.3034e-09,  6.1350e-02,  4.1102e-03,\n",
      "         4.1762e-02,  2.2893e-02,  8.5586e-03, -2.4624e-10,  6.2797e-10,\n",
      "        -3.1154e-09,  1.7041e-02, -1.4603e-02, -4.4219e-10, -5.6522e-02,\n",
      "        -4.9308e-02,  7.6909e-09, -1.8987e-09, -6.5371e-10, -1.0150e-08,\n",
      "        -2.5070e-09, -2.3132e-09, -4.0147e-03,  2.4050e-05,  6.5324e-09,\n",
      "        -5.5510e-02, -3.7288e-09, -1.3709e-02,  2.3231e-03, -5.9614e-09,\n",
      "         3.9437e-10, -6.6180e-04, -1.0997e-02,  4.0553e-03, -5.8356e-09,\n",
      "         2.7529e-10,  1.9303e-02, -2.4860e-03, -4.3234e-09, -2.0216e-03,\n",
      "         2.1411e-02,  2.3612e-09, -4.2069e-09, -2.1633e-02,  2.9805e-09,\n",
      "        -1.4984e-03, -5.8792e-09, -2.2535e-03,  3.9104e-03,  3.6510e-09,\n",
      "         1.3772e-02,  2.3921e-02,  2.3467e-02,  3.3330e-09, -2.1026e-02,\n",
      "         3.6763e-09, -4.3219e-04, -1.7134e-02,  2.3482e-10, -2.7737e-02,\n",
      "         9.5521e-03, -1.0829e-02,  5.7646e-10,  2.1488e-09,  1.0139e-03,\n",
      "        -2.1733e-09,  5.9569e-03,  2.2392e-03, -5.0784e-02,  2.6025e-02,\n",
      "         8.6990e-03, -1.0865e-03,  1.7956e-02, -1.3406e-08, -2.3219e-02,\n",
      "         2.5869e-09,  2.0546e-09, -8.3071e-10,  1.1656e-02,  2.1630e-09,\n",
      "         9.5235e-09,  1.3570e-02,  6.9514e-11,  2.3632e-02,  5.0659e-09,\n",
      "         3.7998e-09, -1.1710e-02,  8.5545e-03,  7.4973e-09, -6.1722e-09,\n",
      "         7.4295e-02,  1.6220e-02, -6.3669e-09, -4.4846e-03, -3.5012e-09,\n",
      "         7.0000e-09, -2.2262e-03, -3.5316e-02, -6.4507e-09, -2.0071e-02,\n",
      "         9.5734e-09, -6.8231e-09, -3.0490e-09,  2.3851e-02,  6.7452e-03,\n",
      "         4.1637e-09,  7.4904e-10, -3.8993e-03,  3.0024e-09,  9.2482e-03,\n",
      "         2.7173e-02, -1.3516e-02,  2.2919e-03, -4.3771e-09,  3.1925e-09,\n",
      "        -4.1574e-03, -2.9836e-04,  6.6991e-09, -3.9923e-09,  1.5224e-03,\n",
      "         1.5786e-09, -2.2606e-02, -1.0773e-02,  3.9869e-03, -2.5000e-09,\n",
      "         2.5679e-02, -1.7733e-02, -3.4456e-09,  6.2406e-10,  1.0751e-08,\n",
      "         7.6742e-09,  3.2066e-02, -3.5461e-03,  1.9225e-02, -3.3655e-03,\n",
      "         1.7894e-02,  9.3457e-09,  1.3499e-09, -8.6993e-09, -5.0240e-10,\n",
      "        -6.0033e-03,  3.2841e-09,  3.2437e-10, -4.1077e-10, -3.3977e-02,\n",
      "        -5.6551e-09])\n",
      "hardware\n",
      "tensor(7.6729e-09)\n",
      "tensor([[-0.0611,  0.0073, -0.0404,  ...,  0.0440, -0.0452, -0.0538],\n",
      "        [ 0.0390,  0.0637,  0.0422,  ...,  0.0436, -0.0105,  0.0527],\n",
      "        [ 0.0454, -0.0108,  0.0622,  ..., -0.0375,  0.0298, -0.0261],\n",
      "        ...,\n",
      "        [-0.0379, -0.0025,  0.0092,  ...,  0.0450,  0.0396, -0.0111],\n",
      "        [-0.0467,  0.0342,  0.0264,  ...,  0.0069,  0.0363,  0.0063],\n",
      "        [ 0.0248,  0.0564,  0.0575,  ...,  0.0331,  0.0505, -0.0234]])\n",
      "tensor([0.7496, 0.8934, 0.1871, 0.1057, 0.3161, 0.8479, 0.0624, 0.2406, 0.0056,\n",
      "        0.3629, 0.0046, 0.6333, 0.7086, 0.5497, 0.2116, 0.2757, 0.3157, 0.2654,\n",
      "        0.3761, 0.4564, 0.8221, 0.3484, 0.3425, 0.2576, 0.6260, 0.0064, 0.4747,\n",
      "        0.9150, 0.8757, 0.2886, 0.1473, 0.7848, 0.8206, 1.0016, 0.0316, 0.7795,\n",
      "        0.6581, 0.3581, 0.7360, 0.2832, 0.7443, 0.1829, 0.8870, 0.2779, 0.1996,\n",
      "        0.1493, 0.0883, 0.7847, 0.6402, 0.3174, 0.8474, 0.5311, 0.0142, 0.4570,\n",
      "        0.4395, 0.8889, 0.1042, 0.9010, 0.8509, 0.1338, 0.0554, 0.9405, 0.1003,\n",
      "        0.1446, 0.2466, 0.9328, 0.1139, 0.1505, 0.8214, 0.1783, 0.5036, 0.2966,\n",
      "        0.3109, 0.6613, 0.2282, 0.0760, 0.6762, 0.3348, 0.7103, 0.0306, 0.9278,\n",
      "        0.2958, 0.1366, 0.3024, 0.8626, 0.4674, 0.4964, 0.0036, 0.6570, 0.8381,\n",
      "        0.1130, 0.1410, 0.4958, 0.5304, 0.6742, 0.7433, 0.9827, 0.2290, 0.0955,\n",
      "        0.0208, 0.8099, 0.3126, 0.0757, 0.0795, 0.5072, 0.4606, 0.0323, 0.3481,\n",
      "        0.9037, 0.5607, 0.0035, 0.7272, 0.0093, 0.8001, 0.1021, 0.5171, 0.8603,\n",
      "        0.2270, 0.4093, 0.8530, 0.0429, 0.5050, 0.4214, 0.8977, 0.3385, 0.5664,\n",
      "        0.6649, 0.7393])\n",
      "tensor([ 3.0495e-03, -1.7773e-03, -1.0217e-03,  1.5273e-10,  3.7738e-04,\n",
      "        -1.0485e-03, -6.3257e-04, -2.3011e-03, -9.6591e-04,  5.4447e-04,\n",
      "         2.3715e-03,  1.4906e-03, -4.1597e-04,  4.1421e-04, -1.2137e-03,\n",
      "        -1.5273e-10, -1.5273e-10,  1.5273e-10,  4.4224e-03,  5.9378e-04,\n",
      "         9.1178e-04, -3.0416e-03, -1.9380e-03,  7.2770e-04, -2.4050e-04,\n",
      "         1.5273e-10,  1.5273e-10,  1.0485e-03, -6.0807e-07,  3.1693e-04,\n",
      "        -1.5273e-10,  1.5273e-10, -2.3698e-03,  9.4527e-04,  1.8842e-03,\n",
      "        -5.7162e-04,  1.9078e-03,  1.0026e-03,  6.3257e-04,  2.7609e-03,\n",
      "         1.5273e-10,  1.0485e-03,  5.4025e-04, -4.1597e-04, -1.5273e-10,\n",
      "        -4.1597e-04,  4.1695e-04, -4.4578e-04, -4.1597e-04, -4.1597e-04,\n",
      "         5.6299e-03,  5.5138e-04,  2.1461e-03, -6.3257e-04,  2.9429e-03,\n",
      "         1.5273e-10,  4.2521e-04,  2.1672e-04,  3.2746e-03, -6.3257e-04,\n",
      "         1.5273e-10,  4.1597e-04, -6.9545e-04, -7.8447e-04, -1.1360e-03,\n",
      "        -9.9138e-04, -4.3121e-04,  4.1597e-04, -1.5273e-10,  1.5273e-10,\n",
      "         2.3544e-03,  1.0693e-03,  6.3257e-04,  2.7782e-03, -1.9353e-03,\n",
      "         4.4681e-04, -4.1597e-04, -8.8115e-04, -1.5273e-10, -1.5273e-10,\n",
      "         6.3257e-04, -1.8801e-03, -1.0158e-03, -2.9183e-03, -1.7716e-03,\n",
      "         6.3257e-04, -5.2883e-04, -1.0485e-03, -4.1586e-03, -1.5273e-10,\n",
      "         2.3085e-03, -6.1121e-03,  4.1597e-04, -1.8697e-03,  1.2599e-03,\n",
      "         1.7093e-03,  9.7747e-04,  1.0485e-03, -3.0561e-03,  6.3257e-04,\n",
      "        -1.0485e-03,  1.4437e-03,  2.3393e-03, -2.3775e-03, -9.7221e-04,\n",
      "        -4.1597e-04, -6.3257e-04,  1.1208e-03,  1.6643e-03,  8.2889e-04,\n",
      "        -2.4021e-03,  6.3257e-04,  1.3080e-03,  1.0485e-03, -1.3125e-03,\n",
      "         8.0791e-04,  2.7962e-03, -6.3257e-04, -4.9263e-04,  1.6748e-03,\n",
      "         6.1262e-05,  1.6871e-03, -1.8869e-03,  8.0945e-04,  1.5273e-10,\n",
      "        -6.3257e-04, -1.0485e-03, -5.6421e-04])\n",
      "tensor([[-0.0138,  0.0212,  0.0139,  0.0973, -0.0107, -0.0319, -0.0626,  0.0697,\n",
      "         -0.0294,  0.0134, -0.0169, -0.0194,  0.1019,  0.0349,  0.0275, -0.0267,\n",
      "         -0.0454,  0.0397, -0.0070, -0.0224, -0.0332, -0.0072,  0.0591,  0.0400,\n",
      "          0.0451,  0.0761,  0.0198,  0.0207, -0.0503,  0.0593, -0.0594,  0.0644,\n",
      "         -0.0183,  0.0476,  0.0321,  0.0614, -0.0163,  0.0654,  0.0243,  0.0209,\n",
      "          0.0224,  0.0571, -0.0674,  0.0716, -0.0509,  0.0948, -0.0466,  0.0120,\n",
      "          0.0836,  0.0652, -0.0278, -0.0693,  0.0081, -0.0204,  0.0307,  0.0322,\n",
      "         -0.0256, -0.0561,  0.0287, -0.0353,  0.0438, -0.0978, -0.0293,  0.0197,\n",
      "         -0.0727, -0.0490,  0.0266, -0.0137, -0.0397,  0.0823,  0.0438,  0.0663,\n",
      "          0.0419, -0.0388, -0.0279, -0.0117,  0.0809, -0.0776, -0.0997, -0.0628,\n",
      "          0.0513,  0.0228,  0.0123, -0.0504,  0.0275,  0.0204, -0.0291, -0.0307,\n",
      "         -0.0768, -0.0512, -0.0767,  0.0061, -0.0468, -0.0607, -0.0201,  0.0132,\n",
      "          0.0157,  0.0739, -0.0197,  0.0607, -0.0729,  0.0397,  0.0377,  0.0145,\n",
      "          0.0258,  0.0211, -0.0583, -0.0160, -0.0340, -0.0506,  0.0092,  0.0581,\n",
      "          0.0982,  0.0906, -0.0227,  0.0143, -0.0202, -0.0398, -0.0609, -0.0134,\n",
      "          0.0530,  0.0577, -0.0520, -0.0766,  0.0503, -0.0134, -0.0319, -0.0226],\n",
      "        [ 0.0737, -0.0200, -0.0664,  0.0175,  0.0726, -0.0660,  0.0766, -0.0970,\n",
      "         -0.0113,  0.0874, -0.0750,  0.0204, -0.0393, -0.0186,  0.0105, -0.0735,\n",
      "         -0.0148,  0.0908, -0.0635, -0.0167, -0.0314, -0.0453, -0.0455, -0.0351,\n",
      "          0.0869,  0.0808,  0.0296,  0.0716, -0.0482,  0.0513, -0.0487,  0.0091,\n",
      "          0.0657, -0.0504, -0.0198,  0.0811,  0.0589,  0.0252, -0.0354, -0.0642,\n",
      "          0.0862,  0.0256, -0.0127, -0.0318, -0.0759, -0.0570, -0.0106, -0.0666,\n",
      "         -0.0298, -0.0209,  0.0203, -0.0523,  0.0201,  0.0994,  0.0280,  0.0253,\n",
      "         -0.0620, -0.0332,  0.0404,  0.0497,  0.0148,  0.0923, -0.0056,  0.0266,\n",
      "          0.0629, -0.0526, -0.0110,  0.0311, -0.0295,  0.0457, -0.0342,  0.0066,\n",
      "         -0.0432,  0.0095, -0.0301,  0.0527, -0.0601,  0.0269, -0.0143, -0.0424,\n",
      "         -0.0358,  0.0353,  0.0380, -0.0159,  0.0574, -0.0279,  0.0225, -0.0179,\n",
      "          0.0076, -0.0551,  0.0402,  0.0412,  0.0758,  0.0265, -0.0349, -0.0225,\n",
      "         -0.0508,  0.0370, -0.0380, -0.0100, -0.0753, -0.0969, -0.0184,  0.0935,\n",
      "         -0.0818, -0.0692,  0.0423,  0.0160, -0.0311, -0.0214, -0.0570, -0.0253,\n",
      "         -0.0163,  0.0169,  0.0206, -0.0407,  0.0180,  0.0562,  0.0231,  0.0408,\n",
      "         -0.0290,  0.0504,  0.0073, -0.0223,  0.0408,  0.0230, -0.0800, -0.0260],\n",
      "        [-0.0309,  0.0272,  0.0811,  0.0670, -0.0618,  0.0430,  0.0260, -0.0139,\n",
      "          0.0628,  0.0123, -0.0209, -0.0653,  0.0393,  0.0199,  0.0464, -0.0179,\n",
      "         -0.0292,  0.0435, -0.0072, -0.0118, -0.0201,  0.0026, -0.0267,  0.0236,\n",
      "          0.0144,  0.0477,  0.0613, -0.0341,  0.0110, -0.0105, -0.0611,  0.0663,\n",
      "         -0.0096,  0.0484,  0.0682, -0.0125, -0.0640,  0.0105, -0.0393,  0.0166,\n",
      "          0.0521, -0.0514, -0.0382,  0.0521, -0.0149,  0.0165,  0.0531,  0.0644,\n",
      "          0.0363,  0.0749,  0.0841,  0.0136, -0.0579,  0.0151,  0.0133,  0.0663,\n",
      "         -0.0586,  0.0095, -0.0188,  0.0089,  0.0622, -0.0233,  0.0501,  0.0323,\n",
      "         -0.0165,  0.0085,  0.0690, -0.0810, -0.0399,  0.0533, -0.0625,  0.0253,\n",
      "         -0.0240,  0.0343, -0.0155,  0.0065,  0.0541, -0.0229, -0.0225, -0.0750,\n",
      "         -0.0386,  0.0327,  0.0076,  0.0236,  0.0735, -0.0348, -0.0112,  0.0514,\n",
      "          0.0255, -0.0685,  0.0112, -0.0251, -0.0486, -0.0205,  0.0837, -0.0569,\n",
      "          0.0172, -0.0578, -0.0180, -0.0365,  0.0666,  0.0144,  0.0482, -0.0133,\n",
      "          0.0104,  0.0564,  0.0124, -0.0725, -0.0147,  0.0207, -0.0183, -0.0408,\n",
      "          0.0095, -0.0602, -0.0415,  0.0082, -0.0650,  0.0211,  0.0168, -0.0440,\n",
      "         -0.0135,  0.0291,  0.0542,  0.0610,  0.0602,  0.0538,  0.0104, -0.0105]])\n"
     ]
    }
   ],
   "source": [
    "# print the model weights for reference if needed\n",
    "print(\"software\")\n",
    "for param in cnv_software_model.parameters():\n",
    "    print(param.data)\n",
    "\n",
    "print(\"hardware\")\n",
    "for param in cnv_hardware_model.parameters():\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "regular [0.11638211 0.73559391 0.14802397] prediction 1 [0.11638211 0.73559391 0.14802397] prediction 1 target 1.0\n",
      "regular [0.10703662 0.79417246 0.09879092] prediction 1 [0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.32753091 0.09840626 0.57406283] prediction 2 [0.32753091 0.09840626 0.57406283] prediction 2 target 2.0\n",
      "regular [0.20007521 0.66592111 0.13400368] prediction 1 [0.20007521 0.66592111 0.13400368] prediction 1 target 1.0\n",
      "regular [0.10703662 0.79417246 0.09879092] prediction 1 [0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.44975909 0.13512957 0.41511134] prediction 0 [0.44975909 0.13512957 0.41511134] prediction 0 target 2.0\n",
      "regular [0.45449006 0.18817248 0.35733745] prediction 0 [0.45449006 0.18817248 0.35733745] prediction 0 target 2.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 0.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "regular [0.25632393 0.61909519 0.12458088] prediction 1 [0.25632393 0.61909519 0.12458088] prediction 1 target 1.0\n",
      "regular [0.25632393 0.61909519 0.12458088] prediction 1 [0.25632393 0.61909519 0.12458088] prediction 1 target 1.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 0.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 0.0\n",
      "regular [0.10703662 0.79417246 0.09879092] prediction 1 [0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "regular [0.38101396 0.13438225 0.48460379] prediction 2 [0.38101396 0.13438225 0.48460379] prediction 2 target 0.0\n",
      "regular [0.10703662 0.79417246 0.09879092] prediction 1 [0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "regular [0.17564234 0.68626097 0.13809669] prediction 1 [0.17564234 0.68626097 0.13809669] prediction 1 target 1.0\n",
      "regular [0.20007521 0.66592111 0.13400368] prediction 1 [0.20007521 0.66592111 0.13400368] prediction 1 target 1.0\n",
      "regular [0.11001825 0.8162952  0.07368654] prediction 1 [0.11001825 0.8162952  0.07368654] prediction 1 target 1.0\n",
      "regular [0.12335408 0.77966024 0.09698568] prediction 1 [0.12335408 0.77966024 0.09698568] prediction 1 target 1.0\n",
      "regular [0.45449009 0.35733741 0.18817249] prediction 0 [0.45449009 0.35733741 0.18817249] prediction 0 target 0.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "regular [0.56601866 0.2343486  0.19963274] prediction 0 [0.56601866 0.2343486  0.19963274] prediction 0 target 0.0\n",
      "regular [0.17152321 0.67016705 0.15830973] prediction 1 [0.17152321 0.67016705 0.15830973] prediction 1 target 1.0\n",
      "regular [0.58326769 0.24149024 0.17524207] prediction 0 [0.58326769 0.24149024 0.17524207] prediction 0 target 2.0\n",
      "regular [0.17931056 0.12009622 0.70059321] prediction 2 [0.17931056 0.12009622 0.70059321] prediction 2 target 2.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.20007521 0.66592111 0.13400368] prediction 1 [0.20007521 0.66592111 0.13400368] prediction 1 target 1.0\n",
      "regular [0.39559774 0.10124969 0.50315257] prediction 2 [0.39559774 0.10124969 0.50315257] prediction 2 target 0.0\n",
      "regular [0.4194781  0.12603168 0.45449023] prediction 2 [0.4194781  0.12603168 0.45449023] prediction 2 target 0.0\n",
      "regular [0.53447421 0.16058212 0.30494367] prediction 0 [0.53447421 0.16058212 0.30494367] prediction 0 target 0.0\n",
      "regular [0.14378663 0.77417606 0.08203731] prediction 1 [0.14378663 0.77417606 0.08203731] prediction 1 target 1.0\n",
      "regular [0.3688562  0.08042042 0.55072338] prediction 2 [0.3688562  0.08042042 0.55072338] prediction 2 target 0.0\n",
      "regular [0.51928963 0.13290752 0.34780284] prediction 0 [0.51928963 0.13290752 0.34780284] prediction 0 target 0.0\n",
      "regular [0.41511123 0.38313264 0.20175613] prediction 0 [0.41511123 0.38313264 0.20175613] prediction 0 target 0.0\n",
      "regular [0.16241289 0.74492259 0.09266452] prediction 1 [0.16241289 0.74492259 0.09266452] prediction 1 target 1.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "regular [0.09634224 0.83913096 0.06452681] prediction 1 [0.09634224 0.83913096 0.06452681] prediction 1 target 1.0\n",
      "regular [0.17931056 0.12009622 0.70059321] prediction 2 [0.17931056 0.12009622 0.70059321] prediction 2 target 2.0\n",
      "regular [0.08539299 0.87310354 0.04150347] prediction 1 [0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "regular [0.17564234 0.68626097 0.13809669] prediction 1 [0.17564234 0.68626097 0.13809669] prediction 1 target 1.0\n",
      "regular [0.32753091 0.09840626 0.57406283] prediction 2 [0.32753091 0.09840626 0.57406283] prediction 2 target 2.0\n",
      "regular [0.21362728 0.07534558 0.71102714] prediction 2 [0.21362728 0.07534558 0.71102714] prediction 2 target 0.0\n",
      "regular [0.52971898 0.1154928  0.35478821] prediction 0 [0.52971898 0.1154928  0.35478821] prediction 0 target 0.0\n",
      "regular [0.08539299 0.87310354 0.04150347] prediction 1 [0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "regular [0.10703662 0.79417246 0.09879092] prediction 1 [0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.41048165 0.14477543 0.44474293] prediction 2 [0.41048165 0.14477543 0.44474293] prediction 2 target 0.0\n",
      "regular [0.26912586 0.65001558 0.08085856] prediction 1 [0.26912586 0.65001558 0.08085856] prediction 1 target 1.0\n",
      "regular [0.56938609 0.10575127 0.32486265] prediction 0 [0.56938609 0.10575127 0.32486265] prediction 0 target 0.0\n",
      "regular [0.195519   0.15372448 0.65075651] prediction 2 [0.195519   0.15372448 0.65075651] prediction 2 target 0.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.10703662 0.79417246 0.09879092] prediction 1 [0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "regular [0.32753091 0.09840626 0.57406283] prediction 2 [0.32753091 0.09840626 0.57406283] prediction 2 target 2.0\n",
      "regular [0.48967302 0.12532739 0.38499958] prediction 0 [0.48967302 0.12532739 0.38499958] prediction 0 target 0.0\n",
      "regular [0.20771093 0.69133548 0.10095358] prediction 1 [0.20771093 0.69133548 0.10095358] prediction 1 target 2.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "regular [0.42743571 0.28628214 0.28628214] prediction 0 [0.42743571 0.28628214 0.28628214] prediction 0 target 1.0\n",
      "regular [0.27238859 0.06971542 0.65789599] prediction 2 [0.27238859 0.06971542 0.65789599] prediction 2 target 0.0\n",
      "regular [0.46752245 0.16489351 0.36758404] prediction 0 [0.46752245 0.16489351 0.36758404] prediction 0 target 0.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.15983722 0.73310914 0.10705363] prediction 1 [0.15983722 0.73310914 0.10705363] prediction 1 target 1.0\n",
      "regular [0.17931056 0.12009622 0.70059321] prediction 2 [0.17931056 0.12009622 0.70059321] prediction 2 target 2.0\n",
      "regular [0.2163782 0.613497  0.1701248] prediction 1 [0.2163782 0.613497  0.1701248] prediction 1 target 1.0\n",
      "regular [0.12335408 0.77966024 0.09698568] prediction 1 [0.12335408 0.77966024 0.09698568] prediction 1 target 1.0\n",
      "regular [0.25632393 0.61909519 0.12458088] prediction 1 [0.25632393 0.61909519 0.12458088] prediction 1 target 1.0\n",
      "regular [0.2108644  0.70183143 0.08730417] prediction 1 [0.2108644  0.70183143 0.08730417] prediction 1 target 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 0.0\n",
      "regular [0.46707355 0.10183443 0.43109203] prediction 0 [0.46707355 0.10183443 0.43109203] prediction 0 target 0.0\n",
      "regular [0.48596973 0.06549779 0.44853247] prediction 0 [0.48596973 0.06549779 0.44853247] prediction 0 target 2.0\n",
      "regular [0.17564234 0.68626097 0.13809669] prediction 1 [0.17564234 0.68626097 0.13809669] prediction 1 target 1.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 0.0\n",
      "regular [0.45449006 0.18817248 0.35733745] prediction 0 [0.45449006 0.18817248 0.35733745] prediction 0 target 2.0\n",
      "regular [0.17564234 0.68626097 0.13809669] prediction 1 [0.17564234 0.68626097 0.13809669] prediction 1 target 1.0\n",
      "regular [0.20007521 0.66592111 0.13400368] prediction 1 [0.20007521 0.66592111 0.13400368] prediction 1 target 1.0\n",
      "regular [0.46707355 0.10183443 0.43109203] prediction 0 [0.46707355 0.10183443 0.43109203] prediction 0 target 0.0\n",
      "regular [0.09634224 0.83913096 0.06452681] prediction 1 [0.09634224 0.83913096 0.06452681] prediction 1 target 1.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.62464946 0.18767527 0.18767527] prediction 0 [0.62464946 0.18767527 0.18767527] prediction 0 target 0.0\n",
      "regular [0.40689973 0.07557289 0.51752738] prediction 2 [0.40689973 0.07557289 0.51752738] prediction 2 target 2.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "regular [0.08539299 0.87310354 0.04150347] prediction 1 [0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "regular [0.08539299 0.87310354 0.04150347] prediction 1 [0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "regular [0.08539299 0.87310354 0.04150347] prediction 1 [0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 0.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "regular [0.53893965 0.10009648 0.36096387] prediction 0 [0.53893965 0.10009648 0.36096387] prediction 0 target 0.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 0.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "regular [0.08539299 0.87310354 0.04150347] prediction 1 [0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "regular [0.11899135 0.75208556 0.12892309] prediction 1 [0.11899135 0.75208556 0.12892309] prediction 1 target 1.0\n",
      "regular [0.5863751  0.20681245 0.20681245] prediction 0 [0.5863751  0.20681245 0.20681245] prediction 0 target 0.0\n",
      "regular [0.36885619 0.08042041 0.5507234 ] prediction 2 [0.36885619 0.08042041 0.5507234 ] prediction 2 target 0.0\n",
      "regular [0.08539299 0.87310354 0.04150347] prediction 1 [0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "regular [0.40689973 0.07557289 0.51752738] prediction 2 [0.40689973 0.07557289 0.51752738] prediction 2 target 2.0\n",
      "regular [0.17564234 0.68626097 0.13809669] prediction 1 [0.17564234 0.68626097 0.13809669] prediction 1 target 1.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.29218708 0.19569715 0.51211577] prediction 2 [0.29218708 0.19569715 0.51211577] prediction 2 target 2.0\n",
      "regular [0.2163782 0.613497  0.1701248] prediction 1 [0.2163782 0.613497  0.1701248] prediction 1 target 1.0\n",
      "regular [0.440685   0.08184778 0.47746722] prediction 2 [0.440685   0.08184778 0.47746722] prediction 2 target 2.0\n",
      "regular [0.10703662 0.79417246 0.09879092] prediction 1 [0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "regular [0.08539299 0.87310354 0.04150347] prediction 1 [0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "regular [0.44636572 0.25467345 0.29896083] prediction 0 [0.44636572 0.25467345 0.29896083] prediction 0 target 0.0\n",
      "regular [0.08539299 0.87310354 0.04150347] prediction 1 [0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "regular [0.53893963 0.10009649 0.36096388] prediction 0 [0.53893963 0.10009649 0.36096388] prediction 0 target 0.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "regular [0.42430648 0.24208759 0.33360593] prediction 0 [0.42430648 0.24208759 0.33360593] prediction 0 target 0.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.17564231 0.68626099 0.1380967 ] prediction 1 [0.17564231 0.68626099 0.1380967 ] prediction 1 target 1.0\n",
      "regular [0.60490752 0.21334872 0.18174375] prediction 0 [0.60490752 0.21334872 0.18174375] prediction 0 target 1.0\n",
      "regular [0.08539299 0.87310354 0.04150347] prediction 1 [0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "regular [0.09634224 0.83913096 0.06452681] prediction 1 [0.09634224 0.83913096 0.06452681] prediction 1 target 1.0\n",
      "regular [0.10703662 0.79417246 0.09879092] prediction 1 [0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "regular [0.11899135 0.75208555 0.1289231 ] prediction 1 [0.11899135 0.75208555 0.1289231 ] prediction 1 target 1.0\n",
      "regular [0.16664923 0.76435301 0.06899776] prediction 1 [0.16664923 0.76435301 0.06899776] prediction 1 target 1.0\n",
      "regular [0.11899135 0.75208556 0.12892309] prediction 1 [0.11899135 0.75208556 0.12892309] prediction 1 target 1.0\n",
      "regular [0.42430648 0.33360592 0.2420876 ] prediction 0 [0.42430648 0.33360592 0.2420876 ] prediction 0 target 0.0\n",
      "regular [0.36885619 0.08042041 0.5507234 ] prediction 2 [0.36885619 0.08042041 0.5507234 ] prediction 2 target 0.0\n",
      "regular [0.17931056 0.12009622 0.70059321] prediction 2 [0.17931056 0.12009622 0.70059321] prediction 2 target 2.0\n",
      "regular [0.5740626  0.14692619 0.27901121] prediction 0 [0.5740626  0.14692619 0.27901121] prediction 0 target 0.0\n",
      "regular [0.17931056 0.12009622 0.70059321] prediction 2 [0.17931056 0.12009622 0.70059321] prediction 2 target 2.0\n",
      "regular [0.45449006 0.18817248 0.35733745] prediction 0 [0.45449006 0.18817248 0.35733745] prediction 0 target 2.0\n",
      "regular [0.50388526 0.28749119 0.20862355] prediction 0 [0.50388526 0.28749119 0.20862355] prediction 0 target 0.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "regular [0.10703662 0.79417246 0.09879092] prediction 1 [0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "regular [0.53447421 0.16058212 0.30494367] prediction 0 [0.53447421 0.16058212 0.30494367] prediction 0 target 0.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 0.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.42430648 0.24208759 0.33360593] prediction 0 [0.42430648 0.24208759 0.33360593] prediction 0 target 0.0\n",
      "regular [0.13685198 0.73683859 0.12630943] prediction 1 [0.13685198 0.73683859 0.12630943] prediction 1 target 1.0\n",
      "regular [0.45449006 0.18817248 0.35733745] prediction 0 [0.45449006 0.18817248 0.35733745] prediction 0 target 2.0\n",
      "regular [0.41511123 0.38313265 0.20175612] prediction 0 [0.41511123 0.38313265 0.20175612] prediction 0 target 0.0\n",
      "regular [0.23533126 0.09743417 0.66723457] prediction 2 [0.23533126 0.09743417 0.66723457] prediction 2 target 2.0\n",
      "regular [0.40040104 0.165778   0.43382096] prediction 2 [0.40040104 0.165778   0.43382096] prediction 2 target 0.0\n",
      "regular [0.48596971 0.06549779 0.4485325 ] prediction 0 [0.48596971 0.06549779 0.4485325 ] prediction 0 target 0.0\n",
      "regular [0.12694771 0.68351209 0.1895402 ] prediction 1 [0.12694771 0.68351209 0.1895402 ] prediction 1 target 1.0\n",
      "regular [0.20007519 0.66592113 0.13400368] prediction 1 [0.20007519 0.66592113 0.13400368] prediction 1 target 1.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.17931056 0.12009622 0.70059321] prediction 2 [0.17931056 0.12009622 0.70059321] prediction 2 target 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regular [0.54749824 0.14012724 0.31237452] prediction 0 [0.54749824 0.14012724 0.31237452] prediction 0 target 0.0\n",
      "regular [0.20007519 0.66592113 0.13400368] prediction 1 [0.20007519 0.66592113 0.13400368] prediction 1 target 1.0\n",
      "regular [0.2163782 0.613497  0.1701248] prediction 1 [0.2163782 0.613497  0.1701248] prediction 1 target 1.0\n",
      "regular [0.45449006 0.18817248 0.35733745] prediction 0 [0.45449006 0.18817248 0.35733745] prediction 0 target 2.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "regular [0.10703662 0.79417246 0.09879092] prediction 1 [0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "regular [0.23050775 0.21275033 0.55674192] prediction 2 [0.23050775 0.21275033 0.55674192] prediction 2 target 0.0\n",
      "regular [0.09527316 0.82981946 0.07490738] prediction 1 [0.09527316 0.82981946 0.07490738] prediction 1 target 1.0\n",
      "regular [0.45449006 0.18817248 0.35733745] prediction 0 [0.45449006 0.18817248 0.35733745] prediction 0 target 2.0\n",
      "regular [0.38101396 0.13438227 0.48460377] prediction 2 [0.38101396 0.13438227 0.48460377] prediction 2 target 0.0\n",
      "regular [0.43943292 0.40558067 0.15498641] prediction 0 [0.43943292 0.40558067 0.15498641] prediction 0 target 0.0\n",
      "regular [0.16692773 0.65221174 0.18086053] prediction 1 [0.16692773 0.65221174 0.18086053] prediction 1 target 1.0\n",
      "regular [0.56574823 0.0553324  0.37891936] prediction 0 [0.56574823 0.0553324  0.37891936] prediction 0 target 0.0\n",
      "regular [0.2163782 0.613497  0.1701248] prediction 1 [0.2163782 0.613497  0.1701248] prediction 1 target 1.0\n",
      "regular [0.2163782 0.613497  0.1701248] prediction 1 [0.2163782 0.613497  0.1701248] prediction 1 target 1.0\n",
      "regular [0.25122763 0.51689834 0.23187402] prediction 1 [0.25122763 0.51689834 0.23187402] prediction 1 target 2.0\n",
      "regular [0.25632393 0.61909519 0.12458088] prediction 1 [0.25632393 0.61909519 0.12458088] prediction 1 target 1.0\n",
      "regular [0.25632393 0.61909519 0.12458088] prediction 1 [0.25632393 0.61909519 0.12458088] prediction 1 target 1.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "regular [0.28220534 0.13716    0.58063466] prediction 2 [0.28220534 0.13716    0.58063466] prediction 2 target 0.0\n",
      "regular [0.38101396 0.13438225 0.4846038 ] prediction 2 [0.38101396 0.13438225 0.4846038 ] prediction 2 target 0.0\n",
      "regular [0.2163782 0.613497  0.1701248] prediction 1 [0.2163782 0.613497  0.1701248] prediction 1 target 1.0\n",
      "regular [0.17564234 0.68626097 0.13809669] prediction 1 [0.17564234 0.68626097 0.13809669] prediction 1 target 1.0\n",
      "regular [0.46386586 0.31068183 0.22545231] prediction 0 [0.46386586 0.31068183 0.22545231] prediction 0 target 0.0\n",
      "regular [0.40040106 0.16577799 0.43382095] prediction 2 [0.40040106 0.16577799 0.43382095] prediction 2 target 0.0\n",
      "number of inputs of each class [52, 67, 53]\n",
      "correctly predicted (split) [28, 65, 42]\n",
      "correctly predicted class 1 at 53.846\n",
      "correctly predicted class 2 at 97.015\n",
      "correctly predicted class 3 at 79.245\n",
      "accuracy at 78.488\n",
      "correctly predicted (original) [28, 65, 42]\n",
      "correctly predicted class 1 at 53.846\n",
      "correctly predicted class 2 at 97.015\n",
      "correctly predicted class 3 at 79.245\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "regular [0.2163782 0.613497  0.1701248] prediction 1 [0.2163782 0.613497  0.1701248] prediction 1 target 1.0\n",
      "regular [0.25088854 0.14314418 0.60596728] prediction 2 [0.25088854 0.14314418 0.60596728] prediction 2 target 0.0\n",
      "regular [0.47989332 0.19869017 0.32141651] prediction 0 [0.47989332 0.19869017 0.32141651] prediction 0 target 0.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 0.0\n",
      "regular [0.11123246 0.825304   0.06346354] prediction 1 [0.11123246 0.825304   0.06346354] prediction 1 target 1.0\n",
      "regular [0.2163782 0.613497  0.1701248] prediction 1 [0.2163782 0.613497  0.1701248] prediction 1 target 1.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.10703662 0.79417246 0.09879092] prediction 1 [0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "regular [0.25632393 0.61909519 0.12458088] prediction 1 [0.25632393 0.61909519 0.12458088] prediction 1 target 1.0\n",
      "regular [0.12335408 0.77966024 0.09698568] prediction 1 [0.12335408 0.77966024 0.09698568] prediction 1 target 1.0\n",
      "regular [0.26912586 0.65001558 0.08085856] prediction 1 [0.26912586 0.65001558 0.08085856] prediction 1 target 1.0\n",
      "regular [0.16692772 0.18086052 0.65221177] prediction 2 [0.16692772 0.18086052 0.65221177] prediction 2 target 0.0\n",
      "regular [0.5863751  0.20681245 0.20681245] prediction 0 [0.5863751  0.20681245 0.20681245] prediction 0 target 0.0\n",
      "regular [0.44975909 0.13512957 0.41511134] prediction 0 [0.44975909 0.13512957 0.41511134] prediction 0 target 2.0\n",
      "regular [0.08539299 0.87310354 0.04150347] prediction 1 [0.08539299 0.87310354 0.04150347] prediction 1 target 1.0\n",
      "regular [0.61272394 0.25368598 0.13359008] prediction 0 [0.61272394 0.25368598 0.13359008] prediction 0 target 0.0\n",
      "regular [0.17152322 0.67016705 0.15830973] prediction 1 [0.17152322 0.67016705 0.15830973] prediction 1 target 1.0\n",
      "regular [0.32202025 0.11357542 0.56440433] prediction 2 [0.32202025 0.11357542 0.56440433] prediction 2 target 2.0\n",
      "regular [0.45449006 0.18817248 0.35733745] prediction 0 [0.45449006 0.18817248 0.35733745] prediction 0 target 2.0\n",
      "regular [0.20007519 0.66592113 0.13400368] prediction 1 [0.20007519 0.66592113 0.13400368] prediction 1 target 1.0\n",
      "regular [0.47922855 0.14398365 0.3767878 ] prediction 0 [0.47922855 0.14398365 0.3767878 ] prediction 0 target 0.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.33661819 0.07339169 0.58999011] prediction 2 [0.33661819 0.07339169 0.58999011] prediction 2 target 0.0\n",
      "regular [0.21813607 0.05582997 0.72603396] prediction 2 [0.21813607 0.05582997 0.72603396] prediction 2 target 2.0\n",
      "regular [0.20007521 0.66592111 0.13400368] prediction 1 [0.20007521 0.66592111 0.13400368] prediction 1 target 1.0\n",
      "regular [0.10703662 0.79417246 0.09879092] prediction 1 [0.10703662 0.79417246 0.09879092] prediction 1 target 1.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.17564234 0.68626097 0.13809669] prediction 1 [0.17564234 0.68626097 0.13809669] prediction 1 target 1.0\n",
      "regular [0.17564234 0.68626097 0.13809669] prediction 1 [0.17564234 0.68626097 0.13809669] prediction 1 target 1.0\n",
      "regular [0.37330343 0.06933311 0.55736346] prediction 2 [0.37330343 0.06933311 0.55736346] prediction 2 target 0.0\n",
      "regular [0.27563099 0.15726096 0.56710805] prediction 2 [0.27563099 0.15726096 0.56710805] prediction 2 target 0.0\n",
      "regular [0.58326769 0.17524207 0.24149024] prediction 0 [0.58326769 0.17524207 0.24149024] prediction 0 target 0.0\n",
      "regular [0.15983724 0.10705365 0.73310911] prediction 2 [0.15983724 0.10705365 0.73310911] prediction 2 target 2.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 2.0\n",
      "regular [0.24179425 0.0726468  0.68555895] prediction 2 [0.24179425 0.0726468  0.68555895] prediction 2 target 0.0\n",
      "regular [0.40716532 0.27270576 0.32012893] prediction 0 [0.40716532 0.27270576 0.32012893] prediction 0 target 0.0\n",
      "regular [0.11899135 0.75208556 0.12892309] prediction 1 [0.11899135 0.75208556 0.12892309] prediction 1 target 1.0\n",
      "regular [0.30876326 0.15006793 0.54116882] prediction 2 [0.30876326 0.15006793 0.54116882] prediction 2 target 2.0\n",
      "regular [0.2163782 0.613497  0.1701248] prediction 1 [0.2163782 0.613497  0.1701248] prediction 1 target 1.0\n",
      "number of inputs of each class [13, 16, 13]\n",
      "correctly predicted (split) [6, 16, 11]\n",
      "correctly predicted class 1 at 46.154\n",
      "correctly predicted class 2 at 100.000\n",
      "correctly predicted class 3 at 84.615\n",
      "accuracy at 78.571\n",
      "correctly predicted (original) [6, 16, 11]\n",
      "correctly predicted class 1 at 46.154\n",
      "correctly predicted class 2 at 100.000\n",
      "correctly predicted class 3 at 84.615\n"
     ]
    }
   ],
   "source": [
    "# test through both models\n",
    "def test_on_everything_split(data_loader, print_output = False):\n",
    "\n",
    "    list_input_train_total = [0] * NUM_CLASSES\n",
    "    list_input_train_correct_total = [0] * NUM_CLASSES\n",
    "    list_input_train_original_correct_total = [0] * NUM_CLASSES\n",
    "    \n",
    "    for i, data in enumerate(data_loader):\n",
    "        (input, target) = data\n",
    "        input = input.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        software_output = cnv_software_model(input)\n",
    "        output = cnv_hardware_model(software_output)\n",
    "        #print(target, output)\n",
    "        \n",
    "        regular_output = cnv_pretrained_model(input)\n",
    "\n",
    "        for i in range(len(output)):\n",
    "            if print_output:\n",
    "                print(\"regular\", softmax(regular_output[i].tolist()), \"prediction\", get_prediction(regular_output[i]),\n",
    "                      softmax(output[i].tolist()), \"prediction\", get_prediction(output[i]), \"target\", target[i].tolist())\n",
    "\n",
    "            list_input_train_total[int(target[i].tolist())] += 1\n",
    "\n",
    "            if int(target[i].tolist()) == get_prediction(output[i]):\n",
    "                list_input_train_correct_total[int(target[i].tolist())] += 1\n",
    "\n",
    "            if int(target[i].tolist()) == get_prediction(regular_output[i]):\n",
    "                list_input_train_original_correct_total[int(target[i].tolist())] += 1\n",
    "\n",
    "    print(\"number of inputs of each class\", list_input_train_total)\n",
    "    print(\"correctly predicted (split)\", list_input_train_correct_total)\n",
    "\n",
    "    running_total = 0\n",
    "    running_total_correct = 0\n",
    "    for i in range(len(list_input_train_total)):\n",
    "        print(\"correctly predicted class %d at %.3f\" % ((i + 1), (list_input_train_correct_total[i] * 100 / list_input_train_total[i])))\n",
    "        running_total += list_input_train_total[i]\n",
    "        running_total_correct += list_input_train_correct_total[i]\n",
    "    \n",
    "    print(\"accuracy at %.3f\" % ((running_total_correct * 100 / running_total)))\n",
    "    \n",
    "    print(\"correctly predicted (original)\", list_input_train_original_correct_total)\n",
    "    \n",
    "    running_total = 0\n",
    "    running_total_correct = 0\n",
    "        \n",
    "    for i in range(len(list_input_train_total)):\n",
    "        print(\"correctly predicted class %d at %.3f\" % ((i + 1), (list_input_train_original_correct_total[i] * 100 / list_input_train_total[i])))\n",
    "        running_total += list_input_train_total[i]\n",
    "        running_total_correct += list_input_train_original_correct_total[i]\n",
    "    \n",
    "test_on_everything_split(train_loader, print_output = True)\n",
    "test_on_everything_split(test_loader, print_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNV_hardware(\n",
      "  (linear_features): ModuleList(\n",
      "    (0): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): RescalingIntQuant(\n",
      "            (int_quant): IntQuant(\n",
      "              (float_to_int_impl): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): RoundSte()\n",
      "                  (1): Identity()\n",
      "                )\n",
      "              )\n",
      "              (tensor_clamp_impl): TensorClamp()\n",
      "            )\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (int_scaling_impl): IntScaling(\n",
      "              (forward_impl): SignedFpIntScale()\n",
      "            )\n",
      "            (msb_clamp_bit_width_impl): BitWidthConst()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): QuantLinear(\n",
      "      in_features=256, out_features=128, bias=False\n",
      "      (weight_reg): WeightReg()\n",
      "      (weight_quant): WeightQuantProxy(\n",
      "        (tensor_quant): BinaryQuant(\n",
      "          (scaling_impl): ParameterStatsScaling(\n",
      "            (parameter_list_stats): ParameterListStats(\n",
      "              (first_tracked_param): _ViewParameterWrapper()\n",
      "              (stats): Stats(\n",
      "                (stats_impl): AbsAve()\n",
      "              )\n",
      "            )\n",
      "            (stats_scaling_impl): StatsScaling(\n",
      "              (affine_rescaling): Identity()\n",
      "              (restrict_scaling): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "              (restrict_scaling_preprocess): LogTwo()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bias_quant): BiasQuantProxy()\n",
      "    )\n",
      "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): ClampedBinaryQuant(\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): QuantLinear(\n",
      "    in_features=128, out_features=3, bias=False\n",
      "    (weight_reg): WeightReg()\n",
      "    (weight_quant): WeightQuantProxy(\n",
      "      (tensor_quant): BinaryQuant(\n",
      "        (scaling_impl): ParameterStatsScaling(\n",
      "          (parameter_list_stats): ParameterListStats(\n",
      "            (first_tracked_param): _ViewParameterWrapper()\n",
      "            (stats): Stats(\n",
      "              (stats_impl): AbsAve()\n",
      "            )\n",
      "          )\n",
      "          (stats_scaling_impl): StatsScaling(\n",
      "            (affine_rescaling): Identity()\n",
      "            (restrict_scaling): RestrictValue(\n",
      "              (forward_impl): Sequential(\n",
      "                (0): PowerOfTwo()\n",
      "                (1): ClampMin()\n",
      "              )\n",
      "            )\n",
      "            (restrict_scaling_preprocess): LogTwo()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bias_quant): BiasQuantProxy()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# export the trained hardware model to ONNX to synthesize\n",
    "INPUT_SPECIFICATIONS_HARDWARE = (1, 256) # batch size, length\n",
    "bo.export_finn_onnx(cnv_hardware_model, INPUT_SPECIFICATIONS_HARDWARE, build_dir + file_name + \".onnx\")\n",
    "print(cnv_hardware_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Preparing the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/workspace/finn/notebooks/CNN/cnn_1d_3_classes_24.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fdb97fcbc18>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the onnx model made by brevitas\n",
    "model = ModelWrapper(build_dir + file_name + \".onnx\")\n",
    "showInNetron(build_dir + file_name + \".onnx\")\n",
    "\n",
    "# use http://localhost:8081/ since this is on Ubuntu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/workspace/finn/notebooks/CNN/cnn_1d_3_classes_24_tidy.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fdb97fcb9b0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do some transformations and then show on netron\n",
    "\n",
    "model = model.transform(DoubleToSingleFloat())\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "#model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(build_dir + file_name + \"_tidy.onnx\")\n",
    "\n",
    "showInNetron(build_dir + file_name + \"_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is exported, let's have a look at its layer structure with Netron. Remember that the visualization below is interactive, you can click on the individual nodes and view the layer attributes, trained weights and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the network is composed of a repeating convolution-convolution-maxpool layer pattern to extract features using 3x3 convolution kernels (with weights binarized) and `Sign` activations, followed by fully connected layers acting as the classifier. Also notice the initial `MultiThreshold` layer at the beginning of the network, which is quantizing float inputs to 8-bit ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How FINN Implements Convolutions: Lowering and Streamlining\n",
    "\n",
    "In FINN, we implement convolutions with the *lowering* approach: we convert them to matrix-matrix multiply operations, where one of the matrices is generated by sliding a window over the input image. You can read more about the sliding window operator and how convolution lowering works [in this notebook](https://github.com/maltanar/qnn-inference-examples/blob/master/3-convolutional-binarized-gtsrb.ipynb). The streaming dataflow architecture we will end up with is going to look something like this figure from the [FINN-R paper](https://arxiv.org/abs/1809.04570):\n",
    "\n",
    "![](cnv-mp-fc.png)\n",
    "\n",
    "Note how the convolution layer looks very similar to the fully connected one in terms of the matrix-vector-threshold unit (MVTU), but now the MVTU is preceded by a sliding window unit that produces the matrix from the input image. All of these building blocks, including the `MaxPool` layer you see in this figure, exist as templated Vivado HLS C++ functions in [finn-hlslib](https://github.com/Xilinx/finn-hlslib).\n",
    "\n",
    "\n",
    "To target this kind of hardware architecture with our network we'll apply a convolution lowering transformation, in addition to streamlining. You may recall the *streamlining transformation* that we applied to the TFC-w1a1 network, which is a series of mathematical simplifications that allow us to get rid of floating point scaling operations by implementing few-bit activations as thresholding operations. **The current implementation of streamlining is highly network-specific and may not work for your network if its topology is very different than the example network here. We hope to rectify this in future releases.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/workspace/finn/notebooks/CNN/cnn_1d_3_classes_24_streamlined.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fdb17737a58>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelWrapper(build_dir + file_name + \"_tidy.onnx\")\n",
    "\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "model = model.transform(Streamline())\n",
    "\n",
    "model.save(build_dir + file_name + \"_streamlined.onnx\")\n",
    "showInNetron(build_dir + file_name + \"_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't go into too much detail about what happens in each transformation and why they are called in the particular order they are (feel free to visualize the intermediate steps using Netron yourself if you are curious) but here is a brief summmmary:\n",
    "\n",
    "* `Streamline` moves floating point scaling and addition operations closer to the input of the nearest thresholding activation and absorbs them into thresholds\n",
    "* `LowerConvsToMatMul` converts ONNX `Conv` nodes into sequences of `Im2Col, MatMul` nodes as discussed above. `Im2Col` is a custom FINN ONNX high-level node type that implements the sliding window operator.\n",
    "* `MakeMaxPoolNHWC` and `AbsorbTransposeIntoMultiThreshold` convert the *data layout* of the network into the NHWC data layout that finn-hlslib primitives use. NCHW means the tensor dimensions are ordered as `(N : batch, H : height, W : width, C : channels)` (assuming 2D images). The ONNX standard ops normally use the NCHW layout, but the ONNX intermediate representation itself does not dictate any data layout.\n",
    "* You may recall `ConvertBipolarMatMulToXnorPopcount` from the TFC-w1a1 example, which is needed to implement bipolar-by-bipolar (w1a1) networks correctly using finn-hlslib.\n",
    "\n",
    "Let's visualize the streamlined and lowered network with Netron. Observe how all the `Conv` nodes have turned into pairs of `Im2Col, MatMul` nodes, and many nodes including `BatchNorm, Mul, Add` nodes have disappeared and replaced with `MultiThreshold` nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Partitioning, Conversion to HLS Layers and Folding\n",
    "\n",
    "The next steps will be (again) very similar to what we did for the TFC-w1a1 network. We'll first convert the layers that we can put into the FPGA into their HLS equivalents and separate them out into a *dataflow partition*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/workspace/finn/notebooks/CNN/cnn_1d_3_classes_24_dataflow_model.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fdb9786dbe0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.move_reshape import MoveReshape\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "\n",
    "# choose the memory mode for the MVTU units, decoupled or const\n",
    "mem_mode = \"decoupled\"\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \"_streamlined.onnx\")\n",
    "model = model.transform(to_hls.InferBinaryStreamingFCLayer(mem_mode))\n",
    "model = model.transform(to_hls.InferQuantizedStreamingFCLayer(mem_mode))\n",
    "model = model.transform(to_hls.InferConvInpGen())\n",
    "model = model.transform(to_hls.InferStreamingMaxPool())\n",
    "\n",
    "# get rid of Reshape(-1, 1) operation between hlslib nodes\n",
    "model = model.transform(MoveReshape())\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(build_dir + file_name + \"_dataflow_parent.onnx\")\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "\n",
    "# save the dataflow partition with a different name for easier access\n",
    "dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "dataflow_model.save(build_dir + file_name + \"_dataflow_model.onnx\")\n",
    "showInNetron(build_dir + file_name + \"_dataflow_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if the maxpoolnhwc can be made into streamingmaxpool_batch nodes\n",
    "# basically, the input dimension into the maxpool node must be divisible by the maxpool kernel size\n",
    "# if this is not fulfilled, then the dataflowmodel will be broken up into several pieces\n",
    "# if done correctly, the dataflowmodel should be one linear model\n",
    "# this is commented out by default\n",
    "\n",
    "class InferStreamingMaxPool_test(Transformation):\n",
    "    \"\"\"Convert MaxPoolNHWC layers to StreamingMaxPool layers.\"\"\"\n",
    "\n",
    "    def apply(self, model):\n",
    "        graph = model.graph\n",
    "        node_ind = 0\n",
    "        graph_modified = False\n",
    "        for n in graph.node:\n",
    "            node_ind += 1\n",
    "            if n.op_type == \"MaxPoolNHWC\":\n",
    "                mp_input = n.input[0]\n",
    "                mp_output = n.output[0]\n",
    "                mp_in_shape = model.get_tensor_shape(mp_input)\n",
    "                # mp_out_shape = model.get_tensor_shape(mp_output)\n",
    "                dt = model.get_tensor_datatype(mp_input)\n",
    "                mp_inst = getCustomOp(n)\n",
    "                # stride = mp_inst.get_nodeattr(\"strides\")[0]\n",
    "                k = mp_inst.get_nodeattr(\"kernel_shape\")[0]\n",
    "                # pad = mp_inst.get_nodeattr(\"pads\")[0]\n",
    "                ifm_ch = mp_in_shape[-1]\n",
    "                ifm_dim = mp_in_shape[1]\n",
    "                # ofm_dim = mp_out_shape[1]\n",
    "                print(ifm_dim)\n",
    "                print(k)\n",
    "                if ifm_dim % k == 0:\n",
    "                    print(\"setting\")\n",
    "                    # create equivalent StreamingMaxPool_Batch node\n",
    "                    # TODO support non-k strides\n",
    "                    new_node = helper.make_node(\n",
    "                        \"StreamingMaxPool_Batch\",\n",
    "                        [mp_input],\n",
    "                        [mp_output],\n",
    "                        domain=\"finn\",\n",
    "                        backend=\"fpgadataflow\",\n",
    "                        PoolDim=k,\n",
    "                        NumChannels=ifm_ch,\n",
    "                        ImgDim=ifm_dim,\n",
    "                        dataType=dt.name,\n",
    "                    )\n",
    "                    graph.node.insert(node_ind, new_node)\n",
    "                    # remove old nodes\n",
    "                    graph.node.remove(n)\n",
    "                    graph_modified = True\n",
    "        if graph_modified:\n",
    "            model = model.transform(InferShapes())\n",
    "            model = model.transform(InferDataTypes())\n",
    "        return (model, graph_modified)\n",
    "\n",
    "# choose the memory mode for the MVTU units, decoupled or const\n",
    "#mem_mode = \"decoupled\"\n",
    "\n",
    "#model = ModelWrapper(build_dir + file_name + \"_streamlined.onnx\")\n",
    "#model = model.transform(to_hls.InferBinaryStreamingFCLayer(mem_mode))\n",
    "#model = model.transform(to_hls.InferQuantizedStreamingFCLayer(mem_mode))\n",
    "#model = model.transform(to_hls.InferConvInpGen())\n",
    "#model = model.transform(InferStreamingMaxPool_test())\n",
    "#model.save(build_dir + file_name + \"_dataflow_model.onnx\")\n",
    "#showInNetron(build_dir + file_name + \"_dataflow_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the additional `MoveReshape` transformation that was not used for TFC-w1a1. In the last Netron visualization you may have noticed a `Reshape` operation towards the end of the network where the convolutional part of the network ends and the fully-connected layers started. That `Reshape` is essentialy a tensor flattening operation, which we can remove for the purposes of hardware implementation. We can examine the contents of the dataflow partition with Netron, and observe the `ConvolutionInputGenerator`, `StreamingFCLayer_Batch` and `StreamingMaxPool_Batch` nodes that implement the sliding window, matrix multiply and maxpool operations in hlslib. *Note that the StreamingFCLayer instances following the ConvolutionInputGenerator nodes are really implementing the convolutions, despite the name. The final three StreamingFCLayer instances implement actual FC layers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to set the *folding factors* for certain layers to adjust the performance of our accelerator, similar to the TFC-w1a1 example. We'll also set the desired FIFO depths around those layers, which are important to achieve full throughput in the accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \"MultiThreshold_0_out0\"\n",
      "input: \"MatMul_0_param0\"\n",
      "input: \"MultiThreshold_1_param0\"\n",
      "output: \"MultiThreshold_1_out0\"\n",
      "op_type: \"StreamingFCLayer_Batch\"\n",
      "attribute {\n",
      "  name: \"ActVal\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"MH\"\n",
      "  i: 128\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"MW\"\n",
      "  i: 256\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"PE\"\n",
      "  i: 4\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"SIMD\"\n",
      "  i: 32\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"backend\"\n",
      "  s: \"fpgadataflow\"\n",
      "  type: STRING\n",
      "}\n",
      "attribute {\n",
      "  name: \"binaryXnorMode\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"inputDataType\"\n",
      "  s: \"UINT8\"\n",
      "  type: STRING\n",
      "}\n",
      "attribute {\n",
      "  name: \"mem_mode\"\n",
      "  s: \"decoupled\"\n",
      "  type: STRING\n",
      "}\n",
      "attribute {\n",
      "  name: \"noActivation\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"numInputVectors\"\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"outputDataType\"\n",
      "  s: \"BINARY\"\n",
      "  type: STRING\n",
      "}\n",
      "attribute {\n",
      "  name: \"resType\"\n",
      "  s: \"ap_resource_lut()\"\n",
      "  type: STRING\n",
      "}\n",
      "attribute {\n",
      "  name: \"weightDataType\"\n",
      "  s: \"BIPOLAR\"\n",
      "  type: STRING\n",
      "}\n",
      "attribute {\n",
      "  name: \"inFIFODepth\"\n",
      "  i: 128\n",
      "  type: INT\n",
      "}\n",
      "domain: \"finn\"\n",
      "\n",
      "input: \"MultiThreshold_1_out0\"\n",
      "input: \"XnorPopcountMatMul_0_param0\"\n",
      "output: \"XnorPopcountMatMul_0_out0\"\n",
      "op_type: \"StreamingFCLayer_Batch\"\n",
      "attribute {\n",
      "  name: \"ActVal\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"MH\"\n",
      "  i: 3\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"MW\"\n",
      "  i: 128\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"PE\"\n",
      "  i: 3\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"SIMD\"\n",
      "  i: 8\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"backend\"\n",
      "  s: \"fpgadataflow\"\n",
      "  type: STRING\n",
      "}\n",
      "attribute {\n",
      "  name: \"binaryXnorMode\"\n",
      "  i: 1\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"inputDataType\"\n",
      "  s: \"BINARY\"\n",
      "  type: STRING\n",
      "}\n",
      "attribute {\n",
      "  name: \"mem_mode\"\n",
      "  s: \"decoupled\"\n",
      "  type: STRING\n",
      "}\n",
      "attribute {\n",
      "  name: \"noActivation\"\n",
      "  i: 1\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"numInputVectors\"\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "attribute {\n",
      "  name: \"outputDataType\"\n",
      "  s: \"UINT32\"\n",
      "  type: STRING\n",
      "}\n",
      "attribute {\n",
      "  name: \"resType\"\n",
      "  s: \"ap_resource_lut()\"\n",
      "  type: STRING\n",
      "}\n",
      "attribute {\n",
      "  name: \"weightDataType\"\n",
      "  s: \"BINARY\"\n",
      "  type: STRING\n",
      "}\n",
      "attribute {\n",
      "  name: \"inFIFODepth\"\n",
      "  i: 128\n",
      "  type: INT\n",
      "}\n",
      "domain: \"finn\"\n",
      "\n",
      "\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/workspace/finn/notebooks/CNN/cnn_1d_3_classes_24_folded_intermediate.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fdb9787cb38>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_tlastmarker import InsertTLastMarker\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \"_dataflow_model.onnx\")\n",
    "fc_layers = model.get_nodes_by_op_type(\"StreamingFCLayer_Batch\")\n",
    "# each tuple is (PE, SIMD, in_fifo_depth) for a layer\n",
    "folding = [\n",
    "    (4, 32, 128),\n",
    "    (3, 8, 128)\n",
    "]\n",
    "\n",
    "for fcl, (pe, simd, ififodepth) in zip(fc_layers, folding):\n",
    "    fcl_inst = getCustomOp(fcl)\n",
    "    fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    fcl_inst.set_nodeattr(\"inFIFODepth\", ififodepth)\n",
    "    print(fcl)\n",
    "\n",
    "# use same SIMD values for the sliding window operators\n",
    "swg_layers = model.get_nodes_by_op_type(\"ConvolutionInputGenerator\")\n",
    "for i in range(len(swg_layers)):\n",
    "    swg_inst = getCustomOp(swg_layers[i])\n",
    "    simd = folding[i][1]\n",
    "    swg_inst.set_nodeattr(\"SIMD\", simd)\n",
    "\n",
    "# save intermediate so that we can reference in netron and debug if folding factors are not correct\n",
    "model.save(build_dir + file_name + \"_folded_intermediate.onnx\")\n",
    "showInNetron(build_dir + file_name + \"_folded_intermediate.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/workspace/finn/notebooks/CNN/cnn_1d_3_classes_24_folded.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fdb9786d2e8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the transformation\n",
    "model = ModelWrapper(build_dir + file_name + \"_folded_intermediate.onnx\")\n",
    "\n",
    "model = model.transform(InsertDWC())\n",
    "model = model.transform(InsertFIFO())\n",
    "model = model.transform(InsertTLastMarker())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "\n",
    "model.save(build_dir + file_name + \"_folded.onnx\")\n",
    "showInNetron(build_dir + file_name + \"_folded.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we visualize in Netron to observe the `StreamingDataWidthConverter` and `StreamingFIFO` nodes that have been inserted into graph, as well as the folding factors in the `PE` and `SIMD` attributes of each `StreamingFCLayer_Batch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network is now ready and we can start with the hardware generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hardware Generation\n",
    "\n",
    "From this point onward, the steps we have to follow do not depend on the particular network and will be exactly the same as the TFC-w1a1 example. We first proceed with HLS synthesis, **which may take 10-20 minutes depending on your host computer and your RAM cause of WSL**.\n",
    "\n",
    "**Note: WSL takes 10GB of RAM to perform synthesis, else it crashes halfway.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0:04:26.973955\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.prepare_ip import PrepareIP\n",
    "from finn.transformation.fpgadataflow.hlssynth_ip import HLSSynthIP\n",
    "from finn.util.basic import pynq_part_map\n",
    "time = datetime.datetime.now()\n",
    "\n",
    "test_pynq_board = \"Ultra96\"\n",
    "test_fpga_part = pynq_part_map[test_pynq_board]\n",
    "target_clk_ns = 10\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \"_folded.onnx\")\n",
    "model = model.transform(PrepareIP(test_fpga_part, target_clk_ns))\n",
    "model = model.transform(HLSSynthIP())\n",
    "model.save(build_dir + file_name + \"_ipgen.onnx\")\n",
    "print(\"took\", datetime.datetime.now() - time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the HLS synthesis is complete, we can stitch together the generated IP blocks into a larger IP that is the implementation of our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0:00:25.290564\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.replace_verilog_relpaths import (\n",
    "    ReplaceVerilogRelPaths,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.create_stitched_ip import CreateStitchedIP\n",
    "time = datetime.datetime.now()\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \"_ipgen.onnx\")\n",
    "model = model.transform(ReplaceVerilogRelPaths())\n",
    "model = model.transform(CreateStitchedIP(test_fpga_part))\n",
    "model.save(build_dir + file_name + \"_ipstitch.onnx\")\n",
    "print(\"took\", datetime.datetime.now() - time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a PYNQ project that includes the hardware \"shell\" that will support our accelerator, including the data movers, and run Vivado synthesis, **which may take around 30 minutes depending on your host computer.**\n",
    "\n",
    "*If you'd like to watch the progress, you can open the generated project file (printed below) with the Vivado GUI.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vivado synthesis project is at /tmp/finn_dev_dant/vivado_pynq_proj_merp1y74/resizer.xpr\n",
      "took 0:00:27.497358\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.make_pynq_proj import MakePYNQProject\n",
    "from finn.transformation.fpgadataflow.synth_pynq_proj import SynthPYNQProject\n",
    "time = datetime.datetime.now()\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \"_ipstitch.onnx\")\n",
    "model = model.transform(MakePYNQProject(test_pynq_board))\n",
    "vivado_proj = model.get_metadata_prop(\"vivado_pynq_proj\")\n",
    "print(\"Vivado synthesis project is at %s/resizer.xpr\" % vivado_proj)\n",
    "model.save(build_dir + file_name + \"_pynqproj.onnx\")\n",
    "print(\"took\", datetime.datetime.now() - time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0:13:27.306990\n"
     ]
    }
   ],
   "source": [
    "time = datetime.datetime.now()\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \"_pynqproj.onnx\")\n",
    "model = model.transform(SynthPYNQProject())\n",
    "model.save(build_dir + file_name + \"_synth.onnx\")\n",
    "print(\"took\", datetime.datetime.now() - time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deployment and Remote Execution\n",
    "\n",
    "Now that we're done with the hardware generation, we can generate a Python driver for accelerator and copy the necessary files onto our PYNQ board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "from finn.transformation.fpgadataflow.make_deployment import DeployToPYNQ\n",
    "\n",
    "# FINN will use ssh to deploy and run the generated accelerator\n",
    "# please run ultra96_port_forwarding.ipynb before transferring\n",
    "ip = \"localhost\"\n",
    "port = \"3100\"\n",
    "username = \"xilinx\"\n",
    "password = \"xilinx\"\n",
    "target_dir = \"/home/xilinx/finn/cnv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(build_dir + file_name + \"_synth.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy to the remote Ultra96\n",
    "model = model.transform(MakePYNQDriver())\n",
    "model = model.transform(DeployToPYNQ(ip, port, username, password, target_dir))\n",
    "model.save(build_dir + file_name + \"_pynq_deploy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh: connect to host localhost port 3100: Cannot assign requested address\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# verify that the files are copied over\n",
    "pynq_folder_name = vivado_proj[36:]\n",
    "#print(pynq_folder_name)\n",
    "! sshpass -p {password} ssh {username}@{ip} -p {port} 'ls -l {target_dir}/*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Dataset\n",
    "\n",
    "Load the testing data and do remote execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_model = ModelWrapper(build_dir + file_name + \"_dataflow_parent.onnx\")\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "sdp_node.set_nodeattr(\"model\", build_dir + file_name + \"_pynq_deploy.onnx\")\n",
    "parent_model.save(build_dir + file_name + \"_dataflow_parent_with_remote_bitfile_exec.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[ 48.6600,  54.0600,  60.2600,  66.4300,  68.1000, 113.7700, 116.4400,\n",
      "         119.4800, 120.8500, 120.9900, 120.1700, 118.0200, 115.6000,  67.2000,\n",
      "          67.5500,  64.4100,  58.1100,  49.5600,  40.4200,  25.8900,   9.9900,\n",
      "          -5.0000, -37.6100,  51.4800],\n",
      "        [ 28.7900,  28.5100,  34.4000,  51.2800,  78.8500,  96.0800, 106.5600,\n",
      "         112.0700, 113.3500, 111.3400, 106.7200, 100.8200,  93.4700,  78.5000,\n",
      "          57.1500,  39.7000,  27.0400,  24.5800,  28.4300,  38.1500,  52.4600,\n",
      "          60.8300,  68.8200,  21.6100]]), output: 1.0\n"
     ]
    }
   ],
   "source": [
    "# get the next input from the test_loader (dataset is random)\n",
    "\n",
    "test_input, test_output = next(iter(test_loader))\n",
    "test_output = test_output[0]\n",
    "\n",
    "print(\"input: {}, output: {}\".format(test_input[0], test_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-717f350c5cc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#print(np.load(\"input.npy\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mtest_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'input_dict' is not defined"
     ]
    }
   ],
   "source": [
    "def test_exec(model, input_dict, return_full_exec_context=False):\n",
    "    if not model.check_all_tensor_shapes_specified():\n",
    "        raise Exception(\"Found unspecified tensor shapes, try infer_shapes\")\n",
    "\n",
    "    graph = model.graph\n",
    "    # first, we need to make sure that every variable required by the graph has\n",
    "    # some buffer associated with it. this includes graph inputs (which includes\n",
    "    # the input data as well as the trained parameters) and the graph ValueInfo\n",
    "    # (intermediate tensors between layers)\n",
    "    # this is provided by the execution_context, which is a dict of np.ndarray\n",
    "    execution_context = model.make_empty_exec_context()\n",
    "    # fill in any inputs provided to this function\n",
    "    for inp_name in input_dict.keys():\n",
    "        if inp_name in execution_context:\n",
    "            if execution_context[inp_name].shape == input_dict[inp_name].shape:\n",
    "                execution_context[inp_name] = input_dict[inp_name]\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    \"Shape mismatch for provided input %s: found %s expected %s \"\n",
    "                    % (\n",
    "                        inp_name,\n",
    "                        str(execution_context[inp_name].shape),\n",
    "                        str(input_dict[inp_name].shape),\n",
    "                    )\n",
    "                )\n",
    "        # else:\n",
    "        # raise Exception(\"Provided input not found in graph context: %s\" % inp_name)\n",
    "\n",
    "    # check if model has an execution mode set\n",
    "    # if None, execute model node by node using execute_node()\n",
    "    # if set to \"remote_pynq\" execute model on PYNQ board\n",
    "    # if set to \"rtlsim\" execute model using pyverilator\n",
    "    model_exec_mode = model.get_metadata_prop(\"exec_mode\")\n",
    "    print(model_exec_mode)\n",
    "\n",
    "    inp = execution_context[model.graph.input[0].name]\n",
    "    print(inp)\n",
    "    # make copy of array before saving it\n",
    "    inp = inp.copy()\n",
    "    np.save(os.path.join(\"input_test.npy\"), inp)\n",
    "    \n",
    "    input = np.load(\"input_test.npy\")\n",
    "    print(input)\n",
    "\n",
    "#print(np.load(\"input.npy\"))\n",
    "test_exec(parent_model, input_dict, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from finn.core.onnx_exec import execute_onnx\n",
    "\n",
    "# perform software inference locally here\n",
    "software_output = cnv_software_model(test_input)\n",
    "\n",
    "iname = parent_model.graph.input[0].name\n",
    "oname = parent_model.graph.output[0].name\n",
    "ishape = parent_model.get_tensor_shape(iname)\n",
    "\n",
    "input_dict = {iname: software_output[0].reshape(ishape).detach().numpy()}\n",
    "print(input_dict)\n",
    "\n",
    "ret = execute_onnx(parent_model, input_dict, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def softmax_logits(logits):\n",
    "    prediction_list = softmax(logits)\n",
    "    max_val = -1\n",
    "    prediction = 0\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] > max_val:\n",
    "            max_val = prediction_list[i]\n",
    "            prediction = i\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "logits = ret[oname].flatten()\n",
    "prob = softmax(logits)\n",
    "\n",
    "print(\"software prediction\")\n",
    "output = cnv_hardware_model(software_output)\n",
    "\n",
    "for i in range(len(output)):\n",
    "    print(softmax(output[i].tolist()), \"prediction\", get_prediction(output[i]), \"target\", target[i].tolist())\n",
    "\n",
    "\n",
    "print(\"hardware prediction\")\n",
    "print(prob)\n",
    "print(\"predicted:\", classes[softmax_logits(logits)], \", actual:\", classes[int(test_output.tolist())])\n",
    "\n",
    "plt.figure(figsize=(20, 3)) \n",
    "plt.bar(classes, prob)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
