{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End FINN Flow for a \"Simple\" Convolutional Net (FINN v0.31b)\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "FINN walkthrough for CNN on Ultra96 for use in CG4002 by Daniel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changelog\n",
    "\n",
    "| Date | Changes |\n",
    "|:------:|:---------:|\n",
    "| 28/9/2020 | Added creation of CNN in section 1.1 (training not setup yet) |\n",
    "| 29/9/2020 | Tweaked creation of CNN and added notes for finn compilation. Added basic training and testing, with a simple CNN being implemented |\n",
    "| 5/10/2020 | Show accuracy and loss during training and cleaned up training code |\n",
    "| 19/10/2020 | Added Brevitas Conv1D layers and create new 1D CNN |\n",
    "\n",
    "## To Do\n",
    "| |\n",
    "|:------|\n",
    "| Show hardware utilization by pulling logs from Vivado in /tmp/ |\n",
    "| Clean up this notebook (FINN transformation parts) |\n",
    "| Exporting to ONNX is not working for 1D CNN, for the hardware synthesis, copy over the FC layers, export and synthesize | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Recap of the End-to-End Flow\n",
    "\n",
    "The FINN compiler comes with many *transformations* that modify the ONNX representation of the network according to certain patterns. This notebook will demonstrate a *possible* sequence of such transformations to take a particular trained network all the way down to hardware, as shown in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](finn-design-flow-example.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: toposort==1.5 from file:///workspace/finn/notebooks/CNN/toposort-1.5-py2.py3-none-any.whl in /workspace/.local/lib/python3.6/site-packages (1.5)\n",
      "toposort                      1.5      \n",
      "Requirement already satisfied: dependencies in /workspace/.local/lib/python3.6/site-packages (4.0.1)\n",
      "dependencies                  4.0.1    \n",
      "Requirement already satisfied: pandas in /workspace/.local/lib/python3.6/site-packages (1.1.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.6/site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "pandas                        1.1.3    \n"
     ]
    }
   ],
   "source": [
    "# run this to install libaries, and restart the kernel for it to take effect\n",
    "\n",
    "# download the whl file on this local machine\n",
    "# restart the entire kernel after installation to have the notebook recognize modules\n",
    "# toposort may not be needed, will delete if needed\n",
    "!pip install /workspace/finn/notebooks/CNN/toposort-1.5-py2.py3-none-any.whl --user\n",
    "!pip list | grep \"toposort\"\n",
    "\n",
    "!pip install dependencies --user\n",
    "!pip list | grep \"dependencies\"\n",
    "\n",
    "!pip install pandas --user\n",
    "!pip list | grep \"pandas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the build directory and file name\n",
    "build_dir = \"/workspace/finn/notebooks/CNN\"\n",
    "file_name = \"/test_cnn_1d_brevitas_new_dataset\"\n",
    "model_path = \"/cnv_1d.pt\"\n",
    "\n",
    "# exports a new model\n",
    "create_new_model = True\n",
    "\n",
    "# train the network, will implement loading of statedict later on\n",
    "train_network = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports are put here\n",
    "from finn.util.basic import make_build_dir\n",
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "import datetime # for tracking time for each code block\n",
    "import time\n",
    "\n",
    "# 1. Brevitas Export, FINN Import and Tidy-Up\n",
    "\n",
    "# 1.1 Network Setup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "from dependencies import Injector, value\n",
    "\n",
    "from brevitas.core.bit_width import BitWidthImplType\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.core.scaling import ScalingImplType\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module, ModuleList, BatchNorm2d, MaxPool2d, BatchNorm1d, MaxPool1d, Sequential\n",
    "\n",
    "from brevitas.nn import QuantConv2d, QuantLinear\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "\n",
    "# 1.2 Training the Network\n",
    "\n",
    "# imports\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 1.3 Software Comparison\n",
    "import torch.nn.functional as func\n",
    "\n",
    "# 1.4 Export and Tidy-Up\n",
    "\n",
    "import onnx\n",
    "import brevitas.onnx as bo\n",
    "\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.double_to_single_float import DoubleToSingleFloat\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames\n",
    "\n",
    "# not included in v0.3b\n",
    "#from finn.transformation.general import RemoveStaticGraphInputs\n",
    "\n",
    "# 2. How FINN Implements Convolutions: Lowering and Streamlining\n",
    "\n",
    "from finn.transformation.streamline import Streamline\n",
    "from finn.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from finn.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC\n",
    "#from finn.transformation.general import RemoveUnusedTensors\n",
    "\n",
    "# 3. Partitioning, Conversion to HLS Layers and Folding\n",
    "\n",
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (CreateDataflowPartition,)\n",
    "#from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "\n",
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_tlastmarker import InsertTLastMarker\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "\n",
    "from finn.transformation.fpgadataflow.prepare_ip import PrepareIP\n",
    "from finn.transformation.fpgadataflow.hlssynth_ip import HLSSynthIP\n",
    "from finn.util.basic import pynq_part_map\n",
    "\n",
    "from finn.transformation.fpgadataflow.replace_verilog_relpaths import (ReplaceVerilogRelPaths,)\n",
    "from finn.transformation.fpgadataflow.create_stitched_ip import CreateStitchedIP\n",
    "\n",
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_tlastmarker import InsertTLastMarker\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "\n",
    "import toposort\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "#from finn.transformation.fpgadataflow.make_pynq_proj import MakePYNQProject\n",
    "#from finn.transformation.fpgadataflow.synth_pynq_proj import SynthPYNQProject\n",
    "\n",
    "# 4. Hardware Generation\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "from finn.transformation.fpgadataflow.make_deployment import DeployToPYNQ\n",
    "\n",
    "#from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "from finn.transformation import Transformation\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.util.basic import get_by_name, make_build_dir\n",
    "from finn.util.basic import get_num_default_workers\n",
    "from finn.util.basic import pynq_part_map\n",
    "\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (CreateDataflowPartition,)\n",
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "#from finn.transformation.fpgadataflow.insert_iodma import InsertIODMA\n",
    "from finn.transformation.fpgadataflow.prepare_ip import PrepareIP\n",
    "from finn.transformation.fpgadataflow.hlssynth_ip import HLSSynthIP\n",
    "from finn.transformation.fpgadataflow.create_stitched_ip import CreateStitchedIP\n",
    "#from finn.transformation.fpgadataflow.floorplan import Floorplan\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames\n",
    "from shutil import copy\n",
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "\n",
    "# 5. Deployment and Remote Execution\n",
    "\n",
    "import pkg_resources as pk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "\n",
    "import numpy as np\n",
    "from finn.core.onnx_exec import execute_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Brevitas Export, FINN Import and Tidy-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Network Setup\n",
    "\n",
    "Declare the network below, and then create the CNN.\n",
    "\n",
    "Note that you may have to trail and error for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# network for CNV_1w1a\\n# QuantConv2d configuration (IN_CH, OUT_CH, bias)\\nCNV_OUT_CH_POOL = [(0, 64, False), (1, 64, True), (2, 128, False), (3, 128, True), (4, 256, False), (5, 256, False)]\\n\\n# Intermediate QuantLinear configuration\\nINTERMEDIATE_FC_PER_OUT_CH_SCALING = True\\nINTERMEDIATE_FC_FEATURES = [(256, 512), (512, 512)] # (IN_CH, OUT_CH)\\n\\n# Last QuantLinear configuration\\nLAST_FC_IN_FEATURES = 512\\nLAST_FC_PER_OUT_CH_SCALING = False\\n\\n# MaxPool2d configuration\\nPOOL_SIZE = 2\\n\\n# Network specific bit-widths and IO\\nWEIGHT_BIT_WIDTH = 1\\nACT_BIT_WIDTH = 1\\nIN_BIT_WIDTH = 8\\nNUM_CLASSES = 10\\nIN_CHANNELS = 3\\n\\nINPUT_SPECIFICATIONS = (1, 3, 32, 32)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declare features of the CNN\n",
    "\n",
    "# notes on CNN structure:\n",
    "# conv layers should have alternating maxpool layers, and the last layer should not be a max pool layer\n",
    "# last layer needs Sequential() to flatten for the fully connected layers\n",
    "#\n",
    "# maxpool layers must have their input size be divisible by kernel size, this is mandatory\n",
    "# Conv1D is non-synthesizeable under FINN 0.3b\n",
    "\n",
    "# QuantConv1d configuration (i, OUT_CH, is_maxpool_enabled)\n",
    "CNV_OUT_CH_POOL = [(0, 16, False), (1, 32, True), (2, 64, False), (3, 128, False)]\n",
    "KERNEL_SIZE = 2 # default 3\n",
    "NUM_CONV_LAYERS = 3\n",
    "\n",
    "# Intermediate QuantLinear configuration\n",
    "INTERMEDIATE_FC_PER_OUT_CH_SCALING = True\n",
    "INTERMEDIATE_FC_FEATURES = [(128, 256)] # (IN_CH, OUT_CH)\n",
    "\n",
    "# Last QuantLinear configuration\n",
    "LAST_FC_IN_FEATURES = 256\n",
    "LAST_FC_PER_OUT_CH_SCALING = False\n",
    "\n",
    "# MaxPool2d configuration\n",
    "POOL_SIZE = 2\n",
    "\n",
    "# Network specific bit-widths and IO\n",
    "WEIGHT_BIT_WIDTH = 1\n",
    "ACT_BIT_WIDTH = 1\n",
    "IN_BIT_WIDTH = 8\n",
    "NUM_CLASSES = 3\n",
    "IN_CHANNELS = 2\n",
    "\n",
    "# only use inputs that are multiples of 4\n",
    "INPUT_SPECIFICATIONS = (1, 2, 8) # batch size, channels, length\n",
    "\n",
    "\"\"\"\n",
    "# network for CNV_1w1a\n",
    "# QuantConv2d configuration (IN_CH, OUT_CH, bias)\n",
    "CNV_OUT_CH_POOL = [(0, 64, False), (1, 64, True), (2, 128, False), (3, 128, True), (4, 256, False), (5, 256, False)]\n",
    "\n",
    "# Intermediate QuantLinear configuration\n",
    "INTERMEDIATE_FC_PER_OUT_CH_SCALING = True\n",
    "INTERMEDIATE_FC_FEATURES = [(256, 512), (512, 512)] # (IN_CH, OUT_CH)\n",
    "\n",
    "# Last QuantLinear configuration\n",
    "LAST_FC_IN_FEATURES = 512\n",
    "LAST_FC_PER_OUT_CH_SCALING = False\n",
    "\n",
    "# MaxPool2d configuration\n",
    "POOL_SIZE = 2\n",
    "\n",
    "# Network specific bit-widths and IO\n",
    "WEIGHT_BIT_WIDTH = 1\n",
    "ACT_BIT_WIDTH = 1\n",
    "IN_BIT_WIDTH = 8\n",
    "NUM_CLASSES = 10\n",
    "IN_CHANNELS = 3\n",
    "\n",
    "INPUT_SPECIFICATIONS = (1, 3, 32, 32)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuantConv1D as per https://github.com/Xilinx/brevitas/blob/quant_quartznet_4b-r0/brevitas/nn/quant_conv1d.py\n",
    "from enum import auto\n",
    "from typing import Union, Optional, Tuple\n",
    "import re\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import docrep\n",
    "from torch.nn import Conv1d, Module\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import conv1d\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from brevitas.core.bit_width import BitWidthParameter, BitWidthConst, BitWidthImplType\n",
    "from brevitas.core.quant import QuantType, IdentityQuant\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.core.scaling import ScalingImplType, SCALING_SCALAR_SHAPE\n",
    "from brevitas.core.stats import StatsInputViewShapeImpl, StatsOp\n",
    "from brevitas.function.ops import max_uint, ceil_ste\n",
    "#from brevitas.function.ops_ste import ceil_ste\n",
    "from brevitas.proxy.parameter_quant import WeightQuantProxy, BiasQuantProxy, WeightReg\n",
    "from brevitas.utils.python_utils import AutoName\n",
    "from brevitas.nn.quant_layer import QuantLayer, SCALING_MIN_VAL\n",
    "from brevitas.config import docstrings\n",
    "__all__ = ['QuantConv1d']\n",
    "\n",
    "\n",
    "class PaddingType(AutoName):\n",
    "    STANDARD = auto()\n",
    "    SAME = auto()\n",
    "\n",
    "\n",
    "@docstrings.dedent\n",
    "class QuantConv1d(QuantLayer, Conv1d):\n",
    "    \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        %(weight_quant_proxy.parameters_with_prefix)s\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: Union[int, Tuple[int]],\n",
    "                 stride: Union[int, Tuple[int]] = 1,\n",
    "                 padding: Union[int, Tuple[int]] = 0,\n",
    "                 padding_type: PaddingType = PaddingType.STANDARD,\n",
    "                 dilation: Union[int, Tuple[int]] = 1,\n",
    "                 groups: int = 1,\n",
    "                 bias: bool = True,\n",
    "                 bias_quant_type: QuantType = QuantType.FP,\n",
    "                 bias_narrow_range: bool = False,\n",
    "                 bias_bit_width: int = None,\n",
    "                 weight_quant_override: WeightQuantProxy = None,\n",
    "                 weight_quant_type: QuantType = QuantType.FP,\n",
    "                 weight_narrow_range: bool = False,\n",
    "                 weight_scaling_override: Optional[Module] = None,\n",
    "                 weight_bit_width_impl_override: Union[BitWidthParameter, BitWidthConst] = None,\n",
    "                 weight_bit_width_impl_type: BitWidthImplType = BitWidthImplType.CONST,\n",
    "                 weight_restrict_bit_width_type: RestrictValueType = RestrictValueType.INT,\n",
    "                 weight_bit_width: int = 32,\n",
    "                 weight_min_overall_bit_width: Optional[int] = 2,\n",
    "                 weight_max_overall_bit_width: Optional[int] = None,\n",
    "                 weight_scaling_impl_type: ScalingImplType = ScalingImplType.STATS,\n",
    "                 weight_scaling_const: Optional[float] = None,\n",
    "                 weight_scaling_stats_op: StatsOp = StatsOp.MAX,\n",
    "                 weight_scaling_per_output_channel: bool = False,\n",
    "                 weight_ternary_threshold: float = 0.5,\n",
    "                 weight_restrict_scaling_type: RestrictValueType = RestrictValueType.LOG_FP,\n",
    "                 weight_scaling_stats_sigma: float = 3.0,\n",
    "                 weight_scaling_min_val: float = SCALING_MIN_VAL,\n",
    "                 weight_override_pretrained_bit_width: bool = False,\n",
    "                 compute_output_scale: bool = False,\n",
    "                 compute_output_bit_width: bool = False,\n",
    "                 return_quant_tensor: bool = False) -> None:\n",
    "        QuantLayer.__init__(self,\n",
    "                            compute_output_scale=compute_output_scale,\n",
    "                            compute_output_bit_width=compute_output_bit_width,\n",
    "                            return_quant_tensor=return_quant_tensor)\n",
    "        Conv1d.__init__(self,\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=padding,\n",
    "                        dilation=dilation,\n",
    "                        groups=groups,\n",
    "                        bias=bias)\n",
    "        if weight_quant_type == QuantType.FP and compute_output_bit_width:\n",
    "            raise Exception(\"Computing output bit width requires enabling quantization\")\n",
    "        if bias_quant_type != QuantType.FP and not (compute_output_scale and compute_output_bit_width):\n",
    "            raise Exception(\"Quantizing bias requires to compute output scale and output bit width\")\n",
    "\n",
    "        self.per_elem_ops = 2 * self.kernel_size[0] * (in_channels // groups)\n",
    "        self.padding_type = padding_type\n",
    "        self.weight_reg = WeightReg()\n",
    "\n",
    "        if weight_quant_override is not None:\n",
    "            self.weight_quant = weight_quant_override\n",
    "            self.weight_quant.add_tracked_parameter(self.weight)\n",
    "        else:\n",
    "            weight_scaling_stats_input_concat_dim = 1\n",
    "            if weight_scaling_per_output_channel:\n",
    "                weight_stats_input_view_shape_impl = StatsInputViewShapeImpl.OVER_OUTPUT_CHANNELS\n",
    "                weight_scaling_shape = self.per_output_channel_broadcastable_shape\n",
    "                weight_scaling_stats_reduce_dim = 1\n",
    "            else:\n",
    "                weight_stats_input_view_shape_impl = StatsInputViewShapeImpl.OVER_TENSOR\n",
    "                weight_scaling_shape = SCALING_SCALAR_SHAPE\n",
    "                weight_scaling_stats_reduce_dim = None\n",
    "\n",
    "            if weight_scaling_stats_op == StatsOp.MAX_AVE:\n",
    "                weight_stats_input_view_shape_impl = StatsInputViewShapeImpl.OVER_OUTPUT_CHANNELS\n",
    "                weight_scaling_stats_reduce_dim = 1\n",
    "\n",
    "            self.weight_quant = WeightQuantProxy(bit_width=weight_bit_width,\n",
    "                                                 quant_type=weight_quant_type,\n",
    "                                                 narrow_range=weight_narrow_range,\n",
    "                                                 scaling_override=weight_scaling_override,\n",
    "                                                 restrict_scaling_type=weight_restrict_scaling_type,\n",
    "                                                 scaling_const=weight_scaling_const,\n",
    "                                                 scaling_stats_op=weight_scaling_stats_op,\n",
    "                                                 scaling_impl_type=weight_scaling_impl_type,\n",
    "                                                 scaling_stats_reduce_dim=weight_scaling_stats_reduce_dim,\n",
    "                                                 scaling_shape=weight_scaling_shape,\n",
    "                                                 bit_width_impl_type=weight_bit_width_impl_type,\n",
    "                                                 bit_width_impl_override=weight_bit_width_impl_override,\n",
    "                                                 restrict_bit_width_type=weight_restrict_bit_width_type,\n",
    "                                                 min_overall_bit_width=weight_min_overall_bit_width,\n",
    "                                                 max_overall_bit_width=weight_max_overall_bit_width,\n",
    "                                                 tracked_parameter_list_init=self.weight,\n",
    "                                                 ternary_threshold=weight_ternary_threshold,\n",
    "                                                 scaling_stats_input_view_shape_impl=weight_stats_input_view_shape_impl,\n",
    "                                                 scaling_stats_input_concat_dim=weight_scaling_stats_input_concat_dim,\n",
    "                                                 scaling_stats_sigma=weight_scaling_stats_sigma,\n",
    "                                                 scaling_min_val=weight_scaling_min_val,\n",
    "                                                 override_pretrained_bit_width=weight_override_pretrained_bit_width)\n",
    "        self.bias_quant = BiasQuantProxy(quant_type=bias_quant_type,\n",
    "                                         bit_width=bias_bit_width,\n",
    "                                         narrow_range=bias_narrow_range)\n",
    "\n",
    "    @property\n",
    "    def per_output_channel_broadcastable_shape(self):\n",
    "        if self.transposed:\n",
    "            raise Exception(\"Transposed filters are not supported.\")\n",
    "        else:\n",
    "            output_dim = 0\n",
    "        per_channel_size = [1] * len(self.weight.size())\n",
    "        per_channel_size[output_dim] = self.out_channels\n",
    "        per_channel_size = tuple(per_channel_size)\n",
    "        return per_channel_size\n",
    "\n",
    "    @property\n",
    "    def int_weight(self):\n",
    "        if isinstance(self.weight_quant.tensor_quant, IdentityQuant):\n",
    "            raise Exception(\"Can't export int weight without quantization enabled\")\n",
    "        return self.weight_quant.int_weight(self.weight)\n",
    "\n",
    "    @property\n",
    "    def quant_weight_scale(self):\n",
    "        \"\"\"\n",
    "        Returns scale factor of the quantized weights with scalar () shape or (self.out_channels, 1, 1)\n",
    "        shape depending on whether scaling is per layer or per-channel.\n",
    "        -------\n",
    "        \"\"\"\n",
    "        if isinstance(self.weight_quant.tensor_quant, IdentityQuant):\n",
    "            raise Exception(\"Can't generate scaling factor without quantization enabled\")\n",
    "        zero_hw_sentinel = self.weight_quant.zero_hw_sentinel\n",
    "        _, scale, _ = self.weight_quant.tensor_quant(self.weight, zero_hw_sentinel)\n",
    "        return scale\n",
    "\n",
    "    def forward(self, input):\n",
    "        output_scale = None\n",
    "        output_bit_width = None\n",
    "        quant_bias_bit_width = None\n",
    "\n",
    "        input, input_scale, input_bit_width = self.unpack_input(input)\n",
    "        quant_weight, quant_weight_scale, quant_weight_bit_width = self.weight_quant(self.weight)\n",
    "        quant_weight = self.weight_reg(quant_weight)\n",
    "\n",
    "        if self.compute_output_bit_width:\n",
    "            assert input_bit_width is not None\n",
    "            output_bit_width = self.max_output_bit_width(input_bit_width, quant_weight_bit_width)\n",
    "        if self.compute_output_scale:\n",
    "            assert input_scale is not None\n",
    "            output_scale = input_scale * quant_weight_scale\n",
    "\n",
    "        if self.bias is not None:\n",
    "            quant_bias, _, quant_bias_bit_width = self.bias_quant(self.bias, output_scale, output_bit_width)\n",
    "            output = self.conv1d(input, quant_weight, quant_bias)\n",
    "        else:\n",
    "            output = self.conv1d(input, quant_weight, None)\n",
    "\n",
    "        if self.compute_output_bit_width and quant_bias_bit_width is not None:\n",
    "            output_bit_width = torch.where(quant_bias_bit_width > output_bit_width,\n",
    "                                           quant_bias_bit_width,\n",
    "                                           output_bit_width)\n",
    "\n",
    "        return self.pack_output(output, output_scale, output_bit_width)\n",
    "\n",
    "    def conv1d(self, x, weight, bias):\n",
    "        if self.padding_type == PaddingType.SAME:\n",
    "            out = self.conv1d_same_padding(x, weight, bias)\n",
    "        else:\n",
    "            out = conv1d(x, weight, bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "        return out\n",
    "\n",
    "    def conv1d_same_padding(self, x, weight, bias):\n",
    "        ih = x.size()[-1]\n",
    "        kh = weight.size()[-1]\n",
    "        sh = self.stride[0]\n",
    "        oh = math.ceil(ih / sh)\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        if pad_h > 0:\n",
    "            x = F.pad(x, [pad_h // 2, pad_h - pad_h // 2])\n",
    "        out = F.conv1d(x, weight, bias, self.stride, 0, self.dilation, self.groups)\n",
    "        return out\n",
    "\n",
    "    def merge_bn_in(self, bn, affine_only, sign_only):\n",
    "        raise Exception(\"Merged Batch-Normalization is not yet supported\")\n",
    "\n",
    "    def max_output_bit_width(self, input_bit_width, weight_bit_width):\n",
    "        max_uint_input = max_uint(bit_width=input_bit_width, narrow_range=False)\n",
    "        max_kernel_val = self.weight_quant.tensor_quant.int_quant.max_uint(weight_bit_width)\n",
    "        group_size = self.out_channels // self.groups\n",
    "        max_uint_output = max_uint_input * max_kernel_val * self.kernel_size[0] * group_size\n",
    "        max_output_bit_width = ceil_ste(torch.log2(max_uint_output))\n",
    "        return max_output_bit_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common\n",
    "from brevitas.core.bit_width import BitWidthImplType\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.core.scaling import ScalingImplType\n",
    "from brevitas.core.stats import StatsOp\n",
    "from brevitas.nn import QuantConv2d, QuantHardTanh, QuantLinear\n",
    "\n",
    "# Quant common\n",
    "BIT_WIDTH_IMPL_TYPE = BitWidthImplType.CONST\n",
    "SCALING_VALUE_TYPE = RestrictValueType.LOG_FP\n",
    "SCALING_IMPL_TYPE = ScalingImplType.PARAMETER\n",
    "NARROW_RANGE_ENABLED = True\n",
    "\n",
    "# Weight quant common\n",
    "STATS_OP = StatsOp.MEAN_LEARN_SIGMA_STD\n",
    "BIAS_ENABLED = False\n",
    "WEIGHT_SCALING_IMPL_TYPE = ScalingImplType.STATS\n",
    "SIGMA = 0.001\n",
    "\n",
    "# QuantHardTanh configuration\n",
    "HARD_TANH_MIN = -1.0\n",
    "HARD_TANH_MAX = 1.0\n",
    "ACT_PER_OUT_CH_SCALING = False\n",
    "\n",
    "# QuantConv2d configuration\n",
    "CONV_PER_OUT_CH_SCALING = True\n",
    "\n",
    "def get_stats_op(quant_type):\n",
    "    if quant_type == QuantType.BINARY:\n",
    "        return StatsOp.AVE\n",
    "    else:\n",
    "        return StatsOp.MAX\n",
    "\n",
    "\n",
    "def get_quant_type(bit_width):\n",
    "    if bit_width is None:\n",
    "        return QuantType.FP\n",
    "    elif bit_width == 1:\n",
    "        return QuantType.BINARY\n",
    "    else:\n",
    "        return QuantType.INT\n",
    "\n",
    "\n",
    "def get_act_quant(act_bit_width, act_quant_type):\n",
    "    if act_quant_type == QuantType.INT:\n",
    "        act_scaling_impl_type = ScalingImplType.PARAMETER\n",
    "    else:\n",
    "        act_scaling_impl_type = ScalingImplType.CONST\n",
    "    return QuantHardTanh(quant_type=act_quant_type,\n",
    "                         bit_width=act_bit_width,\n",
    "                         bit_width_impl_type=BIT_WIDTH_IMPL_TYPE,\n",
    "                         min_val=HARD_TANH_MIN,\n",
    "                         max_val=HARD_TANH_MAX,\n",
    "                         scaling_impl_type=act_scaling_impl_type,\n",
    "                         restrict_scaling_type=SCALING_VALUE_TYPE,\n",
    "                         scaling_per_channel=ACT_PER_OUT_CH_SCALING,\n",
    "                         narrow_range=NARROW_RANGE_ENABLED)\n",
    "\n",
    "\n",
    "def get_quant_linear(in_features, out_features, per_out_ch_scaling, bit_width, quant_type, stats_op):\n",
    "    return QuantLinear(bias=BIAS_ENABLED,\n",
    "                       in_features=in_features,\n",
    "                       out_features=out_features,\n",
    "                       weight_quant_type=quant_type,\n",
    "                       weight_narrow_range=NARROW_RANGE_ENABLED,\n",
    "                       weight_bit_width=bit_width,\n",
    "                       weight_bit_width_impl_type=BIT_WIDTH_IMPL_TYPE,\n",
    "                       weight_scaling_per_output_channel=per_out_ch_scaling,\n",
    "                       weight_scaling_stats_op=stats_op,\n",
    "                       weight_scaling_stats_sigma=SIGMA)\n",
    "\n",
    "\n",
    "def get_quant_conv2d(in_ch, out_ch, bit_width, quant_type, stats_op):\n",
    "    return QuantConv2d(in_channels=in_ch,\n",
    "                       kernel_size=KERNEL_SIZE,\n",
    "                       out_channels=out_ch,\n",
    "                       weight_quant_type=quant_type,\n",
    "                       weight_bit_width=bit_width,\n",
    "                       weight_narrow_range=NARROW_RANGE_ENABLED,\n",
    "                       weight_scaling_impl_type=WEIGHT_SCALING_IMPL_TYPE,\n",
    "                       weight_scaling_stats_op=stats_op,\n",
    "                       weight_scaling_stats_sigma=SIGMA,\n",
    "                       weight_scaling_per_output_channel=CONV_PER_OUT_CH_SCALING,\n",
    "                       weight_restrict_scaling_type=SCALING_VALUE_TYPE,\n",
    "                       weight_bit_width_impl_type=BIT_WIDTH_IMPL_TYPE,\n",
    "                       bias=BIAS_ENABLED)\n",
    "\n",
    "def get_quant_conv1d(in_ch, out_ch, bit_width, quant_type, stats_op):\n",
    "    return QuantConv1d(in_channels=in_ch,\n",
    "                       kernel_size=KERNEL_SIZE,\n",
    "                       out_channels=out_ch,\n",
    "                       weight_quant_type=quant_type,\n",
    "                       weight_bit_width=bit_width,\n",
    "                       weight_narrow_range=NARROW_RANGE_ENABLED,\n",
    "                       weight_scaling_impl_type=WEIGHT_SCALING_IMPL_TYPE,\n",
    "                       weight_scaling_stats_op=stats_op,\n",
    "                       weight_scaling_stats_sigma=SIGMA,\n",
    "                       weight_scaling_per_output_channel=CONV_PER_OUT_CH_SCALING,\n",
    "                       weight_restrict_scaling_type=SCALING_VALUE_TYPE,\n",
    "                       weight_bit_width_impl_type=BIT_WIDTH_IMPL_TYPE,\n",
    "                       bias=BIAS_ENABLED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the classes needed for the CNN, taken from: https://github.com/maltanar/brevitas_cnv_lfc\n",
    "# this is where the pre-trained models also come from, however, we will import the whole thing here to make custom CNNs\n",
    "class CNV(Module):\n",
    "\n",
    "    def __init__(self, num_classes=10, weight_bit_width=None, act_bit_width=None,in_bit_width=None, in_ch=3, device=\"cpu\"):\n",
    "        super(CNV, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        weight_quant_type = get_quant_type(weight_bit_width)\n",
    "        act_quant_type = get_quant_type(act_bit_width)\n",
    "        in_quant_type = get_quant_type(in_bit_width)\n",
    "        stats_op = get_stats_op(weight_quant_type)\n",
    "\n",
    "        self.conv_features = ModuleList()\n",
    "        self.linear_features = ModuleList()\n",
    "        self.conv_features.append(get_act_quant(in_bit_width, in_quant_type))\n",
    "\n",
    "        # convolution layers\n",
    "        for i, out_ch, is_pool_enabled in CNV_OUT_CH_POOL:\n",
    "            self.conv_features.append(get_quant_conv1d(in_ch=in_ch,\n",
    "                                                       out_ch=out_ch,\n",
    "                                                       bit_width=weight_bit_width,\n",
    "                                                       quant_type=weight_quant_type,\n",
    "                                                       stats_op=stats_op))\n",
    "            in_ch = out_ch\n",
    "            self.conv_features.append(BatchNorm1d(in_ch))\n",
    "            if i == (NUM_CONV_LAYERS - 1):\n",
    "                self.conv_features.append(Sequential())\n",
    "            self.conv_features.append(get_act_quant(act_bit_width, act_quant_type))\n",
    "            if is_pool_enabled:\n",
    "                self.conv_features.append(MaxPool1d(kernel_size=2))\n",
    "\n",
    "        # fully connected layers\n",
    "        for in_features, out_features in INTERMEDIATE_FC_FEATURES:\n",
    "            self.linear_features.append(get_quant_linear(in_features=in_features,\n",
    "                                                         out_features=out_features,\n",
    "                                                         per_out_ch_scaling=INTERMEDIATE_FC_PER_OUT_CH_SCALING,\n",
    "                                                         bit_width=weight_bit_width,\n",
    "                                                         quant_type=weight_quant_type,\n",
    "                                                         stats_op=stats_op))\n",
    "            self.linear_features.append(BatchNorm1d(out_features))\n",
    "            self.linear_features.append(get_act_quant(act_bit_width, act_quant_type))\n",
    "            \n",
    "        # last layer\n",
    "        self.fc = get_quant_linear(in_features=LAST_FC_IN_FEATURES,\n",
    "                                   out_features=num_classes,\n",
    "                                   per_out_ch_scaling=LAST_FC_PER_OUT_CH_SCALING,\n",
    "                                   bit_width=weight_bit_width,\n",
    "                                   quant_type=weight_quant_type,\n",
    "                                   stats_op=stats_op)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = 2.0 * x - torch.tensor([1.0]).to(self.device)\n",
    "        for mod in self.conv_features:\n",
    "            x = mod(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        for mod in self.linear_features:\n",
    "            x = mod(x)\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "# will not be used as we wont use cfg\n",
    "def cnv(cfg):\n",
    "    weight_bit_width = cfg.getint('QUANT', 'WEIGHT_BIT_WIDTH')\n",
    "    act_bit_width = cfg.getint('QUANT', 'ACT_BIT_WIDTH')\n",
    "    in_bit_width = cfg.getint('QUANT', 'IN_BIT_WIDTH')\n",
    "    num_classes = cfg.getint('MODEL', 'NUM_CLASSES')\n",
    "    in_channels = cfg.getint('MODEL', 'IN_CHANNELS')\n",
    "    net = CNV(weight_bit_width=weight_bit_width,\n",
    "              act_bit_width=act_bit_width,\n",
    "              in_bit_width=in_bit_width,\n",
    "              num_classes=num_classes,\n",
    "              in_ch=in_channels)\n",
    "    return net\n",
    "\n",
    "def cnv_manual(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS):\n",
    "    net = CNV(weight_bit_width=WEIGHT_BIT_WIDTH,\n",
    "              act_bit_width=ACT_BIT_WIDTH,\n",
    "              in_bit_width=IN_BIT_WIDTH,\n",
    "              num_classes=NUM_CLASSES,\n",
    "              in_ch=IN_CHANNELS)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNV(\n",
      "  (conv_features): ModuleList(\n",
      "    (0): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): RescalingIntQuant(\n",
      "            (int_quant): IntQuant(\n",
      "              (float_to_int_impl): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): RoundSte()\n",
      "                  (1): Identity()\n",
      "                )\n",
      "              )\n",
      "              (tensor_clamp_impl): TensorClamp()\n",
      "            )\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (int_scaling_impl): IntScaling(\n",
      "              (forward_impl): SignedFpIntScale()\n",
      "            )\n",
      "            (msb_clamp_bit_width_impl): BitWidthConst()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): QuantConv1d(\n",
      "      2, 16, kernel_size=(2,), stride=(1,), bias=False\n",
      "      (weight_reg): WeightReg()\n",
      "      (weight_quant): WeightQuantProxy(\n",
      "        (tensor_quant): BinaryQuant(\n",
      "          (scaling_impl): ParameterStatsScaling(\n",
      "            (parameter_list_stats): ParameterListStats(\n",
      "              (first_tracked_param): _ViewParameterWrapper()\n",
      "              (stats): Stats(\n",
      "                (stats_impl): AbsAve()\n",
      "              )\n",
      "            )\n",
      "            (stats_scaling_impl): StatsScaling(\n",
      "              (affine_rescaling): Identity()\n",
      "              (restrict_scaling): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "              (restrict_scaling_preprocess): LogTwo()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bias_quant): BiasQuantProxy()\n",
      "    )\n",
      "    (2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): ClampedBinaryQuant(\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): QuantConv1d(\n",
      "      16, 32, kernel_size=(2,), stride=(1,), bias=False\n",
      "      (weight_reg): WeightReg()\n",
      "      (weight_quant): WeightQuantProxy(\n",
      "        (tensor_quant): BinaryQuant(\n",
      "          (scaling_impl): ParameterStatsScaling(\n",
      "            (parameter_list_stats): ParameterListStats(\n",
      "              (first_tracked_param): _ViewParameterWrapper()\n",
      "              (stats): Stats(\n",
      "                (stats_impl): AbsAve()\n",
      "              )\n",
      "            )\n",
      "            (stats_scaling_impl): StatsScaling(\n",
      "              (affine_rescaling): Identity()\n",
      "              (restrict_scaling): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "              (restrict_scaling_preprocess): LogTwo()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bias_quant): BiasQuantProxy()\n",
      "    )\n",
      "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): ClampedBinaryQuant(\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): QuantConv1d(\n",
      "      32, 64, kernel_size=(2,), stride=(1,), bias=False\n",
      "      (weight_reg): WeightReg()\n",
      "      (weight_quant): WeightQuantProxy(\n",
      "        (tensor_quant): BinaryQuant(\n",
      "          (scaling_impl): ParameterStatsScaling(\n",
      "            (parameter_list_stats): ParameterListStats(\n",
      "              (first_tracked_param): _ViewParameterWrapper()\n",
      "              (stats): Stats(\n",
      "                (stats_impl): AbsAve()\n",
      "              )\n",
      "            )\n",
      "            (stats_scaling_impl): StatsScaling(\n",
      "              (affine_rescaling): Identity()\n",
      "              (restrict_scaling): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "              (restrict_scaling_preprocess): LogTwo()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bias_quant): BiasQuantProxy()\n",
      "    )\n",
      "    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Sequential()\n",
      "    (11): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): ClampedBinaryQuant(\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): QuantConv1d(\n",
      "      64, 128, kernel_size=(2,), stride=(1,), bias=False\n",
      "      (weight_reg): WeightReg()\n",
      "      (weight_quant): WeightQuantProxy(\n",
      "        (tensor_quant): BinaryQuant(\n",
      "          (scaling_impl): ParameterStatsScaling(\n",
      "            (parameter_list_stats): ParameterListStats(\n",
      "              (first_tracked_param): _ViewParameterWrapper()\n",
      "              (stats): Stats(\n",
      "                (stats_impl): AbsAve()\n",
      "              )\n",
      "            )\n",
      "            (stats_scaling_impl): StatsScaling(\n",
      "              (affine_rescaling): Identity()\n",
      "              (restrict_scaling): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "              (restrict_scaling_preprocess): LogTwo()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bias_quant): BiasQuantProxy()\n",
      "    )\n",
      "    (13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): ClampedBinaryQuant(\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear_features): ModuleList(\n",
      "    (0): QuantLinear(\n",
      "      in_features=128, out_features=256, bias=False\n",
      "      (weight_reg): WeightReg()\n",
      "      (weight_quant): WeightQuantProxy(\n",
      "        (tensor_quant): BinaryQuant(\n",
      "          (scaling_impl): ParameterStatsScaling(\n",
      "            (parameter_list_stats): ParameterListStats(\n",
      "              (first_tracked_param): _ViewParameterWrapper()\n",
      "              (stats): Stats(\n",
      "                (stats_impl): AbsAve()\n",
      "              )\n",
      "            )\n",
      "            (stats_scaling_impl): StatsScaling(\n",
      "              (affine_rescaling): Identity()\n",
      "              (restrict_scaling): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "              (restrict_scaling_preprocess): LogTwo()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bias_quant): BiasQuantProxy()\n",
      "    )\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): ClampedBinaryQuant(\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): QuantLinear(\n",
      "    in_features=256, out_features=3, bias=False\n",
      "    (weight_reg): WeightReg()\n",
      "    (weight_quant): WeightQuantProxy(\n",
      "      (tensor_quant): BinaryQuant(\n",
      "        (scaling_impl): ParameterStatsScaling(\n",
      "          (parameter_list_stats): ParameterListStats(\n",
      "            (first_tracked_param): _ViewParameterWrapper()\n",
      "            (stats): Stats(\n",
      "              (stats_impl): AbsAve()\n",
      "            )\n",
      "          )\n",
      "          (stats_scaling_impl): StatsScaling(\n",
      "            (affine_rescaling): Identity()\n",
      "            (restrict_scaling): RestrictValue(\n",
      "              (forward_impl): Sequential(\n",
      "                (0): PowerOfTwo()\n",
      "                (1): ClampMin()\n",
      "              )\n",
      "            )\n",
      "            (restrict_scaling_preprocess): LogTwo()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bias_quant): BiasQuantProxy()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create the model and export\n",
    "bnn_pynq_model = cnv_manual(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS)\n",
    "\n",
    "# print for reference\n",
    "print(bnn_pynq_model) \n",
    "\n",
    "# export an untrained model\n",
    "# to do: setup checkpoint or state_dict\n",
    "#if create_new_model:\n",
    "#    bo.export_finn_onnx(bnn_pynq_model, INPUT_SPECIFICATIONS, build_dir + file_name + \".onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Training the Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# to be implemented later on\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49152, 33)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the data from csv\n",
    "df = pd.read_csv('dataset_3classes.csv')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([720, 2, 8])\n",
      "torch.Size([180, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "# stitch together 8 inputs (1x8) to make a 8x8 input, and make the training / testing data\n",
    "# 20% set aside for testing\n",
    "num_items = (df.shape[0]) // 8\n",
    "max_data = 900\n",
    "batch_size = 2\n",
    "\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "x_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for i in range(num_items):\n",
    "    if i == max_data:\n",
    "        break\n",
    "    starting_index = i * 8 * (num_items // max_data)\n",
    "    \n",
    "    data = np.array(((df.iloc[starting_index, 0],df.iloc[starting_index + 1,0],df.iloc[starting_index + 2,0],df.iloc[starting_index + 3,0],df.iloc[starting_index + 4,0],df.iloc[starting_index + 5,0],df.iloc[starting_index + 6,0],df.iloc[starting_index + 7,0]),\n",
    "                    (df.iloc[starting_index, 1],df.iloc[starting_index + 1, 1],df.iloc[starting_index + 2, 1],df.iloc[starting_index + 3, 1],df.iloc[starting_index + 4, 1],df.iloc[starting_index + 5, 1],df.iloc[starting_index + 6, 1],df.iloc[starting_index + 7, 1])))\n",
    "    \n",
    "    # do encoding, go by index as shown below\n",
    "    # classes used currently - 6\n",
    "    # idle, elbow_lock, hair, pushback, rocket, scarecrow\n",
    "    if 'shrug' in df.iloc[starting_index, 4]:\n",
    "        value = (0)\n",
    "    elif 'zigzag' in df.iloc[starting_index, 4]:\n",
    "        value = (1)\n",
    "    elif 'windows' in df.iloc[starting_index, 4]:\n",
    "        value = (2)\n",
    "    else:\n",
    "        continue\n",
    "    #\n",
    "    #elif 'pushback' in df.iloc[starting_index, 8]:\n",
    "    #    value = (3)\n",
    "    #elif 'rocket' in df.iloc[starting_index, 8]:\n",
    "    #    value = (4)\n",
    "    #else: # scarecrow\n",
    "    #    value = (5)\n",
    "    \n",
    "    #print(\"input\", i)\n",
    "    #print(\"value\", df.iloc[starting_index, 4])\n",
    "    #print(data)\n",
    "    #print(value)\n",
    "    \n",
    "    if i % 5 != 4: # training\n",
    "        x_train_list.append(data)\n",
    "        y_train_list.append(value) \n",
    "    else: # testing\n",
    "        x_test_list.append(data)\n",
    "        y_test_list.append(value)\n",
    "        \n",
    "# remove extra inputs that cannot fit in batch_size\n",
    "while len(x_train_list) % batch_size != 0:\n",
    "    x_train_list.pop()\n",
    "    y_train_list.pop()\n",
    "    \n",
    "while len(x_test_list) % batch_size != 0:\n",
    "    x_test_list.pop()\n",
    "    y_test_list.pop()\n",
    "        \n",
    "# transform to PyTorch DataLoader\n",
    "tensor_x_train = torch.Tensor(x_train_list) # transform to torch tensor\n",
    "tensor_y_train = torch.Tensor(y_train_list)\n",
    "    \n",
    "print(tensor_x_train.size())\n",
    "\n",
    "dataset_train = TensorDataset(tensor_x_train,tensor_y_train)\n",
    "train_loader = DataLoader(dataset_train, batch_size = batch_size, shuffle = True, num_workers = 1)\n",
    "\n",
    "tensor_x_test = torch.Tensor(x_test_list) # transform to torch tensor\n",
    "tensor_y_test = torch.Tensor(y_test_list)\n",
    "    \n",
    "print(tensor_x_test.size())\n",
    "\n",
    "dataset_test = TensorDataset(tensor_x_test,tensor_y_test)\n",
    "test_loader = DataLoader(dataset_test, batch_size = batch_size, shuffle = True, num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "# validate input data size\n",
    "for data, target in train_loader:\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for training\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.1\n",
    "WEIGHT_DECAY = 0.0001\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "log_freq = 10\n",
    "\n",
    "EPOCHS = 50 # we'll see how long it takes first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' # linux CUDA not set up yet\n",
    "\n",
    "# training stuff\n",
    "# like the CNN model, it is taken from https://github.com/maltanar/brevitas_cnv_lfc\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class TrainingEpochMeters(object):\n",
    "    def __init__(self):\n",
    "        self.batch_time = AverageMeter()\n",
    "        self.data_time = AverageMeter()\n",
    "        self.losses = AverageMeter()\n",
    "        self.top1 = AverageMeter()\n",
    "        self.top5 = AverageMeter()\n",
    "\n",
    "class EvalEpochMeters(object):\n",
    "    def __init__(self):\n",
    "        self.model_time = AverageMeter()\n",
    "        self.loss_time = AverageMeter()\n",
    "        self.losses = AverageMeter()\n",
    "        self.top1 = AverageMeter()\n",
    "        self.top5 = AverageMeter()\n",
    "\n",
    "def eval_model(epoch=None):\n",
    "    eval_meters = EvalEpochMeters()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    bnn_pynq_model.eval()\n",
    "    criterion.eval()\n",
    "\n",
    "    for i, data in enumerate(test_loader):\n",
    "\n",
    "        end = time.time()\n",
    "        (input, target) = data\n",
    "\n",
    "        input = input.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        # compute output\n",
    "        output = bnn_pynq_model(input)\n",
    "\n",
    "        # measure model elapsed time\n",
    "        eval_meters.model_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        #compute loss\n",
    "        loss = criterion(output, target.long())\n",
    "        eval_meters.loss_time.update(time.time() - end)\n",
    "\n",
    "        prec1, prec5 = accuracy(output, target, topk=(1, NUM_CLASSES))\n",
    "        eval_meters.losses.update(loss.item(), input.size(0))\n",
    "        eval_meters.top1.update(prec1.item(), input.size(0))\n",
    "        eval_meters.top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred).long())\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def softmax_train(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def get_prediction_train(output_tensor):\n",
    "    prediction_list = softmax_train(output_tensor.tolist())\n",
    "    max_val = -1\n",
    "    prediction = 0\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] > max_val:\n",
    "            max_val = prediction_list[i]\n",
    "            prediction = i\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# returns the number of correct predictions\n",
    "def compare_output(output, target):\n",
    "    running_total = 0\n",
    "    \n",
    "    # iterate for each item in \n",
    "    for i in range(len(output)):\n",
    "        if get_prediction_train(output[i]) == int(target.tolist()[i]):\n",
    "            running_total += 1\n",
    "    \n",
    "    return running_total\n",
    "        \n",
    "def train_model():\n",
    "    optimizer = optim.SGD(bnn_pynq_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "    # training starts\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # keep track of training / testing metrics\n",
    "    num_inputs_training = len(train_loader) * batch_size\n",
    "    num_inputs_testing = len(test_loader) * batch_size\n",
    "        \n",
    "    for epoch in range(EPOCHS):\n",
    "        # keep track of training / testing metrics\n",
    "        running_loss_training = 0.0\n",
    "        running_loss_testing = 0.0\n",
    "        num_inputs_training_correct = 0\n",
    "        num_inputs_testing_correct = 0\n",
    "\n",
    "        # Set to training mode\n",
    "        bnn_pynq_model.train()\n",
    "        criterion.train()\n",
    "\n",
    "        # Init metrics\n",
    "        epoch_meters = TrainingEpochMeters()\n",
    "        start_data_loading = time.time()\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            (input, target) = data\n",
    "            #print(\"input\", input)\n",
    "            #print(\"target\", target)\n",
    "            \n",
    "            input = input.to(device, non_blocking=True)\n",
    "            target = target.to(device, non_blocking=True)\n",
    "\n",
    "            # measure data loading time\n",
    "            epoch_meters.data_time.update(time.time() - start_data_loading)\n",
    "\n",
    "            # Training batch starts\n",
    "            start_batch = time.time()\n",
    "            output = bnn_pynq_model(input)\n",
    "            loss = criterion(output, target.long())\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # measure elapsed time\n",
    "            epoch_meters.batch_time.update(time.time() - start_batch)\n",
    "            if i % int(log_freq) == 0 or i == len(train_loader) - 1:\n",
    "                # set the third value of topk depending on the number of classes\n",
    "                prec1, prec5 = accuracy(output.detach(), target, topk=(1, NUM_CLASSES))\n",
    "                epoch_meters.losses.update(loss.item(), input.size(0))\n",
    "                epoch_meters.top1.update(prec1.item(), input.size(0))\n",
    "                epoch_meters.top5.update(prec5.item(), input.size(0))\n",
    "                #self.logger.training_batch_cli_log(epoch_meters, epoch, i, len(self.train_loader))\n",
    "                \n",
    "            # update loss and accuracy\n",
    "            running_loss_training += loss.item()\n",
    "            num_inputs_training_correct += compare_output(output, target.long())\n",
    "\n",
    "            # training batch ends\n",
    "            #start_data_loading = time.time()\n",
    "            \n",
    "        # validate\n",
    "        for j, data in enumerate(test_loader):\n",
    "            (input, target) = data\n",
    "            \n",
    "            input = input.to(device, non_blocking=True)\n",
    "            target = target.to(device, non_blocking=True)\n",
    "            \n",
    "            output = bnn_pynq_model(input)\n",
    "            loss = criterion(output, target.long())\n",
    "        \n",
    "            # update loss and accuracy\n",
    "            running_loss_testing += loss.item()\n",
    "            num_inputs_testing_correct += compare_output(output, target.long())\n",
    "            \n",
    "        print('Epoch [%d] (took %.3f) training loss: %.3f, accuracy: %.3f, validation loss: %.3f, accuracy: %.3f' \n",
    "              % (epoch + 1, time.time() - start_data_loading, running_loss_training / num_inputs_training, num_inputs_training_correct / num_inputs_training, \n",
    "                running_loss_testing / num_inputs_testing, num_inputs_testing_correct / num_inputs_testing))\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Perform eval\n",
    "        with torch.no_grad():\n",
    "            top1avg = eval_model(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] (took 43.740) training loss: 0.499, accuracy: 0.600, validation loss: 0.349, accuracy: 0.717\n",
      "Epoch [2] (took 45.631) training loss: 0.383, accuracy: 0.710, validation loss: 0.310, accuracy: 0.706\n",
      "Epoch [3] (took 46.746) training loss: 0.336, accuracy: 0.717, validation loss: 0.335, accuracy: 0.717\n",
      "Epoch [4] (took 46.469) training loss: 0.334, accuracy: 0.703, validation loss: 0.395, accuracy: 0.533\n",
      "Epoch [5] (took 47.526) training loss: 0.327, accuracy: 0.701, validation loss: 0.421, accuracy: 0.644\n",
      "Epoch [6] (took 44.809) training loss: 0.335, accuracy: 0.693, validation loss: 0.349, accuracy: 0.728\n",
      "Epoch [7] (took 45.945) training loss: 0.311, accuracy: 0.710, validation loss: 0.256, accuracy: 0.744\n",
      "Epoch [8] (took 45.750) training loss: 0.295, accuracy: 0.738, validation loss: 0.279, accuracy: 0.767\n",
      "Epoch [9] (took 45.613) training loss: 0.316, accuracy: 0.721, validation loss: 0.292, accuracy: 0.739\n",
      "Epoch [10] (took 47.810) training loss: 0.335, accuracy: 0.671, validation loss: 0.336, accuracy: 0.639\n",
      "Epoch [11] (took 47.547) training loss: 0.316, accuracy: 0.713, validation loss: 0.310, accuracy: 0.694\n",
      "Epoch [12] (took 47.724) training loss: 0.319, accuracy: 0.671, validation loss: 0.394, accuracy: 0.689\n",
      "Epoch [13] (took 55.790) training loss: 0.318, accuracy: 0.706, validation loss: 0.411, accuracy: 0.617\n",
      "Epoch [14] (took 55.096) training loss: 0.322, accuracy: 0.672, validation loss: 0.274, accuracy: 0.728\n",
      "Epoch [15] (took 54.350) training loss: 0.329, accuracy: 0.671, validation loss: 0.274, accuracy: 0.722\n",
      "Epoch [16] (took 53.165) training loss: 0.322, accuracy: 0.669, validation loss: 0.278, accuracy: 0.711\n",
      "Epoch [17] (took 57.117) training loss: 0.348, accuracy: 0.661, validation loss: 0.262, accuracy: 0.750\n",
      "Epoch [18] (took 55.734) training loss: 0.302, accuracy: 0.683, validation loss: 0.242, accuracy: 0.783\n",
      "Epoch [19] (took 52.521) training loss: 0.291, accuracy: 0.682, validation loss: 0.301, accuracy: 0.683\n",
      "Epoch [20] (took 49.790) training loss: 0.280, accuracy: 0.714, validation loss: 0.247, accuracy: 0.739\n",
      "Epoch [21] (took 54.310) training loss: 0.294, accuracy: 0.681, validation loss: 0.306, accuracy: 0.650\n",
      "Epoch [22] (took 50.278) training loss: 0.287, accuracy: 0.703, validation loss: 0.263, accuracy: 0.711\n",
      "Epoch [23] (took 48.807) training loss: 0.303, accuracy: 0.678, validation loss: 0.258, accuracy: 0.639\n",
      "Epoch [24] (took 55.172) training loss: 0.295, accuracy: 0.667, validation loss: 0.266, accuracy: 0.694\n",
      "Epoch [25] (took 56.800) training loss: 0.300, accuracy: 0.701, validation loss: 0.299, accuracy: 0.644\n",
      "Epoch [26] (took 56.703) training loss: 0.295, accuracy: 0.710, validation loss: 0.238, accuracy: 0.756\n",
      "Epoch [27] (took 57.826) training loss: 0.290, accuracy: 0.717, validation loss: 0.366, accuracy: 0.706\n",
      "Epoch [28] (took 57.935) training loss: 0.286, accuracy: 0.714, validation loss: 0.270, accuracy: 0.750\n",
      "Epoch [29] (took 51.912) training loss: 0.339, accuracy: 0.656, validation loss: 0.318, accuracy: 0.650\n",
      "Epoch [30] (took 56.036) training loss: 0.301, accuracy: 0.708, validation loss: 0.271, accuracy: 0.767\n",
      "Epoch [31] (took 60.381) training loss: 0.292, accuracy: 0.690, validation loss: 0.296, accuracy: 0.694\n",
      "Epoch [32] (took 54.560) training loss: 0.286, accuracy: 0.682, validation loss: 0.290, accuracy: 0.672\n",
      "Epoch [33] (took 61.188) training loss: 0.320, accuracy: 0.644, validation loss: 0.296, accuracy: 0.711\n",
      "Epoch [34] (took 60.416) training loss: 0.295, accuracy: 0.690, validation loss: 0.344, accuracy: 0.611\n",
      "Epoch [35] (took 59.929) training loss: 0.304, accuracy: 0.671, validation loss: 0.249, accuracy: 0.767\n",
      "Epoch [36] (took 59.141) training loss: 0.293, accuracy: 0.681, validation loss: 0.282, accuracy: 0.678\n",
      "Epoch [37] (took 56.575) training loss: 0.291, accuracy: 0.682, validation loss: 0.267, accuracy: 0.717\n",
      "Epoch [38] (took 54.407) training loss: 0.268, accuracy: 0.714, validation loss: 0.273, accuracy: 0.694\n",
      "Epoch [39] (took 53.419) training loss: 0.291, accuracy: 0.701, validation loss: 0.257, accuracy: 0.722\n",
      "Epoch [40] (took 55.846) training loss: 0.300, accuracy: 0.683, validation loss: 0.253, accuracy: 0.756\n",
      "Epoch [41] (took 54.846) training loss: 0.306, accuracy: 0.708, validation loss: 0.327, accuracy: 0.750\n",
      "Epoch [42] (took 53.389) training loss: 0.298, accuracy: 0.703, validation loss: 0.303, accuracy: 0.667\n",
      "Epoch [43] (took 45.404) training loss: 0.298, accuracy: 0.692, validation loss: 0.302, accuracy: 0.711\n",
      "Epoch [44] (took 43.318) training loss: 0.296, accuracy: 0.701, validation loss: 0.283, accuracy: 0.700\n",
      "Epoch [45] (took 44.819) training loss: 0.297, accuracy: 0.694, validation loss: 0.282, accuracy: 0.722\n",
      "Epoch [46] (took 50.831) training loss: 0.312, accuracy: 0.703, validation loss: 0.326, accuracy: 0.617\n",
      "Epoch [47] (took 47.838) training loss: 0.305, accuracy: 0.694, validation loss: 0.252, accuracy: 0.739\n",
      "Epoch [48] (took 46.010) training loss: 0.277, accuracy: 0.732, validation loss: 0.274, accuracy: 0.711\n",
      "Epoch [49] (took 46.703) training loss: 0.304, accuracy: 0.685, validation loss: 0.298, accuracy: 0.711\n",
      "Epoch [50] (took 46.222) training loss: 0.292, accuracy: 0.665, validation loss: 0.256, accuracy: 0.689\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "# to do: put up accuracy / loss\n",
    "\n",
    "if train_network:\n",
    "    bnn_pynq_model = cnv_manual(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS)\n",
    "    train_model()\n",
    "    #bo.export_finn_onnx(bnn_pynq_model, INPUT_SPECIFICATIONS, build_dir + file_name + \".onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bnn_pynq_model.state_dict(), build_dir + model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.03838262 0.02923235 0.93238502] prediction 2 target 2.0\n",
      "[0.47808913 0.49705531 0.02485556] prediction 1 target 1.0\n",
      "[0.58396455 0.32579825 0.0902372 ] prediction 0 target 0.0\n"
     ]
    }
   ],
   "source": [
    "# show statistics\n",
    "num_inputs_train_stats = 0\n",
    "num_inputs_test_stats = 0\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def get_prediction(output_tensor):\n",
    "    prediction_list = softmax(output_tensor.tolist())\n",
    "    max_val = -1\n",
    "    prediction = 0\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] > max_val:\n",
    "            max_val = prediction_list[i]\n",
    "            prediction = i\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def test_on_everything(print_output):\n",
    "\n",
    "    list_input_train_total = [0, 0, 0]\n",
    "    list_input_train_correct_total = [0, 0, 0]\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        (input, target) = data\n",
    "        input = input.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        output = bnn_pynq_model(input)\n",
    "        #print(target, output)\n",
    "\n",
    "        for i in range(len(output)):\n",
    "            if print_output:\n",
    "                print(softmax(output[i].tolist()), \"prediction\", get_prediction(output[i]), \"target\", target[i].tolist())\n",
    "\n",
    "            list_input_train_total[int(target[i].tolist())] += 1\n",
    "\n",
    "            if int(target[i].tolist()) == get_prediction(output[i]):\n",
    "                list_input_train_correct_total[int(target[i].tolist())] += 1\n",
    "\n",
    "    for i, data in enumerate(test_loader):\n",
    "        (input, target) = data\n",
    "        input = input.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        output = bnn_pynq_model(input)\n",
    "\n",
    "        for i in range(len(output)):\n",
    "            if print_output:\n",
    "                print(softmax(output[i].tolist()), \"prediction\", get_prediction(output[i]), \"target\", target[i].tolist())\n",
    "\n",
    "            list_input_train_total[int(target[i].tolist())] += 1\n",
    "\n",
    "            if int(target[i].tolist()) == get_prediction(output[i]):\n",
    "                list_input_train_correct_total[int(target[i].tolist())] += 1    \n",
    "\n",
    "    print(\"number of inputs of each class\", list_input_train_total)\n",
    "    print(\"correctly predicted\", list_input_train_correct_total)\n",
    "\n",
    "    running_total = 0\n",
    "    running_total_correct = 0\n",
    "    for i in range(len(list_input_train_total)):\n",
    "        print(\"correctly predicted class %d at %.3f\" % ((i + 1), (list_input_train_correct_total[i] * 100 / list_input_train_total[i])))\n",
    "        running_total += list_input_train_total[i]\n",
    "        running_total_correct += list_input_train_correct_total[i]\n",
    "    \n",
    "    print(\"accuracy at %.3f\" % ((running_total_correct * 100 / running_total)))\n",
    "    \n",
    "test_on_everything(print_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the model weights\n",
    "for param in bnn_pynq_model.parameters():\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Software Comparison\n",
    "\n",
    "We will compare the performance between a normal CNN in PyTorch vs a quantized CNN in Brevitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "CNV_OUT_CH_POOL = [(0, 16, False), (1, 32, True), (2, 64, False), (3, 128, False)]\n",
    "KERNEL_SIZE = 2 # default 3\n",
    "NUM_CONV_LAYERS = 3\n",
    "\n",
    "# Intermediate QuantLinear configuration\n",
    "INTERMEDIATE_FC_PER_OUT_CH_SCALING = True\n",
    "INTERMEDIATE_FC_FEATURES = [(128, 256)] # (IN_CH, OUT_CH)\n",
    "\n",
    "# Last QuantLinear configuration\n",
    "LAST_FC_IN_FEATURES = 256\n",
    "LAST_FC_PER_OUT_CH_SCALING = False\n",
    "\n",
    "# MaxPool2d configuration\n",
    "POOL_SIZE = 2\n",
    "\n",
    "# Network specific bit-widths and IO\n",
    "WEIGHT_BIT_WIDTH = 1\n",
    "ACT_BIT_WIDTH = 1\n",
    "IN_BIT_WIDTH = 8\n",
    "NUM_CLASSES = 6\n",
    "IN_CHANNELS = 1\n",
    "\n",
    "# curently only square inputs are verified\n",
    "# only use inputs that are multiples of 4 (could be that the inputs are given as uint8 so 4x8 = 32bit)\n",
    "INPUT_SPECIFICATIONS = (1, 1, 8, 8) # batch size, channels, length, width\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, KERNEL_SIZE)\n",
    "        self.conv1_bn = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, KERNEL_SIZE)\n",
    "        self.conv2_bn = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, KERNEL_SIZE)\n",
    "        self.conv3_bn = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 128, KERNEL_SIZE)\n",
    "        self.conv4_bn = nn.BatchNorm2d(128)\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv1_bn(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2_bn(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv3_bn(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv4_bn(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = (self.fc1(x))\n",
    "        x = (self.fc2(x))\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "criterion_software = nn.CrossEntropyLoss()\n",
    "optimizer_software = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# keep track of training / testing metrics\n",
    "num_inputs_training = len(train_loader) * batch_size\n",
    "num_inputs_testing = len(test_loader) * batch_size\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # keep track of training / testing metrics\n",
    "    running_loss_training = 0.0\n",
    "    running_loss_testing = 0.0\n",
    "    num_inputs_training_correct = 0\n",
    "    num_inputs_testing_correct = 0\n",
    "    start_data_loading = time.time()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer_software.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        output = net(inputs)\n",
    "        loss = criterion_software(output, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update loss and accuracy\n",
    "        running_loss_training += loss.item()\n",
    "        num_inputs_training_correct += compare_output(output, target.long())\n",
    "\n",
    "    # validate\n",
    "    for j, data in enumerate(test_loader):\n",
    "        (input, target) = data\n",
    "\n",
    "        #input = input.to(device, non_blocking=True)\n",
    "        #target = target.to(device, non_blocking=True)\n",
    "\n",
    "        output = bnn_pynq_model(input)\n",
    "        loss = criterion(output, target.long())\n",
    "\n",
    "        # update loss and accuracy\n",
    "        running_loss_testing += loss.item()\n",
    "        num_inputs_testing_correct += compare_output(output, target.long())\n",
    "\n",
    "    print('Epoch [%d] (took %.3f) training loss: %.3f, accuracy: %.3f, validation loss: %.3f, accuracy: %.3f' \n",
    "          % (epoch + 1, time.time() - start_data_loading, running_loss_training / num_inputs_training, num_inputs_training_correct / num_inputs_training, \n",
    "            running_loss_testing / num_inputs_testing, num_inputs_testing_correct / num_inputs_testing))\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on the testing data\n",
    "total_time = datetime.timedelta(0)\n",
    "\n",
    "def get_predict_output_class(output):\n",
    "    max_prob = -999\n",
    "    max_prob_class = 0\n",
    "\n",
    "    # get largest probability class\n",
    "    for i in range(len(output)):\n",
    "        if output[i] > max_prob:\n",
    "            max_prob = output[i]\n",
    "            max_prob_class = i\n",
    "            \n",
    "    return max_prob_class\n",
    "\n",
    "for i, data in enumerate(test_loader):\n",
    "    (input, target) = data\n",
    "    time = datetime.datetime.now()\n",
    "    output = bnn_pynq_model(input)\n",
    "    time = datetime.datetime.now() - time\n",
    "    total_time = total_time + time\n",
    "    print(\"inference took\", time)\n",
    "    \n",
    "    for j in range(len(output)):\n",
    "        if get_predict_output_class(output[j]) == target[j]:\n",
    "            is_correct = True\n",
    "        else:\n",
    "            is_correct = False\n",
    "        print(\"output %s, target %s, is correct: %s\" % (get_predict_output_class(output[j]), target[j], is_correct))\n",
    "    \n",
    "print(\"took \", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Preparing the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the onnx model made by brevitas\n",
    "model = ModelWrapper(build_dir + file_name + \".onnx\")\n",
    "showInNetron(build_dir + file_name + \".onnx\")\n",
    "\n",
    "# use http://localhost:8081/ since this is on Ubuntu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some transformations and then show on netron\n",
    "\n",
    "model = model.transform(DoubleToSingleFloat())\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "#model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(build_dir + file_name + \"_tidy.onnx\")\n",
    "\n",
    "showInNetron(build_dir + file_name + \"_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is exported, let's have a look at its layer structure with Netron. Remember that the visualization below is interactive, you can click on the individual nodes and view the layer attributes, trained weights and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the network is composed of a repeating convolution-convolution-maxpool layer pattern to extract features using 3x3 convolution kernels (with weights binarized) and `Sign` activations, followed by fully connected layers acting as the classifier. Also notice the initial `MultiThreshold` layer at the beginning of the network, which is quantizing float inputs to 8-bit ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How FINN Implements Convolutions: Lowering and Streamlining\n",
    "\n",
    "In FINN, we implement convolutions with the *lowering* approach: we convert them to matrix-matrix multiply operations, where one of the matrices is generated by sliding a window over the input image. You can read more about the sliding window operator and how convolution lowering works [in this notebook](https://github.com/maltanar/qnn-inference-examples/blob/master/3-convolutional-binarized-gtsrb.ipynb). The streaming dataflow architecture we will end up with is going to look something like this figure from the [FINN-R paper](https://arxiv.org/abs/1809.04570):\n",
    "\n",
    "![](cnv-mp-fc.png)\n",
    "\n",
    "Note how the convolution layer looks very similar to the fully connected one in terms of the matrix-vector-threshold unit (MVTU), but now the MVTU is preceded by a sliding window unit that produces the matrix from the input image. All of these building blocks, including the `MaxPool` layer you see in this figure, exist as templated Vivado HLS C++ functions in [finn-hlslib](https://github.com/Xilinx/finn-hlslib).\n",
    "\n",
    "\n",
    "To target this kind of hardware architecture with our network we'll apply a convolution lowering transformation, in addition to streamlining. You may recall the *streamlining transformation* that we applied to the TFC-w1a1 network, which is a series of mathematical simplifications that allow us to get rid of floating point scaling operations by implementing few-bit activations as thresholding operations. **The current implementation of streamlining is highly network-specific and may not work for your network if its topology is very different than the example network here. We hope to rectify this in future releases.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(build_dir + file_name + \"_tidy.onnx\")\n",
    "\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "model = model.transform(Streamline())\n",
    "\n",
    "model.save(build_dir + file_name + \"_streamlined.onnx\")\n",
    "showInNetron(build_dir + file_name + \"_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't go into too much detail about what happens in each transformation and why they are called in the particular order they are (feel free to visualize the intermediate steps using Netron yourself if you are curious) but here is a brief summmmary:\n",
    "\n",
    "* `Streamline` moves floating point scaling and addition operations closer to the input of the nearest thresholding activation and absorbs them into thresholds\n",
    "* `LowerConvsToMatMul` converts ONNX `Conv` nodes into sequences of `Im2Col, MatMul` nodes as discussed above. `Im2Col` is a custom FINN ONNX high-level node type that implements the sliding window operator.\n",
    "* `MakeMaxPoolNHWC` and `AbsorbTransposeIntoMultiThreshold` convert the *data layout* of the network into the NHWC data layout that finn-hlslib primitives use. NCHW means the tensor dimensions are ordered as `(N : batch, H : height, W : width, C : channels)` (assuming 2D images). The ONNX standard ops normally use the NCHW layout, but the ONNX intermediate representation itself does not dictate any data layout.\n",
    "* You may recall `ConvertBipolarMatMulToXnorPopcount` from the TFC-w1a1 example, which is needed to implement bipolar-by-bipolar (w1a1) networks correctly using finn-hlslib.\n",
    "\n",
    "Let's visualize the streamlined and lowered network with Netron. Observe how all the `Conv` nodes have turned into pairs of `Im2Col, MatMul` nodes, and many nodes including `BatchNorm, Mul, Add` nodes have disappeared and replaced with `MultiThreshold` nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Partitioning, Conversion to HLS Layers and Folding\n",
    "\n",
    "The next steps will be (again) very similar to what we did for the TFC-w1a1 network. We'll first convert the layers that we can put into the FPGA into their HLS equivalents and separate them out into a *dataflow partition*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.move_reshape import MoveReshape\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "\n",
    "# choose the memory mode for the MVTU units, decoupled or const\n",
    "mem_mode = \"decoupled\"\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \"_streamlined.onnx\")\n",
    "model = model.transform(to_hls.InferBinaryStreamingFCLayer(mem_mode))\n",
    "model = model.transform(to_hls.InferQuantizedStreamingFCLayer(mem_mode))\n",
    "model = model.transform(to_hls.InferConvInpGen())\n",
    "model = model.transform(to_hls.InferStreamingMaxPool())\n",
    "\n",
    "# get rid of Reshape(-1, 1) operation between hlslib nodes\n",
    "model = model.transform(MoveReshape())\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(build_dir + file_name + \"_dataflow_parent.onnx\")\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "\n",
    "# save the dataflow partition with a different name for easier access\n",
    "dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "dataflow_model.save(build_dir + file_name + \"_dataflow_model.onnx\")\n",
    "showInNetron(build_dir + file_name + \"_dataflow_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if the maxpoolnhwc can be made into streamingmaxpool_batch nodes\n",
    "# basically, the input dimension into the maxpool node must be divisible by the maxpool kernel size\n",
    "# if this is not fulfilled, then the dataflowmodel will be broken up into several pieces\n",
    "# if done correctly, the dataflowmodel should be one linear model\n",
    "# this is commented out by default\n",
    "\n",
    "class InferStreamingMaxPool_test(Transformation):\n",
    "    \"\"\"Convert MaxPoolNHWC layers to StreamingMaxPool layers.\"\"\"\n",
    "\n",
    "    def apply(self, model):\n",
    "        graph = model.graph\n",
    "        node_ind = 0\n",
    "        graph_modified = False\n",
    "        for n in graph.node:\n",
    "            node_ind += 1\n",
    "            if n.op_type == \"MaxPoolNHWC\":\n",
    "                mp_input = n.input[0]\n",
    "                mp_output = n.output[0]\n",
    "                mp_in_shape = model.get_tensor_shape(mp_input)\n",
    "                # mp_out_shape = model.get_tensor_shape(mp_output)\n",
    "                dt = model.get_tensor_datatype(mp_input)\n",
    "                mp_inst = getCustomOp(n)\n",
    "                # stride = mp_inst.get_nodeattr(\"strides\")[0]\n",
    "                k = mp_inst.get_nodeattr(\"kernel_shape\")[0]\n",
    "                # pad = mp_inst.get_nodeattr(\"pads\")[0]\n",
    "                ifm_ch = mp_in_shape[-1]\n",
    "                ifm_dim = mp_in_shape[1]\n",
    "                # ofm_dim = mp_out_shape[1]\n",
    "                print(ifm_dim)\n",
    "                print(k)\n",
    "                if ifm_dim % k == 0:\n",
    "                    print(\"setting\")\n",
    "                    # create equivalent StreamingMaxPool_Batch node\n",
    "                    # TODO support non-k strides\n",
    "                    new_node = helper.make_node(\n",
    "                        \"StreamingMaxPool_Batch\",\n",
    "                        [mp_input],\n",
    "                        [mp_output],\n",
    "                        domain=\"finn\",\n",
    "                        backend=\"fpgadataflow\",\n",
    "                        PoolDim=k,\n",
    "                        NumChannels=ifm_ch,\n",
    "                        ImgDim=ifm_dim,\n",
    "                        dataType=dt.name,\n",
    "                    )\n",
    "                    graph.node.insert(node_ind, new_node)\n",
    "                    # remove old nodes\n",
    "                    graph.node.remove(n)\n",
    "                    graph_modified = True\n",
    "        if graph_modified:\n",
    "            model = model.transform(InferShapes())\n",
    "            model = model.transform(InferDataTypes())\n",
    "        return (model, graph_modified)\n",
    "\n",
    "# choose the memory mode for the MVTU units, decoupled or const\n",
    "#mem_mode = \"decoupled\"\n",
    "\n",
    "#model = ModelWrapper(build_dir + file_name + \"_streamlined.onnx\")\n",
    "#model = model.transform(to_hls.InferBinaryStreamingFCLayer(mem_mode))\n",
    "#model = model.transform(to_hls.InferQuantizedStreamingFCLayer(mem_mode))\n",
    "#model = model.transform(to_hls.InferConvInpGen())\n",
    "#model = model.transform(InferStreamingMaxPool_test())\n",
    "#model.save(build_dir + file_name + \"_dataflow_model.onnx\")\n",
    "#showInNetron(build_dir + file_name + \"_dataflow_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the additional `MoveReshape` transformation that was not used for TFC-w1a1. In the last Netron visualization you may have noticed a `Reshape` operation towards the end of the network where the convolutional part of the network ends and the fully-connected layers started. That `Reshape` is essentialy a tensor flattening operation, which we can remove for the purposes of hardware implementation. We can examine the contents of the dataflow partition with Netron, and observe the `ConvolutionInputGenerator`, `StreamingFCLayer_Batch` and `StreamingMaxPool_Batch` nodes that implement the sliding window, matrix multiply and maxpool operations in hlslib. *Note that the StreamingFCLayer instances following the ConvolutionInputGenerator nodes are really implementing the convolutions, despite the name. The final three StreamingFCLayer instances implement actual FC layers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to set the *folding factors* for certain layers to adjust the performance of our accelerator, similar to the TFC-w1a1 example. We'll also set the desired FIFO depths around those layers, which are important to achieve full throughput in the accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_tlastmarker import InsertTLastMarker\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \"_dataflow_model.onnx\")\n",
    "fc_layers = model.get_nodes_by_op_type(\"StreamingFCLayer_Batch\")\n",
    "# each tuple is (PE, SIMD, in_fifo_depth) for a layer\n",
    "folding = [\n",
    "    (16, 1, 128),\n",
    "    (32, 8, 128),\n",
    "    (32, 8, 128),\n",
    "    (16, 2, 128),\n",
    "    (16, 8, 128),\n",
    "    (3, 8, 81)\n",
    "]\n",
    "\n",
    "for fcl, (pe, simd, ififodepth) in zip(fc_layers, folding):\n",
    "    fcl_inst = getCustomOp(fcl)\n",
    "    fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    fcl_inst.set_nodeattr(\"inFIFODepth\", ififodepth)\n",
    "    print(fcl)\n",
    "\n",
    "# use same SIMD values for the sliding window operators\n",
    "swg_layers = model.get_nodes_by_op_type(\"ConvolutionInputGenerator\")\n",
    "for i in range(len(swg_layers)):\n",
    "    swg_inst = getCustomOp(swg_layers[i])\n",
    "    simd = folding[i][1]\n",
    "    swg_inst.set_nodeattr(\"SIMD\", simd)\n",
    "\n",
    "# save intermediate so that we can reference in netron and debug if folding factors are not correct\n",
    "model.save(build_dir + file_name + \"_folded_intermediate.onnx\")\n",
    "showInNetron(build_dir + file_name + \"_folded_intermediate.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the transformation\n",
    "model = ModelWrapper(build_dir + file_name + \"_folded_intermediate.onnx\")\n",
    "\n",
    "model = model.transform(InsertDWC())\n",
    "model = model.transform(InsertFIFO())\n",
    "model = model.transform(InsertTLastMarker())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "\n",
    "model.save(build_dir + file_name + \"_folded.onnx\")\n",
    "showInNetron(build_dir + file_name + \"_folded.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we visualize in Netron to observe the `StreamingDataWidthConverter` and `StreamingFIFO` nodes that have been inserted into graph, as well as the folding factors in the `PE` and `SIMD` attributes of each `StreamingFCLayer_Batch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network is now ready and we can start with the hardware generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hardware Generation\n",
    "\n",
    "From this point onward, the steps we have to follow do not depend on the particular network and will be exactly the same as the TFC-w1a1 example. We first proceed with HLS synthesis, **which may take 10-20 minutes depending on your host computer and your RAM cause of WSL**.\n",
    "\n",
    "**Note: WSL takes 10GB of RAM to perform synthesis, else it crashes halfway.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.prepare_ip import PrepareIP\n",
    "from finn.transformation.fpgadataflow.hlssynth_ip import HLSSynthIP\n",
    "from finn.util.basic import pynq_part_map\n",
    "time = datetime.datetime.now()\n",
    "\n",
    "test_pynq_board = \"Ultra96\"\n",
    "test_fpga_part = pynq_part_map[test_pynq_board]\n",
    "target_clk_ns = 10\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \"_folded.onnx\")\n",
    "model = model.transform(PrepareIP(test_fpga_part, target_clk_ns))\n",
    "model = model.transform(HLSSynthIP())\n",
    "model.save(build_dir + file_name + \"_ipgen.onnx\")\n",
    "print(\"took\", datetime.datetime.now() - time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the HLS synthesis is complete, we can stitch together the generated IP blocks into a larger IP that is the implementation of our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.replace_verilog_relpaths import (\n",
    "    ReplaceVerilogRelPaths,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.create_stitched_ip import CreateStitchedIP\n",
    "time = datetime.datetime.now()\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \"_ipgen.onnx\")\n",
    "model = model.transform(ReplaceVerilogRelPaths())\n",
    "model = model.transform(CreateStitchedIP(test_fpga_part))\n",
    "model.save(build_dir + file_name + \"_ipstitch.onnx\")\n",
    "print(\"took\", datetime.datetime.now() - time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a PYNQ project that includes the hardware \"shell\" that will support our accelerator, including the data movers, and run Vivado synthesis, **which may take around 30 minutes depending on your host computer.**\n",
    "\n",
    "*If you'd like to watch the progress, you can open the generated project file (printed below) with the Vivado GUI.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.make_pynq_proj import MakePYNQProject\n",
    "from finn.transformation.fpgadataflow.synth_pynq_proj import SynthPYNQProject\n",
    "time = datetime.datetime.now()\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \"_ipstitch.onnx\")\n",
    "model = model.transform(MakePYNQProject(test_pynq_board))\n",
    "vivado_proj = model.get_metadata_prop(\"vivado_pynq_proj\")\n",
    "print(\"Vivado synthesis project is at %s/resizer.xpr\" % vivado_proj)\n",
    "model.save(build_dir + file_name + \"_pynqproj.onnx\")\n",
    "print(\"took\", datetime.datetime.now() - time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.datetime.now()\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \"_pynqproj.onnx\")\n",
    "model = model.transform(SynthPYNQProject())\n",
    "model.save(build_dir + file_name + \"_synth.onnx\")\n",
    "print(\"took\", datetime.datetime.now() - time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deployment and Remote Execution\n",
    "\n",
    "Now that we're done with the hardware generation, we can generate a Python driver for accelerator and copy the necessary files onto our PYNQ board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "from finn.transformation.fpgadataflow.make_deployment import DeployToPYNQ\n",
    "\n",
    "# FINN will use ssh to deploy and run the generated accelerator\n",
    "# please run ultra96_port_forwarding.ipynb before transferring\n",
    "ip = \"localhost\"\n",
    "port = \"3100\"\n",
    "username = \"xilinx\"\n",
    "password = \"xilinx\"\n",
    "target_dir = \"/home/xilinx/finn/cnv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(build_dir + file_name + \"_synth.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy to the remote Ultra96\n",
    "model = model.transform(MakePYNQDriver())\n",
    "model = model.transform(DeployToPYNQ(ip, port, username, password, target_dir))\n",
    "model.save(build_dir + file_name + \"_pynq_deploy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the files are copied over\n",
    "pynq_folder_name = vivado_proj[36:]\n",
    "#print(pynq_folder_name)\n",
    "! sshpass -p {password} ssh {username}@{ip} -p {port} 'ls -l {target_dir}/*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Dataset\n",
    "\n",
    "Load the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random input from the test_loader\n",
    "import random\n",
    "test_input_number = random.randrange(0, len(x_test_list) - 1)\n",
    "\n",
    "test_input = x_test_list[test_input_number]\n",
    "test_input = test_input.astype(np.float32)\n",
    "\n",
    "test_output = y_test_list[test_input_number]\n",
    "\n",
    "print(\"using input\", test_input_number)\n",
    "print(\"input: {}, output: {}\".format(test_input, test_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import specific data from csv\n",
    "df = pd.read_csv('dataset_3classes.csv')\n",
    "\n",
    "df.shape\n",
    "\n",
    "starting_index = 10 * 8\n",
    "\n",
    "test_input = np.array(((((df.iloc[starting_index, 0],df.iloc[starting_index + 1,0],df.iloc[starting_index + 2,0],df.iloc[starting_index + 3,0],df.iloc[starting_index + 4,0],df.iloc[starting_index + 5,0],df.iloc[starting_index + 6,0],df.iloc[starting_index + 7,0]),\n",
    "        (df.iloc[starting_index, 1],df.iloc[starting_index + 1,1],df.iloc[starting_index + 2,1],df.iloc[starting_index + 3,1],df.iloc[starting_index + 4,1],df.iloc[starting_index + 5,1],df.iloc[starting_index + 6,1],df.iloc[starting_index + 7,1]),\n",
    "        (df.iloc[starting_index, 2],df.iloc[starting_index + 1,2],df.iloc[starting_index + 2,2],df.iloc[starting_index + 3,2],df.iloc[starting_index + 4,2],df.iloc[starting_index + 5,2],df.iloc[starting_index + 6,2],df.iloc[starting_index + 7,2]),\n",
    "        (df.iloc[starting_index, 3],df.iloc[starting_index + 1,3],df.iloc[starting_index + 2,3],df.iloc[starting_index + 3,3],df.iloc[starting_index + 4,3],df.iloc[starting_index + 5,3],df.iloc[starting_index + 6,3],df.iloc[starting_index + 7,3]),\n",
    "        (df.iloc[starting_index, 0],df.iloc[starting_index + 1,0],df.iloc[starting_index + 2,0],df.iloc[starting_index + 3,0],df.iloc[starting_index + 4,0],df.iloc[starting_index + 5,0],df.iloc[starting_index + 6,0],df.iloc[starting_index + 7,0]),\n",
    "        (df.iloc[starting_index, 1],df.iloc[starting_index + 1,1],df.iloc[starting_index + 2,1],df.iloc[starting_index + 3,1],df.iloc[starting_index + 4,1],df.iloc[starting_index + 5,1],df.iloc[starting_index + 6,1],df.iloc[starting_index + 7,1]),\n",
    "        (df.iloc[starting_index, 2],df.iloc[starting_index + 1,2],df.iloc[starting_index + 2,2],df.iloc[starting_index + 3,2],df.iloc[starting_index + 4,2],df.iloc[starting_index + 5,2],df.iloc[starting_index + 6,2],df.iloc[starting_index + 7,2]),\n",
    "        (df.iloc[starting_index, 3],df.iloc[starting_index + 1,3],df.iloc[starting_index + 2,3],df.iloc[starting_index + 3,3],df.iloc[starting_index + 4,3],df.iloc[starting_index + 5,3],df.iloc[starting_index + 6,3],df.iloc[starting_index + 7,3])), )))\n",
    "    \n",
    "\n",
    "test_input = test_input.astype(np.float32)\n",
    "\n",
    "test_output = df.iloc[starting_index, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_model = ModelWrapper(build_dir + file_name + \"_dataflow_parent.onnx\")\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "sdp_node.set_nodeattr(\"model\", build_dir + file_name + \"_pynq_deploy.onnx\")\n",
    "parent_model.save(build_dir + file_name + \"_dataflow_parent_with_remote_bitfile_exec.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from finn.core.onnx_exec import execute_onnx\n",
    "iname = parent_model.graph.input[0].name\n",
    "oname = parent_model.graph.output[0].name\n",
    "ishape = parent_model.get_tensor_shape(iname)\n",
    "\n",
    "input_dict = {iname: test_input.reshape(ishape)}\n",
    "\n",
    "print(input_dict)\n",
    "\n",
    "ret = execute_onnx(parent_model, input_dict, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "logits = ret[oname].flatten()\n",
    "prob = softmax(logits)\n",
    "\n",
    "classes = [\"zigzag\", \"rocket\", \"hair\"]\n",
    "\n",
    "print(logits)\n",
    "print(test_output)\n",
    "\n",
    "plt.figure(figsize=(20, 3)) \n",
    "plt.bar(classes, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on test_dataset remotely\n",
    "# warning may take a while\n",
    "iname = parent_model.graph.input[0].name\n",
    "oname = parent_model.graph.output[0].name\n",
    "ishape = parent_model.get_tensor_shape(iname)\n",
    "\n",
    "classes = [\"zigzag\", \"hair\", \"rocket\"]\n",
    "correct = 0\n",
    "num_inputs = 0\n",
    "\n",
    "test_loader_remote = DataLoader(dataset_test, batch_size = 1, shuffle = True, num_workers = 1)\n",
    "\n",
    "for i, data in enumerate(test_loader_remote):\n",
    "    (test_input, target) = data\n",
    "    input_dict = {iname: test_input.reshape(ishape)}\n",
    "\n",
    "    ret = execute_onnx(parent_model, input_dict, True)\n",
    "    \n",
    "    logits = ret[oname].flatten()\n",
    "    prob = softmax(logits)\n",
    "    \n",
    "    output_prob = -1.0\n",
    "    output = 0\n",
    "    j = 0\n",
    "    \n",
    "    for item in logits:\n",
    "        if item > output_prob:\n",
    "            output_prob = item\n",
    "            output = j\n",
    "        j += 1\n",
    "        \n",
    "    # convert to class\n",
    "    if output == 0:\n",
    "        output_class = 'zigzag'\n",
    "    elif output == 1:\n",
    "        output_class = 'hair'\n",
    "    elif output == 2:\n",
    "        output_class = 'rocket'\n",
    "        \n",
    "    if target_class == output_class:\n",
    "        correct += 1\n",
    "    num_inputs += 1\n",
    "    \n",
    "    print(test_input)\n",
    "    print(logits)\n",
    "    print(\"actual: %s, prediction: %s\" % (target_class, output_class))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"successfully predicted %d out of %d\" % (correct, num_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on CIFAR-10 (CNV Example)\n",
    "\n",
    "We only have two more steps to be able to remotely execute the deployed bitfile with some test data from the CIFAR-10 dataset. Let's load up some test data that comes bundled with FINN -- and before you ask, that's supposed to be a cat (CIFAR-10 class number 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources as pk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fn = pk.resource_filename(\"finn\", \"data/cifar10/cifar10-test-data-class3.npz\")\n",
    "x = np.load(fn)[\"arr_0\"].astype(np.float32)\n",
    "x = x / 255\n",
    "plt.imshow(x.reshape(3, 32,32).transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we partitioned our original network into a parent graph that contained the non-synthesizable nodes and a child graph that contained the bulk of the network, which we turned into a bitfile. We'll load up the parent graph, modify the `StreamingDataflowPartition` node so that it points to the deployed ONNX graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point to the PYNQ-deployed model as the StreamingDataflowPartition in the parent\n",
    "parent_model = ModelWrapper(build_dir + file_name + \"_dataflow_parent.onnx\")\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "sdp_node.set_nodeattr(\"model\", build_dir + file_name + \"_pynq_deploy.onnx\")\n",
    "parent_model.save(build_dir + file_name + \"_dataflow_parent_with_remote_bitfile_exec.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can call `execute_onnx` on the parent graph, which will internally call remote execution with the bitfile once the `StreamingDataflowPartition` node is reached, grab the results, then continue executing the last portion of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from finn.core.onnx_exec import execute_onnx\n",
    "iname = parent_model.graph.input[0].name\n",
    "oname = parent_model.graph.output[0].name\n",
    "ishape = parent_model.get_tensor_shape(iname)\n",
    "input_dict = {iname: x.reshape(ishape)}\n",
    "print(input_dict)\n",
    "#ret = execute_onnx(parent_model, input_dict, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll pass the output of the network through a softmax function to interpret it as probabilities, and plot the per-class probabilities as a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "logits = ret[oname].flatten()\n",
    "prob = softmax(logits)\n",
    "\n",
    "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "plt.figure(figsize=(20, 3)) \n",
    "plt.bar(classes, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the network correctly predicts this as a class 3 (\"cat\") with high probability. This concludes our tutorial on how to take a convolutional BNN all the way down to hardware with FINN, and execute it remotely on a PYNQ board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input (8, 8)\n",
    "input = [[1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8]]\n",
    "\n",
    "ibuf_normal = np.array(((((input[0][0], ), (input[0][1], ), (input[0][2], ), (input[0][3], ), (input[0][4], ), (input[0][5], ), (input[0][6], ), (input[0][7], )),\n",
    "                         ((input[1][0], ), (input[1][1], ), (input[1][2], ), (input[1][3], ), (input[1][4], ), (input[1][5], ), (input[1][6], ), (input[1][7], )),\n",
    "                         ((input[2][0], ), (input[2][1], ), (input[2][2], ), (input[2][3], ), (input[2][4], ), (input[2][5], ), (input[2][6], ), (input[2][7], )),\n",
    "                         ((input[3][0], ), (input[3][1], ), (input[3][2], ), (input[3][3], ), (input[3][4], ), (input[3][5], ), (input[3][6], ), (input[3][7], )),\n",
    "                         ((input[4][0], ), (input[4][1], ), (input[4][2], ), (input[4][3], ), (input[4][4], ), (input[4][5], ), (input[4][6], ), (input[4][7], )),\n",
    "                         ((input[5][0], ), (input[5][1], ), (input[5][2], ), (input[5][3], ), (input[5][4], ), (input[5][5], ), (input[5][6], ), (input[5][7], )),\n",
    "                         ((input[6][0], ), (input[6][1], ), (input[6][2], ), (input[6][3], ), (input[6][4], ), (input[6][5], ), (input[6][6], ), (input[6][7], )),\n",
    "                         ((input[7][0], ), (input[7][1], ), (input[7][2], ), (input[7][3], ), (input[7][4], ), (input[7][5], ), (input[7][6], ), (input[7][7], )) ), ))\n",
    "\n",
    "print(ibuf_normal.shape)\n",
    "print(ibuf_normal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
