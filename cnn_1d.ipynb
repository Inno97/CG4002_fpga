{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End FINN Flow for a 1D Convolutional Net (FINN v0.31b)\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "FINN walkthrough for 1D CNN on Ultra96 for use in CG4002 by Daniel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changelog\n",
    "\n",
    "| Date | Changes |\n",
    "|:------:|:---------:|\n",
    "| 28/9/2020 | Added creation of CNN in section 1.1 (training not setup yet) |\n",
    "| 29/9/2020 | Tweaked creation of CNN and added notes for finn compilation. Added basic training and testing, with a simple CNN being implemented |\n",
    "| 5/10/2020 | Show accuracy and loss during training and cleaned up training code |\n",
    "| 19/10/2020 | Added Brevitas Conv1D layers and create new 1D CNN |\n",
    "| 20/10/2020 | Add extra 1D CNN using 16 inputs instead of 8, cleaned up notebook with end-to-end flow. |\n",
    "| 28/10/2020 | Finish up 24 input 2D CNN with 2 channels |\n",
    "| 1/11/2020 | Clean up and update documententation |\n",
    "\n",
    "## To Do\n",
    "| |\n",
    "|:------|\n",
    "| Exporting to ONNX is not working for 1D CNN | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-end Flow for this Notebook\n",
    "\n",
    "This notebook provides a 1D CNN, which allows for time-series feature extraction via the convolution layers.\n",
    "\n",
    "The input is a n-channel 1D input, where the channels represent an input value (pitch / roll), while the input tensor itself will contain the different input values over time. They are passed into 1D convolution layers.\n",
    "\n",
    "Afterwards, the 1D input is then inserted into fully connected / feed forward layers.\n",
    "\n",
    "Due to FINN not being able to synthesize 1D convolution layers currently, the convolution layers are not hardware accelerated, but the fully connected layers will be.\n",
    "\n",
    "In the notebook, we will create and train 1D CNN first, then save it locally. Afterwards, we make 2 models for the convolution and fully connected layers, and load the trained weights from the original 1D CNN. The fully connected layers will be passed through the FINN workflow.\n",
    "\n",
    "Appropriate driver code is provided in the Github repo, for running the entire model.\n",
    "\n",
    "This notebook will give us a .pt file (state dict for the software layers) and a bitstream for the hardware layers.\n",
    "\n",
    "For FINN, checkpoints will be saved in the form of ONNX files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](finn-design-flow-example.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: toposort==1.5 from file:///workspace/finn/notebooks/CNN/toposort-1.5-py2.py3-none-any.whl in /workspace/.local/lib/python3.6/site-packages (1.5)\n",
      "toposort                      1.5      \n",
      "Requirement already satisfied: dependencies in /workspace/.local/lib/python3.6/site-packages (4.1.0)\n",
      "dependencies                  4.1.0    \n",
      "Requirement already satisfied: pandas in /workspace/.local/lib/python3.6/site-packages (1.1.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.6/site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "pandas                        1.1.4    \n"
     ]
    }
   ],
   "source": [
    "# run this to install libaries, and restart the kernel for it to take effect\n",
    "# this needs to be done if running on FINN docker\n",
    "\n",
    "# download the whl file on this local machine\n",
    "\n",
    "!pip install /workspace/finn/notebooks/CNN/toposort-1.5-py2.py3-none-any.whl --user\n",
    "!pip list | grep \"toposort\"\n",
    "\n",
    "!pip install dependencies --user\n",
    "!pip list | grep \"dependencies\"\n",
    "\n",
    "!pip install pandas --user\n",
    "!pip list | grep \"pandas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports are put here\n",
    "from finn.util.basic import make_build_dir\n",
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "import datetime # for tracking time for each code block\n",
    "import time\n",
    "\n",
    "# 1. Brevitas Export, FINN Import and Tidy-Up\n",
    "\n",
    "# 1.1 Network Setup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "from dependencies import Injector, value\n",
    "\n",
    "from brevitas.core.bit_width import BitWidthImplType\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.core.scaling import ScalingImplType\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module, ModuleList, BatchNorm2d, MaxPool2d, BatchNorm1d, MaxPool1d, Sequential\n",
    "\n",
    "from brevitas.nn import QuantConv2d, QuantLinear\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "\n",
    "# 1.2 Training the Network\n",
    "\n",
    "# imports\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 1.3 Split Model into Conv1D and FC Layers\n",
    "import torch.nn.functional as func\n",
    "\n",
    "# 1.4 Export and Tidy-Up\n",
    "\n",
    "import onnx\n",
    "import brevitas.onnx as bo\n",
    "\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.double_to_single_float import DoubleToSingleFloat\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames\n",
    "\n",
    "# 2. How FINN Implements Convolutions: Lowering and Streamlining\n",
    "\n",
    "from finn.transformation.streamline import Streamline\n",
    "from finn.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from finn.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC\n",
    "\n",
    "# 3. Partitioning, Conversion to HLS Layers and Folding\n",
    "\n",
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (CreateDataflowPartition,)\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "\n",
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_tlastmarker import InsertTLastMarker\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "\n",
    "from finn.transformation.fpgadataflow.prepare_ip import PrepareIP\n",
    "from finn.transformation.fpgadataflow.hlssynth_ip import HLSSynthIP\n",
    "from finn.util.basic import pynq_part_map\n",
    "\n",
    "from finn.transformation.fpgadataflow.replace_verilog_relpaths import (ReplaceVerilogRelPaths,)\n",
    "from finn.transformation.fpgadataflow.create_stitched_ip import CreateStitchedIP\n",
    "\n",
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_tlastmarker import InsertTLastMarker\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "\n",
    "import toposort\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "\n",
    "# 4. Hardware Generation\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "from finn.transformation.fpgadataflow.make_deployment import DeployToPYNQ\n",
    "\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "from finn.transformation import Transformation\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.util.basic import get_by_name, make_build_dir\n",
    "from finn.util.basic import get_num_default_workers\n",
    "from finn.util.basic import pynq_part_map\n",
    "\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (CreateDataflowPartition,)\n",
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "from finn.transformation.fpgadataflow.prepare_ip import PrepareIP\n",
    "from finn.transformation.fpgadataflow.hlssynth_ip import HLSSynthIP\n",
    "from finn.transformation.fpgadataflow.create_stitched_ip import CreateStitchedIP\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames\n",
    "from shutil import copy\n",
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "\n",
    "# 5. Deployment and Remote Execution\n",
    "\n",
    "import pkg_resources as pk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "\n",
    "import numpy as np\n",
    "from finn.core.onnx_exec import execute_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general build parameters\n",
    "\n",
    "# path names\n",
    "build_dir = \"/workspace/finn/notebooks/CNN\"\n",
    "file_name = \"/cnn_1d_3_classes_24\"\n",
    "model_path = \"/cnv_1d_2_24_all_3_fixed.pt\"\n",
    "\n",
    "# creates a new model\n",
    "create_new_model = True\n",
    "\n",
    "# train the network from scratch\n",
    "train_network = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing, Training and Exporting the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Network Setup\n",
    "\n",
    "Declare the network below, and then create the CNN.\n",
    "\n",
    "Note: You will probably need to trail and error for this part.\n",
    "\n",
    "Do not touch the actual CNN model class, as it is verified to be able to run through the entire workflow.\n",
    "\n",
    "Due to FINN being very restrictive at the moment, not all configurations can result in a proper bitstream being generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare features of the 1D CNN\n",
    "\n",
    "# notes on CNN structure:\n",
    "# maxpool layers must have their input size be divisible by kernel size, this is mandatory\n",
    "# Conv1D is non-synthesizeable under FINN 0.3b\n",
    "\n",
    "\n",
    "# input configurations\n",
    "NUM_CLASSES = 3 # number of output classes / dance moves\n",
    "IN_CHANNELS = 2 # number of input channels / types of input values (pitch / roll / ...)\n",
    "\n",
    "# only use inputs that are multiples of 4 (batch size, channels, length)\n",
    "# channels - number of input channels, length - size of 1D input (do not touch batchsize)\n",
    "INPUT_SPECIFICATIONS = (1, 2, 24)\n",
    "\n",
    "# output classes\n",
    "classes = [\"zigzag\", \"rocket\", \"hair\"]\n",
    "\n",
    "# Convolution / QuantConv1d configuration \n",
    "\n",
    "# the number of convolution layers (i, OUT_CH, is_maxpool_enabled)\n",
    "# i - index of convolution layers OUT_CH - number of output channels, is_maxpool_enabled - inserts maxpool after layer\n",
    "CNV_OUT_CH_POOL = [(0, 32, False), (1, 64, True), (2, 128, False), (3, 256, True)]\n",
    "\n",
    "KERNEL_SIZE = 4 # default 3\n",
    "\n",
    "NUM_CONV_LAYERS = len(CNV_OUT_CH_POOL)\n",
    "\n",
    "# Intermediate QuantLinear configuration\n",
    "\n",
    "# Fully connected / QuantLinear configuration\n",
    "\n",
    "# the number of intermediate fully connected layers (IN_CH, OUT_CH)\n",
    "# IN_CH - input channels, OUT_CH - output channels\n",
    "INTERMEDIATE_FC_FEATURES = [(256, 128)] \n",
    "\n",
    "# performs output scaling in each FC layer\n",
    "INTERMEDIATE_FC_PER_OUT_CH_SCALING = True\n",
    "\n",
    "# last fully connected layer, this is separate as we do not want to scale the output\n",
    "LAST_FC_IN_FEATURES = 128\n",
    "LAST_FC_PER_OUT_CH_SCALING = False\n",
    "\n",
    "# fully connected dropout layers\n",
    "IN_DROPOUT = 0.2\n",
    "HIDDEN_DROPOUT = 0.2\n",
    "\n",
    "# Network specific bit-widths and IO\n",
    "# recommended not to touch at the moment\n",
    "WEIGHT_BIT_WIDTH = 1\n",
    "ACT_BIT_WIDTH = 1\n",
    "IN_BIT_WIDTH = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code contains the 1D Convolution layers and some miscellaneous library code in Brevitas, they have to be manually declared here as the commit of Brevitas used in FINN v0.3b does not contain 1D convolution layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuantConv1D as per https://github.com/Xilinx/brevitas/blob/quant_quartznet_4b-r0/brevitas/nn/quant_conv1d.py\n",
    "from enum import auto\n",
    "from typing import Union, Optional, Tuple\n",
    "import re\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import docrep\n",
    "from torch.nn import Conv1d, Module\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import conv1d\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from brevitas.core.bit_width import BitWidthParameter, BitWidthConst, BitWidthImplType\n",
    "from brevitas.core.quant import QuantType, IdentityQuant\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.core.scaling import ScalingImplType, SCALING_SCALAR_SHAPE\n",
    "from brevitas.core.stats import StatsInputViewShapeImpl, StatsOp\n",
    "from brevitas.function.ops import max_uint, ceil_ste\n",
    "#from brevitas.function.ops_ste import ceil_ste\n",
    "from brevitas.proxy.parameter_quant import WeightQuantProxy, BiasQuantProxy, WeightReg\n",
    "from brevitas.utils.python_utils import AutoName\n",
    "from brevitas.nn.quant_layer import QuantLayer, SCALING_MIN_VAL\n",
    "from brevitas.config import docstrings\n",
    "__all__ = ['QuantConv1d']\n",
    "\n",
    "\n",
    "class PaddingType(AutoName):\n",
    "    STANDARD = auto()\n",
    "    SAME = auto()\n",
    "\n",
    "\n",
    "@docstrings.dedent\n",
    "class QuantConv1d(QuantLayer, Conv1d):\n",
    "    \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        %(weight_quant_proxy.parameters_with_prefix)s\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: Union[int, Tuple[int]],\n",
    "                 stride: Union[int, Tuple[int]] = 1,\n",
    "                 padding: Union[int, Tuple[int]] = 0,\n",
    "                 padding_type: PaddingType = PaddingType.STANDARD,\n",
    "                 dilation: Union[int, Tuple[int]] = 1,\n",
    "                 groups: int = 1,\n",
    "                 bias: bool = True,\n",
    "                 bias_quant_type: QuantType = QuantType.FP,\n",
    "                 bias_narrow_range: bool = False,\n",
    "                 bias_bit_width: int = None,\n",
    "                 weight_quant_override: WeightQuantProxy = None,\n",
    "                 weight_quant_type: QuantType = QuantType.FP,\n",
    "                 weight_narrow_range: bool = False,\n",
    "                 weight_scaling_override: Optional[Module] = None,\n",
    "                 weight_bit_width_impl_override: Union[BitWidthParameter, BitWidthConst] = None,\n",
    "                 weight_bit_width_impl_type: BitWidthImplType = BitWidthImplType.CONST,\n",
    "                 weight_restrict_bit_width_type: RestrictValueType = RestrictValueType.INT,\n",
    "                 weight_bit_width: int = 32,\n",
    "                 weight_min_overall_bit_width: Optional[int] = 2,\n",
    "                 weight_max_overall_bit_width: Optional[int] = None,\n",
    "                 weight_scaling_impl_type: ScalingImplType = ScalingImplType.STATS,\n",
    "                 weight_scaling_const: Optional[float] = None,\n",
    "                 weight_scaling_stats_op: StatsOp = StatsOp.MAX,\n",
    "                 weight_scaling_per_output_channel: bool = False,\n",
    "                 weight_ternary_threshold: float = 0.5,\n",
    "                 weight_restrict_scaling_type: RestrictValueType = RestrictValueType.LOG_FP,\n",
    "                 weight_scaling_stats_sigma: float = 3.0,\n",
    "                 weight_scaling_min_val: float = SCALING_MIN_VAL,\n",
    "                 weight_override_pretrained_bit_width: bool = False,\n",
    "                 compute_output_scale: bool = False,\n",
    "                 compute_output_bit_width: bool = False,\n",
    "                 return_quant_tensor: bool = False) -> None:\n",
    "        QuantLayer.__init__(self,\n",
    "                            compute_output_scale=compute_output_scale,\n",
    "                            compute_output_bit_width=compute_output_bit_width,\n",
    "                            return_quant_tensor=return_quant_tensor)\n",
    "        Conv1d.__init__(self,\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=padding,\n",
    "                        dilation=dilation,\n",
    "                        groups=groups,\n",
    "                        bias=bias)\n",
    "        if weight_quant_type == QuantType.FP and compute_output_bit_width:\n",
    "            raise Exception(\"Computing output bit width requires enabling quantization\")\n",
    "        if bias_quant_type != QuantType.FP and not (compute_output_scale and compute_output_bit_width):\n",
    "            raise Exception(\"Quantizing bias requires to compute output scale and output bit width\")\n",
    "\n",
    "        self.per_elem_ops = 2 * self.kernel_size[0] * (in_channels // groups)\n",
    "        self.padding_type = padding_type\n",
    "        self.weight_reg = WeightReg()\n",
    "\n",
    "        if weight_quant_override is not None:\n",
    "            self.weight_quant = weight_quant_override\n",
    "            self.weight_quant.add_tracked_parameter(self.weight)\n",
    "        else:\n",
    "            weight_scaling_stats_input_concat_dim = 1\n",
    "            if weight_scaling_per_output_channel:\n",
    "                weight_stats_input_view_shape_impl = StatsInputViewShapeImpl.OVER_OUTPUT_CHANNELS\n",
    "                weight_scaling_shape = self.per_output_channel_broadcastable_shape\n",
    "                weight_scaling_stats_reduce_dim = 1\n",
    "            else:\n",
    "                weight_stats_input_view_shape_impl = StatsInputViewShapeImpl.OVER_TENSOR\n",
    "                weight_scaling_shape = SCALING_SCALAR_SHAPE\n",
    "                weight_scaling_stats_reduce_dim = None\n",
    "\n",
    "            if weight_scaling_stats_op == StatsOp.MAX_AVE:\n",
    "                weight_stats_input_view_shape_impl = StatsInputViewShapeImpl.OVER_OUTPUT_CHANNELS\n",
    "                weight_scaling_stats_reduce_dim = 1\n",
    "\n",
    "            self.weight_quant = WeightQuantProxy(bit_width=weight_bit_width,\n",
    "                                                 quant_type=weight_quant_type,\n",
    "                                                 narrow_range=weight_narrow_range,\n",
    "                                                 scaling_override=weight_scaling_override,\n",
    "                                                 restrict_scaling_type=weight_restrict_scaling_type,\n",
    "                                                 scaling_const=weight_scaling_const,\n",
    "                                                 scaling_stats_op=weight_scaling_stats_op,\n",
    "                                                 scaling_impl_type=weight_scaling_impl_type,\n",
    "                                                 scaling_stats_reduce_dim=weight_scaling_stats_reduce_dim,\n",
    "                                                 scaling_shape=weight_scaling_shape,\n",
    "                                                 bit_width_impl_type=weight_bit_width_impl_type,\n",
    "                                                 bit_width_impl_override=weight_bit_width_impl_override,\n",
    "                                                 restrict_bit_width_type=weight_restrict_bit_width_type,\n",
    "                                                 min_overall_bit_width=weight_min_overall_bit_width,\n",
    "                                                 max_overall_bit_width=weight_max_overall_bit_width,\n",
    "                                                 tracked_parameter_list_init=self.weight,\n",
    "                                                 ternary_threshold=weight_ternary_threshold,\n",
    "                                                 scaling_stats_input_view_shape_impl=weight_stats_input_view_shape_impl,\n",
    "                                                 scaling_stats_input_concat_dim=weight_scaling_stats_input_concat_dim,\n",
    "                                                 scaling_stats_sigma=weight_scaling_stats_sigma,\n",
    "                                                 scaling_min_val=weight_scaling_min_val,\n",
    "                                                 override_pretrained_bit_width=weight_override_pretrained_bit_width)\n",
    "        self.bias_quant = BiasQuantProxy(quant_type=bias_quant_type,\n",
    "                                         bit_width=bias_bit_width,\n",
    "                                         narrow_range=bias_narrow_range)\n",
    "\n",
    "    @property\n",
    "    def per_output_channel_broadcastable_shape(self):\n",
    "        if self.transposed:\n",
    "            raise Exception(\"Transposed filters are not supported.\")\n",
    "        else:\n",
    "            output_dim = 0\n",
    "        per_channel_size = [1] * len(self.weight.size())\n",
    "        per_channel_size[output_dim] = self.out_channels\n",
    "        per_channel_size = tuple(per_channel_size)\n",
    "        return per_channel_size\n",
    "\n",
    "    @property\n",
    "    def int_weight(self):\n",
    "        if isinstance(self.weight_quant.tensor_quant, IdentityQuant):\n",
    "            raise Exception(\"Can't export int weight without quantization enabled\")\n",
    "        return self.weight_quant.int_weight(self.weight)\n",
    "\n",
    "    @property\n",
    "    def quant_weight_scale(self):\n",
    "        \"\"\"\n",
    "        Returns scale factor of the quantized weights with scalar () shape or (self.out_channels, 1, 1)\n",
    "        shape depending on whether scaling is per layer or per-channel.\n",
    "        -------\n",
    "        \"\"\"\n",
    "        if isinstance(self.weight_quant.tensor_quant, IdentityQuant):\n",
    "            raise Exception(\"Can't generate scaling factor without quantization enabled\")\n",
    "        zero_hw_sentinel = self.weight_quant.zero_hw_sentinel\n",
    "        _, scale, _ = self.weight_quant.tensor_quant(self.weight, zero_hw_sentinel)\n",
    "        return scale\n",
    "\n",
    "    def forward(self, input):\n",
    "        output_scale = None\n",
    "        output_bit_width = None\n",
    "        quant_bias_bit_width = None\n",
    "\n",
    "        input, input_scale, input_bit_width = self.unpack_input(input)\n",
    "        quant_weight, quant_weight_scale, quant_weight_bit_width = self.weight_quant(self.weight)\n",
    "        quant_weight = self.weight_reg(quant_weight)\n",
    "\n",
    "        if self.compute_output_bit_width:\n",
    "            assert input_bit_width is not None\n",
    "            output_bit_width = self.max_output_bit_width(input_bit_width, quant_weight_bit_width)\n",
    "        if self.compute_output_scale:\n",
    "            assert input_scale is not None\n",
    "            output_scale = input_scale * quant_weight_scale\n",
    "\n",
    "        if self.bias is not None:\n",
    "            quant_bias, _, quant_bias_bit_width = self.bias_quant(self.bias, output_scale, output_bit_width)\n",
    "            output = self.conv1d(input, quant_weight, quant_bias)\n",
    "        else:\n",
    "            output = self.conv1d(input, quant_weight, None)\n",
    "\n",
    "        if self.compute_output_bit_width and quant_bias_bit_width is not None:\n",
    "            output_bit_width = torch.where(quant_bias_bit_width > output_bit_width,\n",
    "                                           quant_bias_bit_width,\n",
    "                                           output_bit_width)\n",
    "\n",
    "        return self.pack_output(output, output_scale, output_bit_width)\n",
    "\n",
    "    def conv1d(self, x, weight, bias):\n",
    "        if self.padding_type == PaddingType.SAME:\n",
    "            out = self.conv1d_same_padding(x, weight, bias)\n",
    "        else:\n",
    "            out = conv1d(x, weight, bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "        return out\n",
    "\n",
    "    def conv1d_same_padding(self, x, weight, bias):\n",
    "        ih = x.size()[-1]\n",
    "        kh = weight.size()[-1]\n",
    "        sh = self.stride[0]\n",
    "        oh = math.ceil(ih / sh)\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        if pad_h > 0:\n",
    "            x = F.pad(x, [pad_h // 2, pad_h - pad_h // 2])\n",
    "        out = F.conv1d(x, weight, bias, self.stride, 0, self.dilation, self.groups)\n",
    "        return out\n",
    "\n",
    "    def merge_bn_in(self, bn, affine_only, sign_only):\n",
    "        raise Exception(\"Merged Batch-Normalization is not yet supported\")\n",
    "\n",
    "    def max_output_bit_width(self, input_bit_width, weight_bit_width):\n",
    "        max_uint_input = max_uint(bit_width=input_bit_width, narrow_range=False)\n",
    "        max_kernel_val = self.weight_quant.tensor_quant.int_quant.max_uint(weight_bit_width)\n",
    "        group_size = self.out_channels // self.groups\n",
    "        max_uint_output = max_uint_input * max_kernel_val * self.kernel_size[0] * group_size\n",
    "        max_output_bit_width = ceil_ste(torch.log2(max_uint_output))\n",
    "        return max_output_bit_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common\n",
    "from brevitas.core.bit_width import BitWidthImplType\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.core.scaling import ScalingImplType\n",
    "from brevitas.core.stats import StatsOp\n",
    "from brevitas.nn import QuantConv2d, QuantHardTanh, QuantLinear\n",
    "\n",
    "# Quant common\n",
    "BIT_WIDTH_IMPL_TYPE = BitWidthImplType.CONST\n",
    "SCALING_VALUE_TYPE = RestrictValueType.LOG_FP\n",
    "SCALING_IMPL_TYPE = ScalingImplType.PARAMETER\n",
    "NARROW_RANGE_ENABLED = True\n",
    "\n",
    "# Weight quant common\n",
    "STATS_OP = StatsOp.MEAN_LEARN_SIGMA_STD\n",
    "BIAS_ENABLED = False\n",
    "WEIGHT_SCALING_IMPL_TYPE = ScalingImplType.STATS\n",
    "SIGMA = 0.001\n",
    "\n",
    "# QuantHardTanh configuration\n",
    "HARD_TANH_MIN = -1.0\n",
    "HARD_TANH_MAX = 1.0\n",
    "ACT_PER_OUT_CH_SCALING = False\n",
    "\n",
    "# QuantConv2d configuration\n",
    "CONV_PER_OUT_CH_SCALING = True\n",
    "\n",
    "def get_stats_op(quant_type):\n",
    "    if quant_type == QuantType.BINARY:\n",
    "        return StatsOp.AVE\n",
    "    else:\n",
    "        return StatsOp.MAX\n",
    "\n",
    "\n",
    "def get_quant_type(bit_width):\n",
    "    if bit_width is None:\n",
    "        return QuantType.FP\n",
    "    elif bit_width == 1:\n",
    "        return QuantType.BINARY\n",
    "    else:\n",
    "        return QuantType.INT\n",
    "\n",
    "\n",
    "def get_act_quant(act_bit_width, act_quant_type):\n",
    "    if act_quant_type == QuantType.INT:\n",
    "        act_scaling_impl_type = ScalingImplType.PARAMETER\n",
    "    else:\n",
    "        act_scaling_impl_type = ScalingImplType.CONST\n",
    "    return QuantHardTanh(quant_type=act_quant_type,\n",
    "                         bit_width=act_bit_width,\n",
    "                         bit_width_impl_type=BIT_WIDTH_IMPL_TYPE,\n",
    "                         min_val=HARD_TANH_MIN,\n",
    "                         max_val=HARD_TANH_MAX,\n",
    "                         scaling_impl_type=act_scaling_impl_type,\n",
    "                         restrict_scaling_type=SCALING_VALUE_TYPE,\n",
    "                         scaling_per_channel=ACT_PER_OUT_CH_SCALING,\n",
    "                         narrow_range=NARROW_RANGE_ENABLED)\n",
    "\n",
    "\n",
    "def get_quant_linear(in_features, out_features, per_out_ch_scaling, bit_width, quant_type, stats_op):\n",
    "    return QuantLinear(bias=BIAS_ENABLED,\n",
    "                       in_features=in_features,\n",
    "                       out_features=out_features,\n",
    "                       weight_quant_type=quant_type,\n",
    "                       weight_narrow_range=NARROW_RANGE_ENABLED,\n",
    "                       weight_bit_width=bit_width,\n",
    "                       weight_bit_width_impl_type=BIT_WIDTH_IMPL_TYPE,\n",
    "                       weight_scaling_per_output_channel=per_out_ch_scaling,\n",
    "                       weight_scaling_stats_op=stats_op,\n",
    "                       weight_scaling_stats_sigma=SIGMA)\n",
    "\n",
    "\n",
    "def get_quant_conv2d(in_ch, out_ch, bit_width, quant_type, stats_op):\n",
    "    return QuantConv2d(in_channels=in_ch,\n",
    "                       kernel_size=KERNEL_SIZE,\n",
    "                       out_channels=out_ch,\n",
    "                       weight_quant_type=quant_type,\n",
    "                       weight_bit_width=bit_width,\n",
    "                       weight_narrow_range=NARROW_RANGE_ENABLED,\n",
    "                       weight_scaling_impl_type=WEIGHT_SCALING_IMPL_TYPE,\n",
    "                       weight_scaling_stats_op=stats_op,\n",
    "                       weight_scaling_stats_sigma=SIGMA,\n",
    "                       weight_scaling_per_output_channel=CONV_PER_OUT_CH_SCALING,\n",
    "                       weight_restrict_scaling_type=SCALING_VALUE_TYPE,\n",
    "                       weight_bit_width_impl_type=BIT_WIDTH_IMPL_TYPE,\n",
    "                       bias=BIAS_ENABLED)\n",
    "\n",
    "def get_quant_conv1d(in_ch, out_ch, bit_width, quant_type, stats_op):\n",
    "    return QuantConv1d(in_channels=in_ch,\n",
    "                       kernel_size=KERNEL_SIZE,\n",
    "                       out_channels=out_ch,\n",
    "                       weight_quant_type=quant_type,\n",
    "                       weight_bit_width=bit_width,\n",
    "                       weight_narrow_range=NARROW_RANGE_ENABLED,\n",
    "                       weight_scaling_impl_type=WEIGHT_SCALING_IMPL_TYPE,\n",
    "                       weight_scaling_stats_op=stats_op,\n",
    "                       weight_scaling_stats_sigma=SIGMA,\n",
    "                       weight_scaling_per_output_channel=CONV_PER_OUT_CH_SCALING,\n",
    "                       weight_restrict_scaling_type=SCALING_VALUE_TYPE,\n",
    "                       weight_bit_width_impl_type=BIT_WIDTH_IMPL_TYPE,\n",
    "                       bias=BIAS_ENABLED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class declaration for the 1D CNN, do not touch unless you want to experiment with the network structure.\n",
    "For the fully connected layers, certain configurations may not be fully supported by FINN.\n",
    "More will be explained for this in the FINN workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the classes needed for the CNN, taken from: https://github.com/maltanar/brevitas_cnv_lfc\n",
    "\n",
    "# this is where the pre-trained models also come from, however, we will import the whole thing here to make custom CNNs\n",
    "class CNV(Module):\n",
    "\n",
    "    def __init__(self, num_classes=10, weight_bit_width=None, act_bit_width=None,in_bit_width=None, in_ch=3, device=\"cpu\"):\n",
    "        super(CNV, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        weight_quant_type = get_quant_type(weight_bit_width)\n",
    "        act_quant_type = get_quant_type(act_bit_width)\n",
    "        in_quant_type = get_quant_type(in_bit_width)\n",
    "        stats_op = get_stats_op(weight_quant_type)\n",
    "\n",
    "        self.conv_features = ModuleList()\n",
    "        self.linear_features = ModuleList()\n",
    "        self.conv_features.append(get_act_quant(in_bit_width, in_quant_type))\n",
    "\n",
    "        # convolution layers\n",
    "        for i, out_ch, is_pool_enabled in CNV_OUT_CH_POOL:\n",
    "            self.conv_features.append(get_quant_conv1d(in_ch=in_ch,\n",
    "                                                       out_ch=out_ch,\n",
    "                                                       bit_width=weight_bit_width,\n",
    "                                                       quant_type=weight_quant_type,\n",
    "                                                       stats_op=stats_op))\n",
    "            in_ch = out_ch\n",
    "            self.conv_features.append(BatchNorm1d(in_ch))\n",
    "            if i == (NUM_CONV_LAYERS - 1):\n",
    "                if is_pool_enabled:\n",
    "                    self.conv_features.append(MaxPool1d(kernel_size=2))\n",
    "                self.conv_features.append(Sequential())\n",
    "            else:\n",
    "                self.conv_features.append(get_act_quant(act_bit_width, act_quant_type))\n",
    "                if is_pool_enabled:\n",
    "                    self.conv_features.append(MaxPool1d(kernel_size=2))\n",
    "\n",
    "        # fully connected layers\n",
    "        self.linear_features.append(get_act_quant(in_bit_width, in_quant_type))\n",
    "        \n",
    "        for in_features, out_features in INTERMEDIATE_FC_FEATURES:\n",
    "            self.linear_features.append(get_quant_linear(in_features=in_features,\n",
    "                                                         out_features=out_features,\n",
    "                                                         per_out_ch_scaling=INTERMEDIATE_FC_PER_OUT_CH_SCALING,\n",
    "                                                         bit_width=weight_bit_width,\n",
    "                                                         quant_type=weight_quant_type,\n",
    "                                                         stats_op=stats_op))\n",
    "            self.linear_features.append(BatchNorm1d(out_features))\n",
    "            self.linear_features.append(get_act_quant(act_bit_width, act_quant_type))\n",
    "            \n",
    "        # last layer\n",
    "        self.fc = get_quant_linear(in_features=LAST_FC_IN_FEATURES,\n",
    "                                   out_features=num_classes,\n",
    "                                   per_out_ch_scaling=LAST_FC_PER_OUT_CH_SCALING,\n",
    "                                   bit_width=weight_bit_width,\n",
    "                                   quant_type=weight_quant_type,\n",
    "                                   stats_op=stats_op)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = 2.0 * x - torch.tensor([1.0]).to(self.device)\n",
    "        for mod in self.conv_features:\n",
    "            x = mod(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        for mod in self.linear_features:\n",
    "            x = mod(x)\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        #out = self.fc_bn(x)\n",
    "        return out\n",
    "\n",
    "# this function will not be used as we wont use cfg, but if you want to, you can use this\n",
    "def cnv(cfg):\n",
    "    weight_bit_width = cfg.getint('QUANT', 'WEIGHT_BIT_WIDTH')\n",
    "    act_bit_width = cfg.getint('QUANT', 'ACT_BIT_WIDTH')\n",
    "    in_bit_width = cfg.getint('QUANT', 'IN_BIT_WIDTH')\n",
    "    num_classes = cfg.getint('MODEL', 'NUM_CLASSES')\n",
    "    in_channels = cfg.getint('MODEL', 'IN_CHANNELS')\n",
    "    net = CNV(weight_bit_width=weight_bit_width,\n",
    "              act_bit_width=act_bit_width,\n",
    "              in_bit_width=in_bit_width,\n",
    "              num_classes=num_classes,\n",
    "              in_ch=in_channels)\n",
    "    return net\n",
    "\n",
    "# helper function to create the CNN\n",
    "def cnv_manual(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS):\n",
    "    net = CNV(weight_bit_width=WEIGHT_BIT_WIDTH,\n",
    "              act_bit_width=ACT_BIT_WIDTH,\n",
    "              in_bit_width=IN_BIT_WIDTH,\n",
    "              num_classes=NUM_CLASSES,\n",
    "              in_ch=IN_CHANNELS)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNV(\n",
      "  (conv_features): ModuleList(\n",
      "    (0): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): RescalingIntQuant(\n",
      "            (int_quant): IntQuant(\n",
      "              (float_to_int_impl): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): RoundSte()\n",
      "                  (1): Identity()\n",
      "                )\n",
      "              )\n",
      "              (tensor_clamp_impl): TensorClamp()\n",
      "            )\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (int_scaling_impl): IntScaling(\n",
      "              (forward_impl): SignedFpIntScale()\n",
      "            )\n",
      "            (msb_clamp_bit_width_impl): BitWidthConst()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): QuantConv1d(\n",
      "      2, 32, kernel_size=(4,), stride=(1,), bias=False\n",
      "      (weight_reg): WeightReg()\n",
      "      (weight_quant): WeightQuantProxy(\n",
      "        (tensor_quant): BinaryQuant(\n",
      "          (scaling_impl): ParameterStatsScaling(\n",
      "            (parameter_list_stats): ParameterListStats(\n",
      "              (first_tracked_param): _ViewParameterWrapper()\n",
      "              (stats): Stats(\n",
      "                (stats_impl): AbsAve()\n",
      "              )\n",
      "            )\n",
      "            (stats_scaling_impl): StatsScaling(\n",
      "              (affine_rescaling): Identity()\n",
      "              (restrict_scaling): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "              (restrict_scaling_preprocess): LogTwo()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bias_quant): BiasQuantProxy()\n",
      "    )\n",
      "    (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): ClampedBinaryQuant(\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): QuantConv1d(\n",
      "      32, 64, kernel_size=(4,), stride=(1,), bias=False\n",
      "      (weight_reg): WeightReg()\n",
      "      (weight_quant): WeightQuantProxy(\n",
      "        (tensor_quant): BinaryQuant(\n",
      "          (scaling_impl): ParameterStatsScaling(\n",
      "            (parameter_list_stats): ParameterListStats(\n",
      "              (first_tracked_param): _ViewParameterWrapper()\n",
      "              (stats): Stats(\n",
      "                (stats_impl): AbsAve()\n",
      "              )\n",
      "            )\n",
      "            (stats_scaling_impl): StatsScaling(\n",
      "              (affine_rescaling): Identity()\n",
      "              (restrict_scaling): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "              (restrict_scaling_preprocess): LogTwo()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bias_quant): BiasQuantProxy()\n",
      "    )\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): ClampedBinaryQuant(\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): QuantConv1d(\n",
      "      64, 128, kernel_size=(4,), stride=(1,), bias=False\n",
      "      (weight_reg): WeightReg()\n",
      "      (weight_quant): WeightQuantProxy(\n",
      "        (tensor_quant): BinaryQuant(\n",
      "          (scaling_impl): ParameterStatsScaling(\n",
      "            (parameter_list_stats): ParameterListStats(\n",
      "              (first_tracked_param): _ViewParameterWrapper()\n",
      "              (stats): Stats(\n",
      "                (stats_impl): AbsAve()\n",
      "              )\n",
      "            )\n",
      "            (stats_scaling_impl): StatsScaling(\n",
      "              (affine_rescaling): Identity()\n",
      "              (restrict_scaling): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "              (restrict_scaling_preprocess): LogTwo()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bias_quant): BiasQuantProxy()\n",
      "    )\n",
      "    (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): ClampedBinaryQuant(\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): QuantConv1d(\n",
      "      128, 256, kernel_size=(4,), stride=(1,), bias=False\n",
      "      (weight_reg): WeightReg()\n",
      "      (weight_quant): WeightQuantProxy(\n",
      "        (tensor_quant): BinaryQuant(\n",
      "          (scaling_impl): ParameterStatsScaling(\n",
      "            (parameter_list_stats): ParameterListStats(\n",
      "              (first_tracked_param): _ViewParameterWrapper()\n",
      "              (stats): Stats(\n",
      "                (stats_impl): AbsAve()\n",
      "              )\n",
      "            )\n",
      "            (stats_scaling_impl): StatsScaling(\n",
      "              (affine_rescaling): Identity()\n",
      "              (restrict_scaling): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "              (restrict_scaling_preprocess): LogTwo()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bias_quant): BiasQuantProxy()\n",
      "    )\n",
      "    (12): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Sequential()\n",
      "  )\n",
      "  (linear_features): ModuleList(\n",
      "    (0): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): RescalingIntQuant(\n",
      "            (int_quant): IntQuant(\n",
      "              (float_to_int_impl): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): RoundSte()\n",
      "                  (1): Identity()\n",
      "                )\n",
      "              )\n",
      "              (tensor_clamp_impl): TensorClamp()\n",
      "            )\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (int_scaling_impl): IntScaling(\n",
      "              (forward_impl): SignedFpIntScale()\n",
      "            )\n",
      "            (msb_clamp_bit_width_impl): BitWidthConst()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): QuantLinear(\n",
      "      in_features=256, out_features=256, bias=False\n",
      "      (weight_reg): WeightReg()\n",
      "      (weight_quant): WeightQuantProxy(\n",
      "        (tensor_quant): BinaryQuant(\n",
      "          (scaling_impl): ParameterStatsScaling(\n",
      "            (parameter_list_stats): ParameterListStats(\n",
      "              (first_tracked_param): _ViewParameterWrapper()\n",
      "              (stats): Stats(\n",
      "                (stats_impl): AbsAve()\n",
      "              )\n",
      "            )\n",
      "            (stats_scaling_impl): StatsScaling(\n",
      "              (affine_rescaling): Identity()\n",
      "              (restrict_scaling): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "              (restrict_scaling_preprocess): LogTwo()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bias_quant): BiasQuantProxy()\n",
      "    )\n",
      "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): ClampedBinaryQuant(\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): QuantLinear(\n",
      "      in_features=256, out_features=128, bias=False\n",
      "      (weight_reg): WeightReg()\n",
      "      (weight_quant): WeightQuantProxy(\n",
      "        (tensor_quant): BinaryQuant(\n",
      "          (scaling_impl): ParameterStatsScaling(\n",
      "            (parameter_list_stats): ParameterListStats(\n",
      "              (first_tracked_param): _ViewParameterWrapper()\n",
      "              (stats): Stats(\n",
      "                (stats_impl): AbsAve()\n",
      "              )\n",
      "            )\n",
      "            (stats_scaling_impl): StatsScaling(\n",
      "              (affine_rescaling): Identity()\n",
      "              (restrict_scaling): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "              (restrict_scaling_preprocess): LogTwo()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (bias_quant): BiasQuantProxy()\n",
      "    )\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): QuantHardTanh(\n",
      "      (act_quant_proxy): ActivationQuantProxy(\n",
      "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "          (activation_impl): Identity()\n",
      "          (tensor_quant): ClampedBinaryQuant(\n",
      "            (scaling_impl): StandaloneScaling(\n",
      "              (restrict_value): RestrictValue(\n",
      "                (forward_impl): Sequential(\n",
      "                  (0): PowerOfTwo()\n",
      "                  (1): ClampMin()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): QuantLinear(\n",
      "    in_features=128, out_features=3, bias=False\n",
      "    (weight_reg): WeightReg()\n",
      "    (weight_quant): WeightQuantProxy(\n",
      "      (tensor_quant): BinaryQuant(\n",
      "        (scaling_impl): ParameterStatsScaling(\n",
      "          (parameter_list_stats): ParameterListStats(\n",
      "            (first_tracked_param): _ViewParameterWrapper()\n",
      "            (stats): Stats(\n",
      "              (stats_impl): AbsAve()\n",
      "            )\n",
      "          )\n",
      "          (stats_scaling_impl): StatsScaling(\n",
      "            (affine_rescaling): Identity()\n",
      "            (restrict_scaling): RestrictValue(\n",
      "              (forward_impl): Sequential(\n",
      "                (0): PowerOfTwo()\n",
      "                (1): ClampMin()\n",
      "              )\n",
      "            )\n",
      "            (restrict_scaling_preprocess): LogTwo()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bias_quant): BiasQuantProxy()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create the model and export\n",
    "bnn_pynq_model = cnv_manual(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS)\n",
    "\n",
    "# print structure for reference\n",
    "print(bnn_pynq_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Training the Network\n",
    "\n",
    "Pretty self-explanatory, we will load up data from CSV using pandas and then train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13319, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the data from csv\n",
    "df = pd.read_csv('dataset_24_inputs.csv')\n",
    "\n",
    "# print the dimensions of the CSV file\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([444, 2, 24])\n",
      "torch.Size([110, 2, 24])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# helper function, parse the data from the pandas dataframe\n",
    "def parse_input(df, starting_index):\n",
    "    data = np.array(((df.iloc[starting_index, 0],df.iloc[starting_index + 1,0],df.iloc[starting_index + 2,0],df.iloc[starting_index + 3,0],df.iloc[starting_index + 4,0],df.iloc[starting_index + 5,0],df.iloc[starting_index + 6,0],df.iloc[starting_index + 7,0],\n",
    "                  df.iloc[starting_index + 8, 0], df.iloc[starting_index + 9, 0], df.iloc[starting_index + 10, 0], df.iloc[starting_index + 11, 0], df.iloc[starting_index + 12, 0], df.iloc[starting_index + 13, 0], df.iloc[starting_index + 14, 0], df.iloc[starting_index + 15, 0],\n",
    "                  df.iloc[starting_index + 16, 0], df.iloc[starting_index + 17, 0], df.iloc[starting_index + 18, 0], df.iloc[starting_index + 19, 0], df.iloc[starting_index + 20, 0], df.iloc[starting_index + 21, 0], df.iloc[starting_index + 22, 0], df.iloc[starting_index + 23, 0]),\n",
    "                 (df.iloc[starting_index, 1],df.iloc[starting_index + 1, 1],df.iloc[starting_index + 2, 1],df.iloc[starting_index + 3, 1],df.iloc[starting_index + 4, 1],df.iloc[starting_index + 5, 1],df.iloc[starting_index + 6, 1],df.iloc[starting_index + 7, 1], \n",
    "                  df.iloc[starting_index + 8, 1], df.iloc[starting_index + 9, 1], df.iloc[starting_index + 10, 1], df.iloc[starting_index + 11, 1], df.iloc[starting_index + 12, 1], df.iloc[starting_index + 13, 1], df.iloc[starting_index + 14, 1], df.iloc[starting_index + 15, 1],\n",
    "                  df.iloc[starting_index + 16, 1], df.iloc[starting_index + 17, 1], df.iloc[starting_index + 18, 1], df.iloc[starting_index + 19, 1], df.iloc[starting_index + 20, 1], df.iloc[starting_index + 21, 1], df.iloc[starting_index + 22, 1], df.iloc[starting_index + 23, 1])))\n",
    "    return data\n",
    "\n",
    "# 20% set aside for testing\n",
    "num_items = (df.shape[0]) // 24\n",
    "max_data = 900\n",
    "batch_size = 2\n",
    "\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "x_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for i in range(num_items):\n",
    "    if i == max_data:\n",
    "        break\n",
    "    starting_index = i * 24\n",
    "    \n",
    "    # 24 inputs\n",
    "    data = parse_input(df, starting_index)\n",
    "    \n",
    "    # do encoding, go by index as shown below\n",
    "    if 'zigzag' in df.iloc[starting_index, 5]:\n",
    "        value = (0)\n",
    "    elif 'rocket' in df.iloc[starting_index, 5]:\n",
    "        value = (1)\n",
    "    elif 'hair' in df.iloc[starting_index, 5]:\n",
    "        value = (2)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # set aside data for training / testing\n",
    "    if i % 5 != 4: \n",
    "        x_train_list.append(data)\n",
    "        y_train_list.append(value) \n",
    "    else:\n",
    "        x_test_list.append(data)\n",
    "        y_test_list.append(value)\n",
    "        \n",
    "# remove extra inputs that cannot fit in batch_size\n",
    "while len(x_train_list) % batch_size != 0:\n",
    "    x_train_list.pop()\n",
    "    y_train_list.pop()\n",
    "    \n",
    "while len(x_test_list) % batch_size != 0:\n",
    "    x_test_list.pop()\n",
    "    y_test_list.pop()\n",
    "        \n",
    "# transform to PyTorch DataLoader\n",
    "tensor_x_train = torch.Tensor(x_train_list) # transform to torch tensor\n",
    "tensor_y_train = torch.Tensor(y_train_list)\n",
    "    \n",
    "print(tensor_x_train.size())\n",
    "\n",
    "dataset_train = TensorDataset(tensor_x_train,tensor_y_train)\n",
    "train_loader = DataLoader(dataset_train, batch_size = batch_size, shuffle = True, num_workers = 1)\n",
    "\n",
    "tensor_x_test = torch.Tensor(x_test_list) # transform to torch tensor\n",
    "tensor_y_test = torch.Tensor(y_test_list)\n",
    "    \n",
    "print(tensor_x_test.size())\n",
    "\n",
    "dataset_test = TensorDataset(tensor_x_test,tensor_y_test)\n",
    "test_loader = DataLoader(dataset_test, batch_size = batch_size, shuffle = True, num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 24])\n"
     ]
    }
   ],
   "source": [
    "# validate input data size\n",
    "for data, target in train_loader:\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for training\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.1\n",
    "WEIGHT_DECAY = 0.0001\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(bnn_pynq_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "log_freq = 10\n",
    "\n",
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show statistics\n",
    "num_inputs_train_stats = 0\n",
    "num_inputs_test_stats = 0\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def get_prediction(output_tensor):\n",
    "    prediction_list = softmax(output_tensor.tolist())\n",
    "    max_val = -1\n",
    "    prediction = 0\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] > max_val:\n",
    "            max_val = prediction_list[i]\n",
    "            prediction = i\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def test_on_everything(print_output):\n",
    "    \n",
    "    list_input_train_total = [0] * NUM_CLASSES\n",
    "    list_input_train_correct_total =  [0] * NUM_CLASSES\n",
    "    \n",
    "    list_input_test_total = [0] * NUM_CLASSES\n",
    "    list_input_test_correct_total =  [0] * NUM_CLASSES\n",
    "        \n",
    "    running_loss_train = 0.0\n",
    "    running_loss_test = 0.0\n",
    "        \n",
    "    for i, data in enumerate(train_loader):\n",
    "        (input, target) = data\n",
    "        input = input.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        output = bnn_pynq_model(input)\n",
    "        loss = criterion(output, target.long())\n",
    "        running_loss_train += loss.item()\n",
    "        #print(target, output)\n",
    "\n",
    "        for i in range(len(output)):\n",
    "            if print_output:\n",
    "                print(softmax(output[i].tolist()), \"prediction\", get_prediction(output[i]), \"target\", target[i].tolist())\n",
    "\n",
    "            list_input_train_total[int(target[i].tolist())] += 1\n",
    "\n",
    "            if int(target[i].tolist()) == get_prediction(output[i]):\n",
    "                list_input_train_correct_total[int(target[i].tolist())] += 1\n",
    "\n",
    "    for i, data in enumerate(test_loader):\n",
    "        (input, target) = data\n",
    "        input = input.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        output = bnn_pynq_model(input)\n",
    "        loss = criterion(output, target.long())\n",
    "        running_loss_test += loss.item()\n",
    "\n",
    "        for i in range(len(output)):\n",
    "            if print_output:\n",
    "                print(softmax(output[i].tolist()), \"prediction\", get_prediction(output[i]), \"target\", target[i].tolist())\n",
    "\n",
    "            list_input_test_total[int(target[i].tolist())] += 1\n",
    "\n",
    "            if int(target[i].tolist()) == get_prediction(output[i]):\n",
    "                list_input_test_correct_total[int(target[i].tolist())] += 1    \n",
    "\n",
    "    print(\"number of inputs of each class\", list_input_train_total, \"correctly predicted\", list_input_train_correct_total)\n",
    "\n",
    "    running_total_train = 0\n",
    "    running_total_train_correct = 0\n",
    "    \n",
    "    running_total_test = 0\n",
    "    running_total_test_correct = 0\n",
    "    \n",
    "    for i in range(len(list_input_train_total)):\n",
    "        print(\"correctly predicted class %d at %.3f (training), at %.3f (testing)\" % ((i + 1), (list_input_train_correct_total[i] * 100 / list_input_train_total[i])\n",
    "                                                                                     , (list_input_test_correct_total[i] * 100 / list_input_test_total[i])))\n",
    "        running_total_train += list_input_train_total[i]\n",
    "        running_total_train_correct += list_input_train_correct_total[i]\n",
    "        running_total_test += list_input_test_total[i]\n",
    "        running_total_test_correct += list_input_test_correct_total[i]\n",
    "    \n",
    "    print(\"accuracy at %.3f, loss at %.3f (training), accuracy at %.3f, loss at %.3f (testing)\" \n",
    "          % ((running_total_train_correct * 100 / running_total_train), running_loss_train / running_total_train,\n",
    "            (running_total_test_correct * 100 / running_total_test), running_loss_test / running_total_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' # linux CUDA not set up yet\n",
    "\n",
    "# training stuff\n",
    "# like the CNN model, it is taken from https://github.com/maltanar/brevitas_cnv_lfc\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class TrainingEpochMeters(object):\n",
    "    def __init__(self):\n",
    "        self.batch_time = AverageMeter()\n",
    "        self.data_time = AverageMeter()\n",
    "        self.losses = AverageMeter()\n",
    "        self.top1 = AverageMeter()\n",
    "        self.top5 = AverageMeter()\n",
    "\n",
    "class EvalEpochMeters(object):\n",
    "    def __init__(self):\n",
    "        self.model_time = AverageMeter()\n",
    "        self.loss_time = AverageMeter()\n",
    "        self.losses = AverageMeter()\n",
    "        self.top1 = AverageMeter()\n",
    "        self.top5 = AverageMeter()\n",
    "\n",
    "def eval_model(epoch=None):\n",
    "    eval_meters = EvalEpochMeters()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    bnn_pynq_model.eval()\n",
    "    criterion.eval()\n",
    "\n",
    "    for i, data in enumerate(test_loader):\n",
    "\n",
    "        end = time.time()\n",
    "        (input, target) = data\n",
    "\n",
    "        input = input.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        # compute output\n",
    "        output = bnn_pynq_model(input)\n",
    "\n",
    "        # measure model elapsed time\n",
    "        eval_meters.model_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        #compute loss\n",
    "        loss = criterion(output, target.long())\n",
    "        eval_meters.loss_time.update(time.time() - end)\n",
    "\n",
    "        prec1, prec5 = accuracy(output, target, topk=(1, NUM_CLASSES))\n",
    "        eval_meters.losses.update(loss.item(), input.size(0))\n",
    "        eval_meters.top1.update(prec1.item(), input.size(0))\n",
    "        eval_meters.top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred).long())\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def softmax_train(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def get_prediction_train(output_tensor):\n",
    "    prediction_list = softmax_train(output_tensor.tolist())\n",
    "    max_val = -1\n",
    "    prediction = 0\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] > max_val:\n",
    "            max_val = prediction_list[i]\n",
    "            prediction = i\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# returns the number of correct predictions\n",
    "def compare_output(output, target):\n",
    "    running_total = 0\n",
    "    \n",
    "    # iterate for each item in \n",
    "    for i in range(len(output)):\n",
    "        if get_prediction_train(output[i]) == int(target.tolist()[i]):\n",
    "            running_total += 1\n",
    "    \n",
    "    return running_total\n",
    "        \n",
    "def train_model():\n",
    "    optimizer = optim.SGD(bnn_pynq_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "    # training starts\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # keep track of training / testing metrics\n",
    "    num_inputs_training = len(train_loader) * batch_size\n",
    "    num_inputs_testing = len(test_loader) * batch_size\n",
    "        \n",
    "    for epoch in range(EPOCHS):\n",
    "        print('Epoch [%d] ' % (epoch + 1))\n",
    "        \n",
    "        # keep track of training / testing metrics\n",
    "        running_loss_training = 0.0\n",
    "        running_loss_testing = 0.0\n",
    "        num_inputs_training_correct = 0\n",
    "        num_inputs_testing_correct = 0\n",
    "\n",
    "        # Set to training mode\n",
    "        bnn_pynq_model.train()\n",
    "        criterion.train()\n",
    "\n",
    "        # Init metrics\n",
    "        epoch_meters = TrainingEpochMeters()\n",
    "        start_data_loading = time.time()\n",
    "        \n",
    "        # train\n",
    "        for i, data in enumerate(train_loader):\n",
    "            (input, target) = data\n",
    "            #print(\"input\", input)\n",
    "            #print(\"target\", target)\n",
    "            \n",
    "            input = input.to(device, non_blocking=True)\n",
    "            target = target.to(device, non_blocking=True)\n",
    "            \n",
    "            # measure data loading time\n",
    "            epoch_meters.data_time.update(time.time() - start_data_loading)\n",
    "\n",
    "            # Training batch starts\n",
    "            start_batch = time.time()\n",
    "            output = bnn_pynq_model(input)\n",
    "            loss = criterion(output, target.long())\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # measure elapsed time\n",
    "            epoch_meters.batch_time.update(time.time() - start_batch)\n",
    "            if i % int(log_freq) == 0 or i == len(train_loader) - 1:\n",
    "                # set the third value of topk depending on the number of classes\n",
    "                prec1, prec5 = accuracy(output.detach(), target, topk=(1, NUM_CLASSES))\n",
    "                epoch_meters.losses.update(loss.item(), input.size(0))\n",
    "                epoch_meters.top1.update(prec1.item(), input.size(0))\n",
    "                epoch_meters.top5.update(prec5.item(), input.size(0))\n",
    "                #self.logger.training_batch_cli_log(epoch_meters, epoch, i, len(self.train_loader))\n",
    "                \n",
    "            # update loss and accuracy\n",
    "            running_loss_training += loss.item()\n",
    "            num_inputs_training_correct += compare_output(output, target.long())\n",
    "            \n",
    "        # Perform eval\n",
    "        with torch.no_grad():\n",
    "            top1avg = eval_model(epoch)\n",
    "            \n",
    "        test_on_everything(print_output = False)\n",
    "        print('(took %.3f)' % (time.time() - start_data_loading))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new network\n",
    "bnn_pynq_model = cnv_manual(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [42, 109, 56]\n",
      "correctly predicted class 1 at 28.378 (training), at 30.556 (testing)\n",
      "correctly predicted class 2 at 73.649 (training), at 75.676 (testing)\n",
      "correctly predicted class 3 at 37.838 (training), at 40.541 (testing)\n",
      "accuracy at 46.622, loss at 0.524 (training), accuracy at 49.091, loss at 0.521 (testing)\n",
      "(took 32.736)\n",
      "Epoch [2] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [73, 121, 49]\n",
      "correctly predicted class 1 at 49.324 (training), at 55.556 (testing)\n",
      "correctly predicted class 2 at 81.757 (training), at 83.784 (testing)\n",
      "correctly predicted class 3 at 33.108 (training), at 16.216 (testing)\n",
      "accuracy at 54.730, loss at 0.445 (training), accuracy at 51.818, loss at 0.452 (testing)\n",
      "(took 37.054)\n",
      "Epoch [3] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [83, 126, 83]\n",
      "correctly predicted class 1 at 56.081 (training), at 38.889 (testing)\n",
      "correctly predicted class 2 at 85.135 (training), at 94.595 (testing)\n",
      "correctly predicted class 3 at 56.081 (training), at 59.459 (testing)\n",
      "accuracy at 65.766, loss at 0.422 (training), accuracy at 64.545, loss at 0.446 (testing)\n",
      "(took 39.055)\n",
      "Epoch [4] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [102, 119, 91]\n",
      "correctly predicted class 1 at 68.919 (training), at 77.778 (testing)\n",
      "correctly predicted class 2 at 80.405 (training), at 83.784 (testing)\n",
      "correctly predicted class 3 at 61.486 (training), at 48.649 (testing)\n",
      "accuracy at 70.270, loss at 0.418 (training), accuracy at 70.000, loss at 0.413 (testing)\n",
      "(took 39.255)\n",
      "Epoch [5] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [115, 130, 61]\n",
      "correctly predicted class 1 at 77.703 (training), at 72.222 (testing)\n",
      "correctly predicted class 2 at 87.838 (training), at 81.081 (testing)\n",
      "correctly predicted class 3 at 41.216 (training), at 29.730 (testing)\n",
      "accuracy at 68.919, loss at 0.382 (training), accuracy at 60.909, loss at 0.405 (testing)\n",
      "(took 35.336)\n",
      "Epoch [6] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [71, 142, 79]\n",
      "correctly predicted class 1 at 47.973 (training), at 30.556 (testing)\n",
      "correctly predicted class 2 at 95.946 (training), at 97.297 (testing)\n",
      "correctly predicted class 3 at 53.378 (training), at 48.649 (testing)\n",
      "accuracy at 65.766, loss at 0.403 (training), accuracy at 59.091, loss at 0.426 (testing)\n",
      "(took 36.031)\n",
      "Epoch [7] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [93, 135, 54]\n",
      "correctly predicted class 1 at 62.838 (training), at 55.556 (testing)\n",
      "correctly predicted class 2 at 91.216 (training), at 94.595 (testing)\n",
      "correctly predicted class 3 at 36.486 (training), at 18.919 (testing)\n",
      "accuracy at 63.514, loss at 0.409 (training), accuracy at 56.364, loss at 0.429 (testing)\n",
      "(took 35.729)\n",
      "Epoch [8] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [99, 136, 53]\n",
      "correctly predicted class 1 at 66.892 (training), at 69.444 (testing)\n",
      "correctly predicted class 2 at 91.892 (training), at 83.784 (testing)\n",
      "correctly predicted class 3 at 35.811 (training), at 29.730 (testing)\n",
      "accuracy at 64.865, loss at 0.407 (training), accuracy at 60.909, loss at 0.415 (testing)\n",
      "(took 36.324)\n",
      "Epoch [9] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [93, 81, 92]\n",
      "correctly predicted class 1 at 62.838 (training), at 55.556 (testing)\n",
      "correctly predicted class 2 at 54.730 (training), at 56.757 (testing)\n",
      "correctly predicted class 3 at 62.162 (training), at 67.568 (testing)\n",
      "accuracy at 59.910, loss at 0.485 (training), accuracy at 60.000, loss at 0.481 (testing)\n",
      "(took 36.919)\n",
      "Epoch [10] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [99, 135, 47]\n",
      "correctly predicted class 1 at 66.892 (training), at 55.556 (testing)\n",
      "correctly predicted class 2 at 91.216 (training), at 86.486 (testing)\n",
      "correctly predicted class 3 at 31.757 (training), at 24.324 (testing)\n",
      "accuracy at 63.288, loss at 0.426 (training), accuracy at 55.455, loss at 0.440 (testing)\n",
      "(took 33.227)\n",
      "Epoch [11] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [97, 119, 61]\n",
      "correctly predicted class 1 at 65.541 (training), at 75.000 (testing)\n",
      "correctly predicted class 2 at 80.405 (training), at 72.973 (testing)\n",
      "correctly predicted class 3 at 41.216 (training), at 29.730 (testing)\n",
      "accuracy at 62.387, loss at 0.410 (training), accuracy at 59.091, loss at 0.422 (testing)\n",
      "(took 31.672)\n",
      "Epoch [12] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [94, 119, 65]\n",
      "correctly predicted class 1 at 63.514 (training), at 63.889 (testing)\n",
      "correctly predicted class 2 at 80.405 (training), at 83.784 (testing)\n",
      "correctly predicted class 3 at 43.919 (training), at 29.730 (testing)\n",
      "accuracy at 62.613, loss at 0.440 (training), accuracy at 59.091, loss at 0.449 (testing)\n",
      "(took 32.231)\n",
      "Epoch [13] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [95, 105, 125]\n",
      "correctly predicted class 1 at 64.189 (training), at 66.667 (testing)\n",
      "correctly predicted class 2 at 70.946 (training), at 75.676 (testing)\n",
      "correctly predicted class 3 at 84.459 (training), at 78.378 (testing)\n",
      "accuracy at 73.198, loss at 0.387 (training), accuracy at 73.636, loss at 0.384 (testing)\n",
      "(took 33.585)\n",
      "Epoch [14] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [100, 122, 76]\n",
      "correctly predicted class 1 at 67.568 (training), at 66.667 (testing)\n",
      "correctly predicted class 2 at 82.432 (training), at 91.892 (testing)\n",
      "correctly predicted class 3 at 51.351 (training), at 48.649 (testing)\n",
      "accuracy at 67.117, loss at 0.387 (training), accuracy at 69.091, loss at 0.390 (testing)\n",
      "(took 31.762)\n",
      "Epoch [15] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [89, 140, 85]\n",
      "correctly predicted class 1 at 60.135 (training), at 63.889 (testing)\n",
      "correctly predicted class 2 at 94.595 (training), at 94.595 (testing)\n",
      "correctly predicted class 3 at 57.432 (training), at 59.459 (testing)\n",
      "accuracy at 70.721, loss at 0.386 (training), accuracy at 72.727, loss at 0.384 (testing)\n",
      "(took 32.409)\n",
      "Epoch [16] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [50, 103, 104]\n",
      "correctly predicted class 1 at 33.784 (training), at 38.889 (testing)\n",
      "correctly predicted class 2 at 69.595 (training), at 64.865 (testing)\n",
      "correctly predicted class 3 at 70.270 (training), at 83.784 (testing)\n",
      "accuracy at 57.883, loss at 0.482 (training), accuracy at 62.727, loss at 0.433 (testing)\n",
      "(took 32.672)\n",
      "Epoch [17] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [98, 113, 43]\n",
      "correctly predicted class 1 at 66.216 (training), at 66.667 (testing)\n",
      "correctly predicted class 2 at 76.351 (training), at 72.973 (testing)\n",
      "correctly predicted class 3 at 29.054 (training), at 18.919 (testing)\n",
      "accuracy at 57.207, loss at 0.452 (training), accuracy at 52.727, loss at 0.470 (testing)\n",
      "(took 32.393)\n",
      "Epoch [18] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [95, 122, 89]\n",
      "correctly predicted class 1 at 64.189 (training), at 58.333 (testing)\n",
      "correctly predicted class 2 at 82.432 (training), at 75.676 (testing)\n",
      "correctly predicted class 3 at 60.135 (training), at 59.459 (testing)\n",
      "accuracy at 68.919, loss at 0.396 (training), accuracy at 64.545, loss at 0.418 (testing)\n",
      "(took 32.308)\n",
      "Epoch [19] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [103, 109, 115]\n",
      "correctly predicted class 1 at 69.595 (training), at 61.111 (testing)\n",
      "correctly predicted class 2 at 73.649 (training), at 67.568 (testing)\n",
      "correctly predicted class 3 at 77.703 (training), at 67.568 (testing)\n",
      "accuracy at 73.649, loss at 0.375 (training), accuracy at 65.455, loss at 0.403 (testing)\n",
      "(took 35.088)\n",
      "Epoch [20] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [95, 117, 113]\n",
      "correctly predicted class 1 at 64.189 (training), at 69.444 (testing)\n",
      "correctly predicted class 2 at 79.054 (training), at 91.892 (testing)\n",
      "correctly predicted class 3 at 76.351 (training), at 81.081 (testing)\n",
      "accuracy at 73.198, loss at 0.386 (training), accuracy at 80.909, loss at 0.373 (testing)\n",
      "(took 33.131)\n",
      "Epoch [21] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [59, 125, 101]\n",
      "correctly predicted class 1 at 39.865 (training), at 33.333 (testing)\n",
      "correctly predicted class 2 at 84.459 (training), at 86.486 (testing)\n",
      "correctly predicted class 3 at 68.243 (training), at 51.351 (testing)\n",
      "accuracy at 64.189, loss at 0.465 (training), accuracy at 57.273, loss at 0.485 (testing)\n",
      "(took 31.767)\n",
      "Epoch [22] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of inputs of each class [148, 148, 148] correctly predicted [115, 68, 83]\n",
      "correctly predicted class 1 at 77.703 (training), at 86.111 (testing)\n",
      "correctly predicted class 2 at 45.946 (training), at 40.541 (testing)\n",
      "correctly predicted class 3 at 56.081 (training), at 40.541 (testing)\n",
      "accuracy at 59.910, loss at 0.437 (training), accuracy at 55.455, loss at 0.447 (testing)\n",
      "(took 32.015)\n",
      "Epoch [23] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [96, 49, 72]\n",
      "correctly predicted class 1 at 64.865 (training), at 77.778 (testing)\n",
      "correctly predicted class 2 at 33.108 (training), at 45.946 (testing)\n",
      "correctly predicted class 3 at 48.649 (training), at 54.054 (testing)\n",
      "accuracy at 48.874, loss at 0.501 (training), accuracy at 59.091, loss at 0.490 (testing)\n",
      "(took 31.429)\n",
      "Epoch [24] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [90, 131, 87]\n",
      "correctly predicted class 1 at 60.811 (training), at 72.222 (testing)\n",
      "correctly predicted class 2 at 88.514 (training), at 83.784 (testing)\n",
      "correctly predicted class 3 at 58.784 (training), at 64.865 (testing)\n",
      "accuracy at 69.369, loss at 0.427 (training), accuracy at 73.636, loss at 0.430 (testing)\n",
      "(took 31.357)\n",
      "Epoch [25] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [107, 123, 71]\n",
      "correctly predicted class 1 at 72.297 (training), at 83.333 (testing)\n",
      "correctly predicted class 2 at 83.108 (training), at 75.676 (testing)\n",
      "correctly predicted class 3 at 47.973 (training), at 56.757 (testing)\n",
      "accuracy at 67.793, loss at 0.399 (training), accuracy at 71.818, loss at 0.389 (testing)\n",
      "(took 33.223)\n",
      "Epoch [26] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [95, 133, 103]\n",
      "correctly predicted class 1 at 64.189 (training), at 66.667 (testing)\n",
      "correctly predicted class 2 at 89.865 (training), at 86.486 (testing)\n",
      "correctly predicted class 3 at 69.595 (training), at 64.865 (testing)\n",
      "accuracy at 74.550, loss at 0.376 (training), accuracy at 72.727, loss at 0.359 (testing)\n",
      "(took 32.583)\n",
      "Epoch [27] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [102, 130, 62]\n",
      "correctly predicted class 1 at 68.919 (training), at 83.333 (testing)\n",
      "correctly predicted class 2 at 87.838 (training), at 86.486 (testing)\n",
      "correctly predicted class 3 at 41.892 (training), at 54.054 (testing)\n",
      "accuracy at 66.216, loss at 0.382 (training), accuracy at 74.545, loss at 0.362 (testing)\n",
      "(took 31.541)\n",
      "Epoch [28] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [84, 137, 84]\n",
      "correctly predicted class 1 at 56.757 (training), at 63.889 (testing)\n",
      "correctly predicted class 2 at 92.568 (training), at 97.297 (testing)\n",
      "correctly predicted class 3 at 56.757 (training), at 59.459 (testing)\n",
      "accuracy at 68.694, loss at 0.376 (training), accuracy at 73.636, loss at 0.364 (testing)\n",
      "(took 31.400)\n",
      "Epoch [29] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [84, 134, 107]\n",
      "correctly predicted class 1 at 56.757 (training), at 61.111 (testing)\n",
      "correctly predicted class 2 at 90.541 (training), at 81.081 (testing)\n",
      "correctly predicted class 3 at 72.297 (training), at 54.054 (testing)\n",
      "accuracy at 73.198, loss at 0.341 (training), accuracy at 65.455, loss at 0.363 (testing)\n",
      "(took 31.525)\n",
      "Epoch [30] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [75, 140, 112]\n",
      "correctly predicted class 1 at 50.676 (training), at 55.556 (testing)\n",
      "correctly predicted class 2 at 94.595 (training), at 91.892 (testing)\n",
      "correctly predicted class 3 at 75.676 (training), at 67.568 (testing)\n",
      "accuracy at 73.649, loss at 0.362 (training), accuracy at 71.818, loss at 0.344 (testing)\n",
      "(took 31.583)\n",
      "Epoch [31] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [94, 145, 87]\n",
      "correctly predicted class 1 at 63.514 (training), at 69.444 (testing)\n",
      "correctly predicted class 2 at 97.973 (training), at 91.892 (testing)\n",
      "correctly predicted class 3 at 58.784 (training), at 43.243 (testing)\n",
      "accuracy at 73.423, loss at 0.346 (training), accuracy at 68.182, loss at 0.345 (testing)\n",
      "(took 32.185)\n",
      "Epoch [32] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [90, 129, 127]\n",
      "correctly predicted class 1 at 60.811 (training), at 69.444 (testing)\n",
      "correctly predicted class 2 at 87.162 (training), at 81.081 (testing)\n",
      "correctly predicted class 3 at 85.811 (training), at 83.784 (testing)\n",
      "accuracy at 77.928, loss at 0.332 (training), accuracy at 78.182, loss at 0.342 (testing)\n",
      "(took 31.406)\n",
      "Epoch [33] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [90, 143, 91]\n",
      "correctly predicted class 1 at 60.811 (training), at 69.444 (testing)\n",
      "correctly predicted class 2 at 96.622 (training), at 94.595 (testing)\n",
      "correctly predicted class 3 at 61.486 (training), at 51.351 (testing)\n",
      "accuracy at 72.973, loss at 0.364 (training), accuracy at 71.818, loss at 0.346 (testing)\n",
      "(took 31.267)\n",
      "Epoch [34] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [118, 125, 101]\n",
      "correctly predicted class 1 at 79.730 (training), at 86.111 (testing)\n",
      "correctly predicted class 2 at 84.459 (training), at 94.595 (testing)\n",
      "correctly predicted class 3 at 68.243 (training), at 64.865 (testing)\n",
      "accuracy at 77.477, loss at 0.322 (training), accuracy at 81.818, loss at 0.330 (testing)\n",
      "(took 31.327)\n",
      "Epoch [35] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [109, 132, 104]\n",
      "correctly predicted class 1 at 73.649 (training), at 72.222 (testing)\n",
      "correctly predicted class 2 at 89.189 (training), at 91.892 (testing)\n",
      "correctly predicted class 3 at 70.270 (training), at 72.973 (testing)\n",
      "accuracy at 77.703, loss at 0.321 (training), accuracy at 79.091, loss at 0.327 (testing)\n",
      "(took 30.985)\n",
      "Epoch [36] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [99, 130, 127]\n",
      "correctly predicted class 1 at 66.892 (training), at 72.222 (testing)\n",
      "correctly predicted class 2 at 87.838 (training), at 91.892 (testing)\n",
      "correctly predicted class 3 at 85.811 (training), at 75.676 (testing)\n",
      "accuracy at 80.180, loss at 0.309 (training), accuracy at 80.000, loss at 0.307 (testing)\n",
      "(took 31.398)\n",
      "Epoch [37] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [102, 118, 118]\n",
      "correctly predicted class 1 at 68.919 (training), at 69.444 (testing)\n",
      "correctly predicted class 2 at 79.730 (training), at 86.486 (testing)\n",
      "correctly predicted class 3 at 79.730 (training), at 81.081 (testing)\n",
      "accuracy at 76.126, loss at 0.333 (training), accuracy at 79.091, loss at 0.328 (testing)\n",
      "(took 32.654)\n",
      "Epoch [38] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [114, 133, 119]\n",
      "correctly predicted class 1 at 77.027 (training), at 83.333 (testing)\n",
      "correctly predicted class 2 at 89.865 (training), at 86.486 (testing)\n",
      "correctly predicted class 3 at 80.405 (training), at 78.378 (testing)\n",
      "accuracy at 82.432, loss at 0.319 (training), accuracy at 82.727, loss at 0.333 (testing)\n",
      "(took 32.211)\n",
      "Epoch [39] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [116, 118, 132]\n",
      "correctly predicted class 1 at 78.378 (training), at 75.000 (testing)\n",
      "correctly predicted class 2 at 79.730 (training), at 81.081 (testing)\n",
      "correctly predicted class 3 at 89.189 (training), at 81.081 (testing)\n",
      "accuracy at 82.432, loss at 0.289 (training), accuracy at 79.091, loss at 0.306 (testing)\n",
      "(took 31.490)\n",
      "Epoch [40] \n",
      "number of inputs of each class [148, 148, 148] correctly predicted [103, 135, 114]\n",
      "correctly predicted class 1 at 69.595 (training), at 69.444 (testing)\n",
      "correctly predicted class 2 at 91.216 (training), at 83.784 (testing)\n",
      "correctly predicted class 3 at 77.027 (training), at 78.378 (testing)\n",
      "accuracy at 79.279, loss at 0.279 (training), accuracy at 77.273, loss at 0.294 (testing)\n",
      "(took 31.331)\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "if train_network:\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bnn_pynq_model.state_dict(), build_dir + model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the model weights\n",
    "for param in bnn_pynq_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Split Model into Conv1D and FC Layers\n",
    "\n",
    "Given that FINN cannot synthesize Conv1D layers, we need to split the model into two components, so the FC layers can undergo synthesis and be accelerated through hardware.\n",
    "\n",
    "If the normal 1D CNN class is modified, please remember to update here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the software half\n",
    "class CNV_software(Module):\n",
    "\n",
    "    def __init__(self, num_classes=10, weight_bit_width=None, act_bit_width=None,in_bit_width=None, in_ch=3, device=\"cpu\"):\n",
    "        super(CNV_software, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        weight_quant_type = get_quant_type(weight_bit_width)\n",
    "        act_quant_type = get_quant_type(act_bit_width)\n",
    "        in_quant_type = get_quant_type(in_bit_width)\n",
    "        stats_op = get_stats_op(weight_quant_type)\n",
    "\n",
    "        self.conv_features = ModuleList()\n",
    "        self.conv_features.append(get_act_quant(in_bit_width, in_quant_type))\n",
    "\n",
    "        # convolution layers\n",
    "        for i, out_ch, is_pool_enabled in CNV_OUT_CH_POOL:\n",
    "            self.conv_features.append(get_quant_conv1d(in_ch=in_ch,\n",
    "                                                       out_ch=out_ch,\n",
    "                                                       bit_width=weight_bit_width,\n",
    "                                                       quant_type=weight_quant_type,\n",
    "                                                       stats_op=stats_op))\n",
    "            in_ch = out_ch\n",
    "            self.conv_features.append(BatchNorm1d(in_ch))\n",
    "            if i == (NUM_CONV_LAYERS - 1):\n",
    "                self.conv_features.append(Sequential())\n",
    "            else:\n",
    "                self.conv_features.append(get_act_quant(act_bit_width, act_quant_type))\n",
    "            if is_pool_enabled:\n",
    "                self.conv_features.append(MaxPool1d(kernel_size=2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = 2.0 * x - torch.tensor([1.0]).to(self.device)\n",
    "        for mod in self.conv_features:\n",
    "            x = mod(x)\n",
    "            \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "def cnv_software(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS):\n",
    "    net = CNV_software(weight_bit_width=WEIGHT_BIT_WIDTH,\n",
    "              act_bit_width=ACT_BIT_WIDTH,\n",
    "              in_bit_width=IN_BIT_WIDTH,\n",
    "              num_classes=NUM_CLASSES,\n",
    "              in_ch=IN_CHANNELS)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hardware half\n",
    "# declare the classes needed for the CNN, taken from: https://github.com/maltanar/brevitas_cnv_lfc\n",
    "# this is where the pre-trained models also come from, however, we will import the whole thing here to make custom CNNs\n",
    "class CNV_hardware(Module):\n",
    "\n",
    "    def __init__(self, num_classes=10, weight_bit_width=None, act_bit_width=None,in_bit_width=None, in_ch=3, device=\"cpu\"):\n",
    "        super(CNV_hardware, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        weight_quant_type = get_quant_type(weight_bit_width)\n",
    "        act_quant_type = get_quant_type(act_bit_width)\n",
    "        in_quant_type = get_quant_type(in_bit_width)\n",
    "        stats_op = get_stats_op(weight_quant_type)\n",
    "\n",
    "        self.linear_features = ModuleList()\n",
    "\n",
    "        # fully connected layers\n",
    "        self.linear_features.append(get_act_quant(in_bit_width, in_quant_type))\n",
    "        #in_features = reduce(mul, in_features)\n",
    "        \n",
    "        for in_features, out_features in INTERMEDIATE_FC_FEATURES:\n",
    "            self.linear_features.append(get_quant_linear(in_features=in_features,\n",
    "                                                         out_features=out_features,\n",
    "                                                         per_out_ch_scaling=INTERMEDIATE_FC_PER_OUT_CH_SCALING,\n",
    "                                                         bit_width=weight_bit_width,\n",
    "                                                         quant_type=weight_quant_type,\n",
    "                                                         stats_op=stats_op))\n",
    "            self.linear_features.append(BatchNorm1d(out_features))\n",
    "            self.linear_features.append(get_act_quant(act_bit_width, act_quant_type))\n",
    "            \n",
    "        # last layer\n",
    "        self.fc = get_quant_linear(in_features=LAST_FC_IN_FEATURES,\n",
    "                                   out_features=num_classes,\n",
    "                                   per_out_ch_scaling=LAST_FC_PER_OUT_CH_SCALING,\n",
    "                                   bit_width=weight_bit_width,\n",
    "                                   quant_type=weight_quant_type,\n",
    "                                   stats_op=stats_op)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        for mod in self.linear_features:\n",
    "            x = mod(x)\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "def cnv_hardware(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS):\n",
    "    net = CNV_hardware(weight_bit_width=WEIGHT_BIT_WIDTH,\n",
    "              act_bit_width=ACT_BIT_WIDTH,\n",
    "              in_bit_width=IN_BIT_WIDTH,\n",
    "              num_classes=NUM_CLASSES,\n",
    "              in_ch=IN_CHANNELS)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate both models\n",
    "cnv_software_model = cnv_software(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS)\n",
    "cnv_hardware_model = cnv_hardware(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS)\n",
    "\n",
    "# split the layers of the original model\n",
    "cnv_pretrained_model = cnv_manual(WEIGHT_BIT_WIDTH, ACT_BIT_WIDTH, IN_BIT_WIDTH, NUM_CLASSES, IN_CHANNELS)\n",
    "cnv_pretrained_model.load_state_dict(torch.load(build_dir + model_path))\n",
    "cnv_pretrained_model.eval()\n",
    "\n",
    "# print the model weights for reference\n",
    "for param in cnv_pretrained_model.parameters():\n",
    "    print(param.data)\n",
    "    \n",
    "# copy over the layers\n",
    "cnv_software_model.conv_features = cnv_pretrained_model.conv_features\n",
    "cnv_hardware_model.linear_features = cnv_pretrained_model.linear_features\n",
    "cnv_hardware_model.fc = cnv_pretrained_model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the model weights for reference, you can verify this if you want\n",
    "print(\"software\")\n",
    "for param in cnv_software_model.parameters():\n",
    "    print(param.data)\n",
    "\n",
    "print(\"hardware\")\n",
    "for param in cnv_hardware_model.parameters():\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test through both models and verify accuracy\n",
    "def test_on_everything_split(data_loader, print_output = False):\n",
    "\n",
    "    list_input_train_total = [0] * NUM_CLASSES\n",
    "    list_input_train_correct_total = [0] * NUM_CLASSES\n",
    "    list_input_train_original_correct_total = [0] * NUM_CLASSES\n",
    "    \n",
    "    for i, data in enumerate(data_loader):\n",
    "        (input, target) = data\n",
    "        input = input.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        software_output = cnv_software_model(input)\n",
    "        output = cnv_hardware_model(software_output)\n",
    "        #print(target, output)\n",
    "        \n",
    "        regular_output = cnv_pretrained_model(input)\n",
    "\n",
    "        for i in range(len(output)):\n",
    "            if print_output:\n",
    "                print(\"regular\", softmax(regular_output[i].tolist()), \"prediction\", get_prediction(regular_output[i]),\n",
    "                      softmax(output[i].tolist()), \"prediction\", get_prediction(output[i]), \"target\", target[i].tolist())\n",
    "\n",
    "            list_input_train_total[int(target[i].tolist())] += 1\n",
    "\n",
    "            if int(target[i].tolist()) == get_prediction(output[i]):\n",
    "                list_input_train_correct_total[int(target[i].tolist())] += 1\n",
    "\n",
    "            if int(target[i].tolist()) == get_prediction(regular_output[i]):\n",
    "                list_input_train_original_correct_total[int(target[i].tolist())] += 1\n",
    "\n",
    "    print(\"number of inputs of each class\", list_input_train_total)\n",
    "    print(\"correctly predicted (split)\", list_input_train_correct_total)\n",
    "\n",
    "    running_total = 0\n",
    "    running_total_correct = 0\n",
    "    for i in range(len(list_input_train_total)):\n",
    "        print(\"correctly predicted class %d at %.3f\" % ((i + 1), (list_input_train_correct_total[i] * 100 / list_input_train_total[i])))\n",
    "        running_total += list_input_train_total[i]\n",
    "        running_total_correct += list_input_train_correct_total[i]\n",
    "    \n",
    "    print(\"accuracy at %.3f\" % ((running_total_correct * 100 / running_total)))\n",
    "    \n",
    "    print(\"correctly predicted (original)\", list_input_train_original_correct_total)\n",
    "    \n",
    "    running_total = 0\n",
    "    running_total_correct = 0\n",
    "        \n",
    "    for i in range(len(list_input_train_total)):\n",
    "        print(\"correctly predicted class %d at %.3f\" % ((i + 1), (list_input_train_original_correct_total[i] * 100 / list_input_train_total[i])))\n",
    "        running_total += list_input_train_total[i]\n",
    "        running_total_correct += list_input_train_original_correct_total[i]\n",
    "    \n",
    "test_on_everything_split(train_loader, print_output = True)\n",
    "test_on_everything_split(test_loader, print_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# export the trained hardware model to ONNX to synthesize\n",
    "INPUT_SPECIFICATIONS_HARDWARE = (1, 256) # batch size, length\n",
    "bo.export_finn_onnx(cnv_hardware_model, INPUT_SPECIFICATIONS_HARDWARE, build_dir + file_name + \".onnx\")\n",
    "\n",
    "print(cnv_hardware_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Preparing the Network\n",
    "\n",
    "The fully connected layers will then undergo the workflow for FINN, to synthesize them into a bitstream to be overlayed onto the Ultra96."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the onnx model made by brevitas\n",
    "# use http://localhost:8081/ if docker is used on Ubuntu WSL\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \".onnx\")\n",
    "showInNetron(build_dir + file_name + \".onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some transformations and then show on netron\n",
    "\n",
    "model = model.transform(DoubleToSingleFloat())\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "#model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(build_dir + file_name + \"_tidy.onnx\")\n",
    "\n",
    "showInNetron(build_dir + file_name + \"_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is exported, let's have a look at its layer structure with Netron. Remember that the visualization below is interactive, you can click on the individual nodes and view the layer attributes, trained weights and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the network is composed of a repeating convolution-convolution-maxpool layer pattern to extract features using 3x3 convolution kernels (with weights binarized) and `Sign` activations, followed by fully connected layers acting as the classifier. Also notice the initial `MultiThreshold` layer at the beginning of the network, which is quantizing float inputs to 8-bit ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How FINN Implements Convolutions: Lowering and Streamlining\n",
    "\n",
    "In FINN, we implement convolutions with the *lowering* approach: we convert them to matrix-matrix multiply operations, where one of the matrices is generated by sliding a window over the input image. You can read more about the sliding window operator and how convolution lowering works [in this notebook](https://github.com/maltanar/qnn-inference-examples/blob/master/3-convolutional-binarized-gtsrb.ipynb). The streaming dataflow architecture we will end up with is going to look something like this figure from the [FINN-R paper](https://arxiv.org/abs/1809.04570):\n",
    "\n",
    "![](cnv-mp-fc.png)\n",
    "\n",
    "Note how the convolution layer looks very similar to the fully connected one in terms of the matrix-vector-threshold unit (MVTU), but now the MVTU is preceded by a sliding window unit that produces the matrix from the input image. All of these building blocks, including the `MaxPool` layer you see in this figure, exist as templated Vivado HLS C++ functions in [finn-hlslib](https://github.com/Xilinx/finn-hlslib).\n",
    "\n",
    "\n",
    "To target this kind of hardware architecture with our network we'll apply a convolution lowering transformation, in addition to streamlining. You may recall the *streamlining transformation* that we applied to the TFC-w1a1 network, which is a series of mathematical simplifications that allow us to get rid of floating point scaling operations by implementing few-bit activations as thresholding operations. **The current implementation of streamlining is highly network-specific and may not work for your network if its topology is very different than the example network here. We hope to rectify this in future releases.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(build_dir + file_name + \"_tidy.onnx\")\n",
    "\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "model = model.transform(Streamline())\n",
    "\n",
    "model.save(build_dir + file_name + \"_streamlined.onnx\")\n",
    "showInNetron(build_dir + file_name + \"_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't go into too much detail about what happens in each transformation and why they are called in the particular order they are (feel free to visualize the intermediate steps using Netron yourself if you are curious) but here is a brief summmmary:\n",
    "\n",
    "* `Streamline` moves floating point scaling and addition operations closer to the input of the nearest thresholding activation and absorbs them into thresholds\n",
    "* `LowerConvsToMatMul` converts ONNX `Conv` nodes into sequences of `Im2Col, MatMul` nodes as discussed above. `Im2Col` is a custom FINN ONNX high-level node type that implements the sliding window operator.\n",
    "* `MakeMaxPoolNHWC` and `AbsorbTransposeIntoMultiThreshold` convert the *data layout* of the network into the NHWC data layout that finn-hlslib primitives use. NCHW means the tensor dimensions are ordered as `(N : batch, H : height, W : width, C : channels)` (assuming 2D images). The ONNX standard ops normally use the NCHW layout, but the ONNX intermediate representation itself does not dictate any data layout.\n",
    "* You may recall `ConvertBipolarMatMulToXnorPopcount` from the TFC-w1a1 example, which is needed to implement bipolar-by-bipolar (w1a1) networks correctly using finn-hlslib.\n",
    "\n",
    "Let's visualize the streamlined and lowered network with Netron. Observe how all the `Conv` nodes have turned into pairs of `Im2Col, MatMul` nodes, and many nodes including `BatchNorm, Mul, Add` nodes have disappeared and replaced with `MultiThreshold` nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Partitioning, Conversion to HLS Layers and Folding\n",
    "\n",
    "The next steps will be (again) very similar to what we did for the TFC-w1a1 network. We'll first convert the layers that we can put into the FPGA into their HLS equivalents and separate them out into a *dataflow partition*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.move_reshape import MoveReshape\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "\n",
    "# choose the memory mode for the MVTU units, decoupled or const\n",
    "mem_mode = \"decoupled\"\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \"_streamlined.onnx\")\n",
    "model = model.transform(to_hls.InferBinaryStreamingFCLayer(mem_mode))\n",
    "model = model.transform(to_hls.InferQuantizedStreamingFCLayer(mem_mode))\n",
    "model = model.transform(to_hls.InferConvInpGen())\n",
    "model = model.transform(to_hls.InferStreamingMaxPool())\n",
    "\n",
    "# get rid of Reshape(-1, 1) operation between hlslib nodes\n",
    "model = model.transform(MoveReshape())\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(build_dir + file_name + \"_dataflow_parent.onnx\")\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "\n",
    "# save the dataflow partition with a different name for easier access\n",
    "dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "dataflow_model.save(build_dir + file_name + \"_dataflow_model.onnx\")\n",
    "showInNetron(build_dir + file_name + \"_dataflow_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the additional `MoveReshape` transformation that was not used for TFC-w1a1. In the last Netron visualization you may have noticed a `Reshape` operation towards the end of the network where the convolutional part of the network ends and the fully-connected layers started. That `Reshape` is essentialy a tensor flattening operation, which we can remove for the purposes of hardware implementation. We can examine the contents of the dataflow partition with Netron, and observe the `ConvolutionInputGenerator`, `StreamingFCLayer_Batch` and `StreamingMaxPool_Batch` nodes that implement the sliding window, matrix multiply and maxpool operations in hlslib. *Note that the StreamingFCLayer instances following the ConvolutionInputGenerator nodes are really implementing the convolutions, despite the name. The final three StreamingFCLayer instances implement actual FC layers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to set the *folding factors* for certain layers to adjust the performance of our accelerator, similar to the TFC-w1a1 example. We'll also set the desired FIFO depths around those layers, which are important to achieve full throughput in the accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.insert_dwc import InsertDWC\n",
    "from finn.transformation.fpgadataflow.insert_tlastmarker import InsertTLastMarker\n",
    "from finn.transformation.fpgadataflow.insert_fifo import InsertFIFO\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \"_dataflow_model.onnx\")\n",
    "fc_layers = model.get_nodes_by_op_type(\"StreamingFCLayer_Batch\")\n",
    "# each tuple is (PE, SIMD, in_fifo_depth) for a layer\n",
    "folding = [\n",
    "    (4, 32, 128),\n",
    "    (4, 32, 128),\n",
    "    (3, 8, 128)\n",
    "]\n",
    "\n",
    "for fcl, (pe, simd, ififodepth) in zip(fc_layers, folding):\n",
    "    fcl_inst = getCustomOp(fcl)\n",
    "    fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    fcl_inst.set_nodeattr(\"inFIFODepth\", ififodepth)\n",
    "    print(fcl)\n",
    "\n",
    "# use same SIMD values for the sliding window operators\n",
    "swg_layers = model.get_nodes_by_op_type(\"ConvolutionInputGenerator\")\n",
    "for i in range(len(swg_layers)):\n",
    "    swg_inst = getCustomOp(swg_layers[i])\n",
    "    simd = folding[i][1]\n",
    "    swg_inst.set_nodeattr(\"SIMD\", simd)\n",
    "\n",
    "# save intermediate so that we can reference in netron and debug if folding factors are not correct\n",
    "model.save(build_dir + file_name + \"_folded_intermediate.onnx\")\n",
    "showInNetron(build_dir + file_name + \"_folded_intermediate.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the transformation\n",
    "model = ModelWrapper(build_dir + file_name + \"_folded_intermediate.onnx\")\n",
    "\n",
    "model = model.transform(InsertDWC())\n",
    "model = model.transform(InsertFIFO())\n",
    "model = model.transform(InsertTLastMarker())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "\n",
    "model.save(build_dir + file_name + \"_folded.onnx\")\n",
    "showInNetron(build_dir + file_name + \"_folded.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we visualize in Netron to observe the `StreamingDataWidthConverter` and `StreamingFIFO` nodes that have been inserted into graph, as well as the folding factors in the `PE` and `SIMD` attributes of each `StreamingFCLayer_Batch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network is now ready and we can start with the hardware generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hardware Generation\n",
    "\n",
    "From this point onward, the steps we have to follow do not depend on the particular network and will be exactly the same as the TFC-w1a1 example. We first proceed with HLS synthesis, **which may take 10-20 minutes depending on your host computer and your RAM cause of WSL**.\n",
    "\n",
    "**Note: WSL takes 10GB of RAM to perform synthesis, else it crashes halfway.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.prepare_ip import PrepareIP\n",
    "from finn.transformation.fpgadataflow.hlssynth_ip import HLSSynthIP\n",
    "from finn.util.basic import pynq_part_map\n",
    "time = datetime.datetime.now()\n",
    "\n",
    "test_pynq_board = \"Ultra96\"\n",
    "test_fpga_part = pynq_part_map[test_pynq_board]\n",
    "target_clk_ns = 10\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \"_folded.onnx\")\n",
    "model = model.transform(PrepareIP(test_fpga_part, target_clk_ns))\n",
    "model = model.transform(HLSSynthIP())\n",
    "model.save(build_dir + file_name + \"_ipgen.onnx\")\n",
    "print(\"took\", datetime.datetime.now() - time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the HLS synthesis is complete, we can stitch together the generated IP blocks into a larger IP that is the implementation of our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.replace_verilog_relpaths import (\n",
    "    ReplaceVerilogRelPaths,\n",
    ")\n",
    "from finn.transformation.fpgadataflow.create_stitched_ip import CreateStitchedIP\n",
    "time = datetime.datetime.now()\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \"_ipgen.onnx\")\n",
    "model = model.transform(ReplaceVerilogRelPaths())\n",
    "model = model.transform(CreateStitchedIP(test_fpga_part))\n",
    "model.save(build_dir + file_name + \"_ipstitch.onnx\")\n",
    "print(\"took\", datetime.datetime.now() - time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a PYNQ project that includes the hardware \"shell\" that will support our accelerator, including the data movers, and run Vivado synthesis, **which may take around 30 minutes depending on your host computer.**\n",
    "\n",
    "*If you'd like to watch the progress, you can open the generated project file (printed below) with the Vivado GUI.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.make_pynq_proj import MakePYNQProject\n",
    "from finn.transformation.fpgadataflow.synth_pynq_proj import SynthPYNQProject\n",
    "time = datetime.datetime.now()\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \"_ipstitch.onnx\")\n",
    "model = model.transform(MakePYNQProject(test_pynq_board))\n",
    "vivado_proj = model.get_metadata_prop(\"vivado_pynq_proj\")\n",
    "print(\"Vivado synthesis project is at %s/resizer.xpr\" % vivado_proj)\n",
    "model.save(build_dir + file_name + \"_pynqproj.onnx\")\n",
    "print(\"took\", datetime.datetime.now() - time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.datetime.now()\n",
    "\n",
    "model = ModelWrapper(build_dir + file_name + \"_pynqproj.onnx\")\n",
    "model = model.transform(SynthPYNQProject())\n",
    "model.save(build_dir + file_name + \"_synth.onnx\")\n",
    "print(\"took\", datetime.datetime.now() - time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deployment and Remote Execution\n",
    "\n",
    "Now that we're done with the hardware generation, we can generate a Python driver for accelerator and copy the necessary files onto our PYNQ board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "from finn.transformation.fpgadataflow.make_deployment import DeployToPYNQ\n",
    "\n",
    "# FINN will use ssh to deploy and run the generated accelerator\n",
    "# please run ultra96_port_forwarding.ipynb before transferring\n",
    "ip = \"localhost\"\n",
    "port = \"3100\"\n",
    "username = \"xilinx\"\n",
    "password = \"xilinx\"\n",
    "target_dir = \"/home/xilinx/finn/cnv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(build_dir + file_name + \"_synth.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy to the remote Ultra96\n",
    "model = model.transform(MakePYNQDriver())\n",
    "model = model.transform(DeployToPYNQ(ip, port, username, password, target_dir))\n",
    "model.save(build_dir + file_name + \"_pynq_deploy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the files are copied over\n",
    "pynq_folder_name = vivado_proj[36:]\n",
    "#print(pynq_folder_name)\n",
    "! sshpass -p {password} ssh {username}@{ip} -p {port} 'ls -l {target_dir}/*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Dataset\n",
    "\n",
    "Load the testing data and do remote execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_model = ModelWrapper(build_dir + file_name + \"_dataflow_parent.onnx\")\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "sdp_node.set_nodeattr(\"model\", build_dir + file_name + \"_pynq_deploy.onnx\")\n",
    "parent_model.save(build_dir + file_name + \"_dataflow_parent_with_remote_bitfile_exec.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the next input from the test_loader (dataset is random)\n",
    "\n",
    "test_input, test_output = next(iter(test_loader))\n",
    "test_output = test_output[0]\n",
    "\n",
    "print(\"input: {}, output: {}\".format(test_input[0], test_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_exec(model, input_dict, return_full_exec_context=False):\n",
    "    if not model.check_all_tensor_shapes_specified():\n",
    "        raise Exception(\"Found unspecified tensor shapes, try infer_shapes\")\n",
    "\n",
    "    graph = model.graph\n",
    "    # first, we need to make sure that every variable required by the graph has\n",
    "    # some buffer associated with it. this includes graph inputs (which includes\n",
    "    # the input data as well as the trained parameters) and the graph ValueInfo\n",
    "    # (intermediate tensors between layers)\n",
    "    # this is provided by the execution_context, which is a dict of np.ndarray\n",
    "    execution_context = model.make_empty_exec_context()\n",
    "    # fill in any inputs provided to this function\n",
    "    for inp_name in input_dict.keys():\n",
    "        if inp_name in execution_context:\n",
    "            if execution_context[inp_name].shape == input_dict[inp_name].shape:\n",
    "                execution_context[inp_name] = input_dict[inp_name]\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    \"Shape mismatch for provided input %s: found %s expected %s \"\n",
    "                    % (\n",
    "                        inp_name,\n",
    "                        str(execution_context[inp_name].shape),\n",
    "                        str(input_dict[inp_name].shape),\n",
    "                    )\n",
    "                )\n",
    "        # else:\n",
    "        # raise Exception(\"Provided input not found in graph context: %s\" % inp_name)\n",
    "\n",
    "    # check if model has an execution mode set\n",
    "    # if None, execute model node by node using execute_node()\n",
    "    # if set to \"remote_pynq\" execute model on PYNQ board\n",
    "    # if set to \"rtlsim\" execute model using pyverilator\n",
    "    model_exec_mode = model.get_metadata_prop(\"exec_mode\")\n",
    "    print(model_exec_mode)\n",
    "\n",
    "    inp = execution_context[model.graph.input[0].name]\n",
    "    print(inp)\n",
    "    # make copy of array before saving it\n",
    "    inp = inp.copy()\n",
    "    np.save(os.path.join(\"input_test.npy\"), inp)\n",
    "    \n",
    "    input = np.load(\"input_test.npy\")\n",
    "    print(input)\n",
    "\n",
    "#print(np.load(\"input.npy\"))\n",
    "test_exec(parent_model, input_dict, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from finn.core.onnx_exec import execute_onnx\n",
    "\n",
    "# perform software inference locally here\n",
    "software_output = cnv_software_model(test_input)\n",
    "\n",
    "iname = parent_model.graph.input[0].name\n",
    "oname = parent_model.graph.output[0].name\n",
    "ishape = parent_model.get_tensor_shape(iname)\n",
    "\n",
    "input_dict = {iname: software_output[0].reshape(ishape).detach().numpy()}\n",
    "print(input_dict)\n",
    "\n",
    "ret = execute_onnx(parent_model, input_dict, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def softmax_logits(logits):\n",
    "    prediction_list = softmax(logits)\n",
    "    max_val = -1\n",
    "    prediction = 0\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] > max_val:\n",
    "            max_val = prediction_list[i]\n",
    "            prediction = i\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "logits = ret[oname].flatten()\n",
    "prob = softmax(logits)\n",
    "\n",
    "print(\"software prediction\")\n",
    "output = cnv_hardware_model(software_output)\n",
    "\n",
    "for i in range(len(output)):\n",
    "    print(softmax(output[i].tolist()), \"prediction\", get_prediction(output[i]), \"target\", target[i].tolist())\n",
    "\n",
    "\n",
    "print(\"hardware prediction\")\n",
    "print(prob)\n",
    "print(\"predicted:\", classes[softmax_logits(logits)], \", actual:\", classes[int(test_output.tolist())])\n",
    "\n",
    "plt.figure(figsize=(20, 3)) \n",
    "plt.bar(classes, prob)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
